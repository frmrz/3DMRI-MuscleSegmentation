{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCMugqt2pZ1j"
      },
      "source": [
        "Installare monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38MyqF0apdps",
        "outputId": "ee397341-efa4-495f-d72e-7d5ec3731dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]\n",
            "  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.13.1+cu116)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imagecodecs\n",
            "  Downloading imagecodecs-2022.12.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.0.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (4.3.3)\n",
            "Collecting openslide-python==1.1.2\n",
            "  Downloading openslide-python-1.1.2.tar.gz (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 KB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (4.64.1)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2022.10.10)\n",
            "Collecting pynrrd\n",
            "  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.99)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (4.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.14.1+cu116)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.3.5)\n",
            "Collecting pytorch-ignite==0.4.10\n",
            "  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 KB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (7.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (6.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.18.3)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.1.1-py3-none-any.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<4.22\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.1.0)\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cucim>=22.8.1\n",
            "  Downloading cucim-22.12.0-py3-none-manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itk>=5.2\n",
            "  Downloading itk-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (8.3 kB)\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite==0.4.10->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (21.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from cucim>=22.8.1->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (7.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown>=4.4.0->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.15.0)\n",
            "Collecting itk-registration==5.3.0\n",
            "  Downloading itk_registration-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (26.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itk-segmentation==5.3.0\n",
            "  Downloading itk_segmentation-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itk-core==5.3.0\n",
            "  Downloading itk_core-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (81.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itk-numerics==5.3.0\n",
            "  Downloading itk_numerics-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (58.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itk-io==5.3.0\n",
            "  Downloading itk_io-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (25.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itk-filtering==5.3.0\n",
            "  Downloading itk_filtering-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (73.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.9.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.7.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (4.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.22->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (22.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (5.10.2)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.0.2)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.11.3)\n",
            "Collecting querystring-parser<2\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.2.0)\n",
            "Collecting gitpython<4,>=2.1.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.4)\n",
            "Collecting gunicorn<21\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0\n",
            "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic<2\n",
            "  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting databricks-cli<1,>=0.8.7\n",
            "  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.4.1)\n",
            "Requirement already satisfied: sqlalchemy<2,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.4.46)\n",
            "Collecting importlib-metadata!=4.7.0,<6,>=3.7.0\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2022.7)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.4.3)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.19.6)\n",
            "Requirement already satisfied: pyarrow<11,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (9.0.0)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.1.4)\n",
            "Collecting shap<1,>=0.40\n",
            "  Downloading shap-0.41.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (575 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.9/575.9 KB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nptyping\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.51.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.38.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.16.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (57.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.8.10)\n",
            "Collecting urllib3>=1.26.0\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from Flask<3->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.1.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (5.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata!=4.7.0,<6,>=3.7.0->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<4,>=2.11->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2022.12.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<2->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<2->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (3.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap<1,>=0.40->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.56.4)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy<2,>=1.4.0->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (2.0.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<4.22->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (1.7.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.4.8)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap<1,>=0.40->mlflow->monai[cucim,einops,fire,gdown,h5py,ignite,imagecodecs,itk,jsonschema,lmdb,matplotlib,mlflow,nibabel,openslide,pandas,pillow,psutil,pydicom,pynrrd,pyyaml,skimage,tensorboard,tensorboardX,tifffile,torchvision,tqdm,transformers]) (0.39.1)\n",
            "Building wheels for collected packages: openslide-python, fire, databricks-cli\n",
            "  Building wheel for openslide-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openslide-python: filename=openslide_python-1.1.2-cp38-cp38-linux_x86_64.whl size=27181 sha256=2ee149feff46d7850365602c3cd0483c9167b6bd664aaa3da5eb3387029b3ed1\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/f7/99/15df0aea11eefca84d990052a0133ead40443e8abe22d18a11\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=467fd69a9c5a0c1e2337d0fe57381e995645c0ffe01986004af5c2cfee65f71f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142894 sha256=5b64f25e3e0ccff6f66c8a59ba0e50c2e1616e9a038366e04d797449fbedc2a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/7c/6e/4bf2c1748c7ecf994ca951591de81674ed6bf633e1e337d873\n",
            "Successfully built openslide-python fire databricks-cli\n",
            "Installing collected packages: tokenizers, websocket-client, urllib3, tensorboardX, smmap, slicer, querystring-parser, pyjwt, pydicom, openslide-python, nptyping, Mako, itk-core, importlib-metadata, imagecodecs, gunicorn, fire, einops, cucim, requests, pytorch-ignite, pynrrd, monai, itk-numerics, itk-io, gitdb, alembic, shap, itk-filtering, huggingface-hub, gitpython, docker, databricks-cli, transformers, mlflow, itk-segmentation, itk-registration, itk\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.2 cucim-22.12.0 databricks-cli-0.17.4 docker-6.0.1 einops-0.6.0 fire-0.5.0 gitdb-4.0.10 gitpython-3.1.30 gunicorn-20.1.0 huggingface-hub-0.11.1 imagecodecs-2022.12.24 importlib-metadata-5.2.0 itk-5.3.0 itk-core-5.3.0 itk-filtering-5.3.0 itk-io-5.3.0 itk-numerics-5.3.0 itk-registration-5.3.0 itk-segmentation-5.3.0 mlflow-2.1.1 monai-1.1.0 nptyping-2.4.1 openslide-python-1.1.2 pydicom-2.3.1 pyjwt-2.6.0 pynrrd-1.0.0 pytorch-ignite-0.4.10 querystring-parser-1.2.4 requests-2.28.2 shap-0.41.0 slicer-0.0.7 smmap-5.0.0 tensorboardX-2.5.1 tokenizers-0.12.1 transformers-4.21.3 urllib3-1.26.14 websocket-client-1.4.2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install 'monai[nibabel, skimage, pillow, tensorboard, gdown, ignite, torchvision, itk, tqdm, lmdb, psutil, cucim, openslide, pandas, einops, transformers, mlflow, matplotlib, tensorboardX, tifffile, imagecodecs, pyyaml, fire, jsonschema, pynrrd, pydicom, h5py]'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAxJ0Co_QZvh",
        "outputId": "e20ac371-627b-416c-a354-f3aef81e8030"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.7.1\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from segmentation-models-pytorch) (0.14.1+cu116)\n",
            "Collecting timm==0.6.12\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from segmentation-models-pytorch) (4.64.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from segmentation-models-pytorch) (7.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1+cu116)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from timm==0.6.12->segmentation-models-pytorch) (0.11.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm==0.6.12->segmentation-models-pytorch) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.28.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (21.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (3.0.9)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=410487f478db3f3fd7c94101daf16956f43bc7311a27d1bcf58acb9c460792b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/b9/90/25a0195cf95fb5533db96f1c77ea3f296b7cc86ae8ae48e3dc\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60966 sha256=1032f8afe2f51bc23c78610cf52a34a50195fcd82c9ef2f0f278599c687620ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/fa/b9/5c82b59d905f95542a192b883c0cc0082407ea2f54beb2f9e6\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.2 timm-0.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqTDtfZapiSa"
      },
      "source": [
        "Import librerie utili"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VOnjJKUGpKvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753814b6-8637-45dd-af3e-ac761b640bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONAI version: 1.1.0\n",
            "Numpy version: 1.21.6\n",
            "Pytorch version: 1.13.1+cu116\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3\n",
            "MONAI __file__: /usr/local/lib/python3.8/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: 0.4.10\n",
            "Nibabel version: 3.0.2\n",
            "scikit-image version: 0.18.3\n",
            "Pillow version: 7.1.2\n",
            "Tensorboard version: 2.9.1\n",
            "gdown version: 4.4.0\n",
            "TorchVision version: 0.14.1+cu116\n",
            "tqdm version: 4.64.1\n",
            "lmdb version: 0.99\n",
            "psutil version: 5.4.8\n",
            "pandas version: 1.3.5\n",
            "einops version: 0.6.0\n",
            "transformers version: 4.21.3\n",
            "mlflow version: 2.1.1\n",
            "pynrrd version: 1.0.0\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import tempfile\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import monai\n",
        "from monai.data import CacheDataset, Dataset, decollate_batch, DataLoader\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    AsDiscrete,\n",
        "    AddChanneld,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    Resized,\n",
        "    EnsureChannelFirstd,\n",
        "    RandFlipd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandShiftIntensityd,\n",
        "    ScaleIntensityRanged,\n",
        "    DataStats,\n",
        "    Spacingd,\n",
        "    AsDiscreted,\n",
        "    LabelToMaskd,\n",
        "    RandRotate90d,\n",
        "    EnsureType,\n",
        "    SaveImaged,\n",
        "    Invertd,\n",
        "    EnsureTyped,\n",
        "    ToTensord,\n",
        "    RandAffined,\n",
        "    EnsureTyped,\n",
        "    CenterSpatialCropd    \n",
        "\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.utils import first,get_torch_version_tuple, set_determinism\n",
        "from monai.losses import DiceLoss,DiceCELoss\n",
        "from monai.networks.nets import UNet, UNETR\n",
        "from monai.networks.layers import Norm\n",
        "\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "print_config()\n",
        "\n",
        "if get_torch_version_tuple() < (1, 6):\n",
        "    raise RuntimeError(\n",
        "        \"AMP feature only exists in PyTorch version greater than v1.6.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8y-R_lOpmif"
      },
      "source": [
        "Set determinism per un training ripetibile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yc3CvkwHpp1x"
      },
      "outputs": [],
      "source": [
        "SEED = 3  \n",
        "set_determinism(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJjgAiOptJY"
      },
      "source": [
        "Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gGkLslhZp5Gv"
      },
      "outputs": [],
      "source": [
        "# main_dir = 'drive/MyDrive/tesi/PROVE 2D'\n",
        "# directory= os.path.join(main_dir,'DATASET 2D','all image ridotto (training)')\n",
        "#directory = 'drive/MyDrive/tesi/PROVE 2D/DATASET 2D/ all image ridotto (training)'\n",
        "dir____='drive/MyDrive/tesi/PROVE 2D/DATASET 2D/all image ridotto (training) '\n",
        "train_IMG_path = sorted(glob(os.path.join(dir____,'imagesTr-coscia','*.png')))\n",
        "train_MASK_path = sorted(glob(os.path.join(dir____,'labelsTr-coscia','*.png')))\n",
        "\n",
        "\n",
        "val_IMG_path = sorted(glob(os.path.join(dir____,'imagesVl-coscia','*.png')))\n",
        "val_MASK_path = sorted(glob(os.path.join(dir____,'labelsVl-coscia','*.png')))\n",
        "\n",
        "\n",
        "train_files = [{\"image\": image_name, \"label\": mask_name} for image_name,mask_name in zip(train_IMG_path,train_MASK_path)]\n",
        "val_files = [{\"image\": image_name, \"label\": mask_name} for image_name,mask_name in zip(val_IMG_path,val_MASK_path)]\n",
        "\n",
        "#train_files = train_files[59:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def transformations():\n",
        "    train_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "            #consideriamo le patches contenenti tutta l'immagine per l'allenamento, senza crop\n",
        "\n",
        "            # RandCropByPosNegLabeld(\n",
        "            #     keys=[\"image\", \"label\"],\n",
        "            #     label_key=\"label\",\n",
        "            #     spatial_size=[112,112],\n",
        "            #     pos=1,\n",
        "            #     neg=1,\n",
        "            #     num_samples=4,\n",
        "            #     image_key=\"image\",\n",
        "            #     image_threshold=0,\n",
        "            # ),\n",
        "           \n",
        "            RandFlipd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                spatial_axis=[0],\n",
        "                prob=0.10,\n",
        "            ),\n",
        "            RandFlipd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                spatial_axis=[1],\n",
        "                prob=0.10,\n",
        "            ),\n",
        "            RandRotate90d(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                prob=0.10,\n",
        "                spatial_axes=[0, 1]\n",
        "            ),\n",
        "            RandShiftIntensityd(\n",
        "                keys=[\"image\"],\n",
        "                offsets=0.10,\n",
        "                prob=0.50,\n",
        "            ),\n",
        "            ToTensord(keys=[\"image\", \"label\"])\n",
        "        ] )      \n",
        "\n",
        "\n",
        "    #trasformazioni del validation\n",
        "    val_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "            ToTensord(keys=[\"image\", \"label\"])\n",
        "        ]\n",
        "    )\n",
        "    return train_transforms, val_transforms\n",
        "\n",
        "\n",
        "def train_process(amp=False):\n",
        "    train_transforms, val_transforms = transformations()\n",
        "    train_ds = Dataset(train_files , train_transforms)\n",
        "    #train_loader = DataLoader( train_ds, batch_size=2)\n",
        "    train_loader = DataLoader( train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
        "\n",
        "    val_ds = Dataset(val_files , val_transforms)\n",
        "    val_loader = DataLoader( val_ds, batch_size=1, num_workers=4)\n",
        "    \n",
        "    model_dir = os.path.join(dir____,'MODEL_nozeros')\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "      os.mkdir(model_dir)\n",
        "    device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "    # model = UNet(\n",
        "    #     spatial_dims=3,\n",
        "    #     in_channels=1,\n",
        "    #     out_channels=2,\n",
        "    #     channels=(16, 32, 64, 128, 256),\n",
        "    #     strides=(2, 2, 2, 2),\n",
        "    #     num_res_units=2,\n",
        "    #     norm=Norm.BATCH,\n",
        "    # ).to(device)\n",
        "    \n",
        "    model = smp.Unet('resnet50',\n",
        "                     encoder_depth = 4,\n",
        "                     encoder_weights= 'imagenet',\n",
        "                     decoder_channels= ( 128 , 64 , 32 ,16),\n",
        "                     in_channels = 1, classes=13).cuda()\n",
        "    \n",
        "    # model.load_state_dict(torch.load(\n",
        "    #  os.path.join(model_dir, \"best_metric_model_Unet_2d.pth\")))\n",
        "\n",
        "    loss_function = DiceLoss(to_onehot_y=True,softmax=True)\n",
        "    \n",
        "    #loss_function = smp.losses.DiceLoss('multiclass',from_logits= True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
        "    scaler = torch.cuda.amp.GradScaler() if amp else None\n",
        "\n",
        "    post_pred = Compose([Activations(softmax=True ), AsDiscrete(argmax=True, to_onehot=13)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=13)])\n",
        "\n",
        "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
        "\n",
        "    max_epochs = 50\n",
        "    val_interval = 1  # do validation for every epoch\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    \n",
        "    epoch_loss_values = []\n",
        "    metric_values = []\n",
        "   \n",
        "    \n",
        "    for epoch in range(max_epochs):\n",
        "        #epoch_start = time.time()\n",
        "        print(\"-\" * 10)\n",
        "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for batch_data in train_loader:\n",
        "            #step_start = time.time()\n",
        "            step += 1\n",
        "            inputs, labels = (\n",
        "                batch_data[\"image\"].cuda(),\n",
        "                batch_data[\"label\"].cuda(),\n",
        "            )\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            if amp and scaler is not None:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(inputs)\n",
        "                    #loss = loss_function(outputs, labels)\n",
        "                    loss = loss_function(outputs, labels)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_function(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            print(\n",
        "                f\"{step}/{len(train_ds) // train_loader.batch_size},\"\n",
        "                f\" train_loss: {loss.item():.4f}\"\n",
        "               \n",
        "            )\n",
        "        epoch_loss /= step\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for val_data in val_loader:\n",
        "                    val_inputs, val_labels = (\n",
        "                        val_data[\"image\"].cuda(),\n",
        "                        val_data[\"label\"].cuda(),\n",
        "                    )\n",
        "                    roi_size = (144,144)\n",
        "                    sw_batch_size = 4\n",
        "                    if amp:\n",
        "                        with torch.cuda.amp.autocast():\n",
        "                            val_outputs = sliding_window_inference(\n",
        "                                val_inputs, roi_size, sw_batch_size, model\n",
        "                            )\n",
        "                    else:\n",
        "                        val_outputs = sliding_window_inference(\n",
        "                            val_inputs, roi_size, sw_batch_size, model\n",
        "                        )\n",
        "                    \n",
        "                    #val_outputs = torch.softmax(val_outputs, dim = 1)\n",
        "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
        "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
        "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "                metric = dice_metric.aggregate().item()\n",
        "                dice_metric.reset()\n",
        "                metric_values.append(metric)\n",
        "                if metric > best_metric:\n",
        "                    best_metric = metric\n",
        "                    best_metric_epoch = epoch + 1\n",
        "                    torch.save(model.state_dict(), os.path.join(\n",
        "                        model_dir, \"best_metric_model_Unet_2d.pth\"))\n",
        "                    #torch.save(model.state_dict(), \"best_metric_model.pth\")\n",
        "                    print(\"saved new best metric model\")\n",
        "                print(\n",
        "                    f\"current epoch: {epoch + 1} current\"\n",
        "                    f\" mean dice: {metric:.4f}\"\n",
        "                    f\" best mean dice: {best_metric:.4f} \"\n",
        "                    f\"at epoch: {best_metric_epoch}\"\n",
        "                )\n",
        "        \n",
        "        \n",
        "    print(\n",
        "        f\"train completed, best_metric: {best_metric:.4f}\"\n",
        "        f\" at epoch: {best_metric_epoch}\"\n",
        "        \n",
        "    )\n",
        "    return (\n",
        "        max_epochs,\n",
        "        epoch_loss_values,\n",
        "        metric_values,\n",
        "        \n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "(\n",
        "    max_epochs,\n",
        "    epoch_loss_values,\n",
        "    metric_values,\n",
        "    \n",
        ") = train_process(amp=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "49R4357olFFH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ed517ec4b3ac440baaa57f27e7803502",
            "bde6aaf540934878965e745c1dfbccd3",
            "5f5b557bef694ea6aa8f048755269629",
            "bc1a1f24ef4548b3a1f6dc814ebfce65",
            "ddfa71c98d484e899177dbd39f0a0816",
            "9a08482bbbdd4f56ad84da9a233ec52c",
            "e912aacd85ef46bfae064b3ca3517307",
            "f7da7c9a619544f6921d934fb7770f0d",
            "84c0dac9a61543fca1bb62622f665cc1",
            "389899da1dc14246b7f5eb7723ddd515",
            "c9424144d80547c7a3239d9ce17f2443"
          ]
        },
        "outputId": "59bbe600-eaf8-416b-ff17-6effbcbc3268"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed517ec4b3ac440baaa57f27e7803502"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "437/448, train_loss: 0.1295\n",
            "438/448, train_loss: 0.1499\n",
            "439/448, train_loss: 0.1607\n",
            "440/448, train_loss: 0.1900\n",
            "441/448, train_loss: 0.2613\n",
            "442/448, train_loss: 0.1472\n",
            "443/448, train_loss: 0.1812\n",
            "444/448, train_loss: 0.2458\n",
            "445/448, train_loss: 0.2146\n",
            "446/448, train_loss: 0.1764\n",
            "447/448, train_loss: 0.1308\n",
            "448/448, train_loss: 0.1472\n",
            "449/448, train_loss: 0.0912\n",
            "epoch 39 average loss: 0.1766\n",
            "current epoch: 39 current mean dice: 0.8318 best mean dice: 0.8333 at epoch: 29\n",
            "----------\n",
            "epoch 40/50\n",
            "1/448, train_loss: 0.1343\n",
            "2/448, train_loss: 0.1715\n",
            "3/448, train_loss: 0.1877\n",
            "4/448, train_loss: 0.2657\n",
            "5/448, train_loss: 0.1623\n",
            "6/448, train_loss: 0.1515\n",
            "7/448, train_loss: 0.1608\n",
            "8/448, train_loss: 0.1362\n",
            "9/448, train_loss: 0.1755\n",
            "10/448, train_loss: 0.2106\n",
            "11/448, train_loss: 0.5101\n",
            "12/448, train_loss: 0.1884\n",
            "13/448, train_loss: 0.1885\n",
            "14/448, train_loss: 0.1867\n",
            "15/448, train_loss: 0.1847\n",
            "16/448, train_loss: 0.0917\n",
            "17/448, train_loss: 0.1503\n",
            "18/448, train_loss: 0.1775\n",
            "19/448, train_loss: 0.1309\n",
            "20/448, train_loss: 0.1730\n",
            "21/448, train_loss: 0.1259\n",
            "22/448, train_loss: 0.1510\n",
            "23/448, train_loss: 0.3279\n",
            "24/448, train_loss: 0.1373\n",
            "25/448, train_loss: 0.2507\n",
            "26/448, train_loss: 0.1495\n",
            "27/448, train_loss: 0.1402\n",
            "28/448, train_loss: 0.1810\n",
            "29/448, train_loss: 0.2662\n",
            "30/448, train_loss: 0.1332\n",
            "31/448, train_loss: 0.1139\n",
            "32/448, train_loss: 0.1381\n",
            "33/448, train_loss: 0.2194\n",
            "34/448, train_loss: 0.1415\n",
            "35/448, train_loss: 0.1788\n",
            "36/448, train_loss: 0.1419\n",
            "37/448, train_loss: 0.0697\n",
            "38/448, train_loss: 0.1525\n",
            "39/448, train_loss: 0.2595\n",
            "40/448, train_loss: 0.1781\n",
            "41/448, train_loss: 0.1179\n",
            "42/448, train_loss: 0.3441\n",
            "43/448, train_loss: 0.1625\n",
            "44/448, train_loss: 0.1256\n",
            "45/448, train_loss: 0.1719\n",
            "46/448, train_loss: 0.2307\n",
            "47/448, train_loss: 0.1978\n",
            "48/448, train_loss: 0.1048\n",
            "49/448, train_loss: 0.1741\n",
            "50/448, train_loss: 0.1417\n",
            "51/448, train_loss: 0.0910\n",
            "52/448, train_loss: 0.1985\n",
            "53/448, train_loss: 0.4909\n",
            "54/448, train_loss: 0.1623\n",
            "55/448, train_loss: 0.1279\n",
            "56/448, train_loss: 0.1285\n",
            "57/448, train_loss: 0.1905\n",
            "58/448, train_loss: 0.1339\n",
            "59/448, train_loss: 0.4957\n",
            "60/448, train_loss: 0.1840\n",
            "61/448, train_loss: 0.1694\n",
            "62/448, train_loss: 0.1804\n",
            "63/448, train_loss: 0.1425\n",
            "64/448, train_loss: 0.1953\n",
            "65/448, train_loss: 0.1447\n",
            "66/448, train_loss: 0.2772\n",
            "67/448, train_loss: 0.0950\n",
            "68/448, train_loss: 0.1814\n",
            "69/448, train_loss: 0.1166\n",
            "70/448, train_loss: 0.0900\n",
            "71/448, train_loss: 0.2522\n",
            "72/448, train_loss: 0.1692\n",
            "73/448, train_loss: 0.1261\n",
            "74/448, train_loss: 0.1731\n",
            "75/448, train_loss: 0.1701\n",
            "76/448, train_loss: 0.1679\n",
            "77/448, train_loss: 0.1855\n",
            "78/448, train_loss: 0.2795\n",
            "79/448, train_loss: 0.1412\n",
            "80/448, train_loss: 0.1421\n",
            "81/448, train_loss: 0.1302\n",
            "82/448, train_loss: 0.1295\n",
            "83/448, train_loss: 0.2852\n",
            "84/448, train_loss: 0.1600\n",
            "85/448, train_loss: 0.1845\n",
            "86/448, train_loss: 0.1831\n",
            "87/448, train_loss: 0.2574\n",
            "88/448, train_loss: 0.1642\n",
            "89/448, train_loss: 0.1441\n",
            "90/448, train_loss: 0.2080\n",
            "91/448, train_loss: 0.0897\n",
            "92/448, train_loss: 0.1451\n",
            "93/448, train_loss: 0.1966\n",
            "94/448, train_loss: 0.1091\n",
            "95/448, train_loss: 0.1623\n",
            "96/448, train_loss: 0.2052\n",
            "97/448, train_loss: 0.1695\n",
            "98/448, train_loss: 0.1512\n",
            "99/448, train_loss: 0.1738\n",
            "100/448, train_loss: 0.2459\n",
            "101/448, train_loss: 0.1349\n",
            "102/448, train_loss: 0.3730\n",
            "103/448, train_loss: 0.1147\n",
            "104/448, train_loss: 0.1437\n",
            "105/448, train_loss: 0.1513\n",
            "106/448, train_loss: 0.2031\n",
            "107/448, train_loss: 0.4308\n",
            "108/448, train_loss: 0.1160\n",
            "109/448, train_loss: 0.2970\n",
            "110/448, train_loss: 0.2216\n",
            "111/448, train_loss: 0.1668\n",
            "112/448, train_loss: 0.1476\n",
            "113/448, train_loss: 0.1671\n",
            "114/448, train_loss: 0.0630\n",
            "115/448, train_loss: 0.1467\n",
            "116/448, train_loss: 0.1009\n",
            "117/448, train_loss: 0.1704\n",
            "118/448, train_loss: 0.1889\n",
            "119/448, train_loss: 0.1652\n",
            "120/448, train_loss: 0.2139\n",
            "121/448, train_loss: 0.1209\n",
            "122/448, train_loss: 0.1296\n",
            "123/448, train_loss: 0.1923\n",
            "124/448, train_loss: 0.1403\n",
            "125/448, train_loss: 0.2113\n",
            "126/448, train_loss: 0.1598\n",
            "127/448, train_loss: 0.1882\n",
            "128/448, train_loss: 0.2071\n",
            "129/448, train_loss: 0.1744\n",
            "130/448, train_loss: 0.1738\n",
            "131/448, train_loss: 0.1848\n",
            "132/448, train_loss: 0.1519\n",
            "133/448, train_loss: 0.1290\n",
            "134/448, train_loss: 0.1861\n",
            "135/448, train_loss: 0.2548\n",
            "136/448, train_loss: 0.1568\n",
            "137/448, train_loss: 0.1764\n",
            "138/448, train_loss: 0.1403\n",
            "139/448, train_loss: 0.2285\n",
            "140/448, train_loss: 0.1410\n",
            "141/448, train_loss: 0.0997\n",
            "142/448, train_loss: 0.1149\n",
            "143/448, train_loss: 0.2442\n",
            "144/448, train_loss: 0.1715\n",
            "145/448, train_loss: 0.1563\n",
            "146/448, train_loss: 0.1107\n",
            "147/448, train_loss: 0.1873\n",
            "148/448, train_loss: 0.2161\n",
            "149/448, train_loss: 0.1552\n",
            "150/448, train_loss: 0.1769\n",
            "151/448, train_loss: 0.1348\n",
            "152/448, train_loss: 0.3798\n",
            "153/448, train_loss: 0.2396\n",
            "154/448, train_loss: 0.1415\n",
            "155/448, train_loss: 0.1752\n",
            "156/448, train_loss: 0.1470\n",
            "157/448, train_loss: 0.3637\n",
            "158/448, train_loss: 0.1330\n",
            "159/448, train_loss: 0.1481\n",
            "160/448, train_loss: 0.1650\n",
            "161/448, train_loss: 0.1778\n",
            "162/448, train_loss: 0.1042\n",
            "163/448, train_loss: 0.1216\n",
            "164/448, train_loss: 0.1215\n",
            "165/448, train_loss: 0.1318\n",
            "166/448, train_loss: 0.1334\n",
            "167/448, train_loss: 0.1785\n",
            "168/448, train_loss: 0.1799\n",
            "169/448, train_loss: 0.3000\n",
            "170/448, train_loss: 0.1476\n",
            "171/448, train_loss: 0.1024\n",
            "172/448, train_loss: 0.0931\n",
            "173/448, train_loss: 0.2152\n",
            "174/448, train_loss: 0.1421\n",
            "175/448, train_loss: 0.0622\n",
            "176/448, train_loss: 0.3515\n",
            "177/448, train_loss: 0.1067\n",
            "178/448, train_loss: 0.3050\n",
            "179/448, train_loss: 0.1101\n",
            "180/448, train_loss: 0.4120\n",
            "181/448, train_loss: 0.1834\n",
            "182/448, train_loss: 0.3372\n",
            "183/448, train_loss: 0.1797\n",
            "184/448, train_loss: 0.1784\n",
            "185/448, train_loss: 0.1832\n",
            "186/448, train_loss: 0.2018\n",
            "187/448, train_loss: 0.1623\n",
            "188/448, train_loss: 0.1789\n",
            "189/448, train_loss: 0.1176\n",
            "190/448, train_loss: 0.4431\n",
            "191/448, train_loss: 0.1409\n",
            "192/448, train_loss: 0.2786\n",
            "193/448, train_loss: 0.1695\n",
            "194/448, train_loss: 0.1428\n",
            "195/448, train_loss: 0.2147\n",
            "196/448, train_loss: 0.1803\n",
            "197/448, train_loss: 0.1772\n",
            "198/448, train_loss: 0.1937\n",
            "199/448, train_loss: 0.2146\n",
            "200/448, train_loss: 0.1613\n",
            "201/448, train_loss: 0.1490\n",
            "202/448, train_loss: 0.1535\n",
            "203/448, train_loss: 0.3082\n",
            "204/448, train_loss: 0.1131\n",
            "205/448, train_loss: 0.1943\n",
            "206/448, train_loss: 0.1839\n",
            "207/448, train_loss: 0.0871\n",
            "208/448, train_loss: 0.1662\n",
            "209/448, train_loss: 0.3579\n",
            "210/448, train_loss: 0.1373\n",
            "211/448, train_loss: 0.2597\n",
            "212/448, train_loss: 0.1944\n",
            "213/448, train_loss: 0.2441\n",
            "214/448, train_loss: 0.1362\n",
            "215/448, train_loss: 0.1920\n",
            "216/448, train_loss: 0.1594\n",
            "217/448, train_loss: 0.1364\n",
            "218/448, train_loss: 0.1982\n",
            "219/448, train_loss: 0.1162\n",
            "220/448, train_loss: 0.1951\n",
            "221/448, train_loss: 0.3925\n",
            "222/448, train_loss: 0.1673\n",
            "223/448, train_loss: 0.1706\n",
            "224/448, train_loss: 0.2533\n",
            "225/448, train_loss: 0.2516\n",
            "226/448, train_loss: 0.1644\n",
            "227/448, train_loss: 0.1325\n",
            "228/448, train_loss: 0.1837\n",
            "229/448, train_loss: 0.2039\n",
            "230/448, train_loss: 0.1582\n",
            "231/448, train_loss: 0.1924\n",
            "232/448, train_loss: 0.1348\n",
            "233/448, train_loss: 0.2423\n",
            "234/448, train_loss: 0.2140\n",
            "235/448, train_loss: 0.0968\n",
            "236/448, train_loss: 0.0717\n",
            "237/448, train_loss: 0.1335\n",
            "238/448, train_loss: 0.4906\n",
            "239/448, train_loss: 0.2637\n",
            "240/448, train_loss: 0.1427\n",
            "241/448, train_loss: 0.1922\n",
            "242/448, train_loss: 0.1412\n",
            "243/448, train_loss: 0.1983\n",
            "244/448, train_loss: 0.1493\n",
            "245/448, train_loss: 0.1322\n",
            "246/448, train_loss: 0.1503\n",
            "247/448, train_loss: 0.1538\n",
            "248/448, train_loss: 0.2280\n",
            "249/448, train_loss: 0.1912\n",
            "250/448, train_loss: 0.2910\n",
            "251/448, train_loss: 0.1388\n",
            "252/448, train_loss: 0.2261\n",
            "253/448, train_loss: 0.1551\n",
            "254/448, train_loss: 0.1119\n",
            "255/448, train_loss: 0.1881\n",
            "256/448, train_loss: 0.1784\n",
            "257/448, train_loss: 0.1220\n",
            "258/448, train_loss: 0.1556\n",
            "259/448, train_loss: 0.2308\n",
            "260/448, train_loss: 0.1385\n",
            "261/448, train_loss: 0.1416\n",
            "262/448, train_loss: 0.1734\n",
            "263/448, train_loss: 0.2970\n",
            "264/448, train_loss: 0.1147\n",
            "265/448, train_loss: 0.1415\n",
            "266/448, train_loss: 0.1051\n",
            "267/448, train_loss: 0.1702\n",
            "268/448, train_loss: 0.1232\n",
            "269/448, train_loss: 0.1166\n",
            "270/448, train_loss: 0.1056\n",
            "271/448, train_loss: 0.1724\n",
            "272/448, train_loss: 0.1490\n",
            "273/448, train_loss: 0.1444\n",
            "274/448, train_loss: 0.1722\n",
            "275/448, train_loss: 0.2334\n",
            "276/448, train_loss: 0.1977\n",
            "277/448, train_loss: 0.2292\n",
            "278/448, train_loss: 0.1634\n",
            "279/448, train_loss: 0.1216\n",
            "280/448, train_loss: 0.1568\n",
            "281/448, train_loss: 0.1455\n",
            "282/448, train_loss: 0.2314\n",
            "283/448, train_loss: 0.2104\n",
            "284/448, train_loss: 0.1483\n",
            "285/448, train_loss: 0.1834\n",
            "286/448, train_loss: 0.1514\n",
            "287/448, train_loss: 0.3498\n",
            "288/448, train_loss: 0.2308\n",
            "289/448, train_loss: 0.1830\n",
            "290/448, train_loss: 0.2096\n",
            "291/448, train_loss: 0.1663\n",
            "292/448, train_loss: 0.1875\n",
            "293/448, train_loss: 0.1452\n",
            "294/448, train_loss: 0.1755\n",
            "295/448, train_loss: 0.2541\n",
            "296/448, train_loss: 0.2039\n",
            "297/448, train_loss: 0.1454\n",
            "298/448, train_loss: 0.2053\n",
            "299/448, train_loss: 0.1608\n",
            "300/448, train_loss: 0.2153\n",
            "301/448, train_loss: 0.2467\n",
            "302/448, train_loss: 0.1342\n",
            "303/448, train_loss: 0.2082\n",
            "304/448, train_loss: 0.0951\n",
            "305/448, train_loss: 0.1615\n",
            "306/448, train_loss: 0.1394\n",
            "307/448, train_loss: 0.2171\n",
            "308/448, train_loss: 0.1448\n",
            "309/448, train_loss: 0.1797\n",
            "310/448, train_loss: 0.4514\n",
            "311/448, train_loss: 0.1269\n",
            "312/448, train_loss: 0.1806\n",
            "313/448, train_loss: 0.1460\n",
            "314/448, train_loss: 0.0596\n",
            "315/448, train_loss: 0.1704\n",
            "316/448, train_loss: 0.1831\n",
            "317/448, train_loss: 0.1511\n",
            "318/448, train_loss: 0.2280\n",
            "319/448, train_loss: 0.1989\n",
            "320/448, train_loss: 0.0658\n",
            "321/448, train_loss: 0.4235\n",
            "322/448, train_loss: 0.1925\n",
            "323/448, train_loss: 0.1451\n",
            "324/448, train_loss: 0.1744\n",
            "325/448, train_loss: 0.1344\n",
            "326/448, train_loss: 0.1403\n",
            "327/448, train_loss: 0.1644\n",
            "328/448, train_loss: 0.3395\n",
            "329/448, train_loss: 0.1723\n",
            "330/448, train_loss: 0.1290\n",
            "331/448, train_loss: 0.0710\n",
            "332/448, train_loss: 0.1078\n",
            "333/448, train_loss: 0.1780\n",
            "334/448, train_loss: 0.1679\n",
            "335/448, train_loss: 0.1625\n",
            "336/448, train_loss: 0.1312\n",
            "337/448, train_loss: 0.1477\n",
            "338/448, train_loss: 0.1643\n",
            "339/448, train_loss: 0.1763\n",
            "340/448, train_loss: 0.1404\n",
            "341/448, train_loss: 0.1474\n",
            "342/448, train_loss: 0.1002\n",
            "343/448, train_loss: 0.1068\n",
            "344/448, train_loss: 0.1902\n",
            "345/448, train_loss: 0.1568\n",
            "346/448, train_loss: 0.1015\n",
            "347/448, train_loss: 0.1668\n",
            "348/448, train_loss: 0.1746\n",
            "349/448, train_loss: 0.2577\n",
            "350/448, train_loss: 0.1938\n",
            "351/448, train_loss: 0.1862\n",
            "352/448, train_loss: 0.2392\n",
            "353/448, train_loss: 0.1008\n",
            "354/448, train_loss: 0.2909\n",
            "355/448, train_loss: 0.1006\n",
            "356/448, train_loss: 0.1694\n",
            "357/448, train_loss: 0.1411\n",
            "358/448, train_loss: 0.1385\n",
            "359/448, train_loss: 0.2136\n",
            "360/448, train_loss: 0.1418\n",
            "361/448, train_loss: 0.1773\n",
            "362/448, train_loss: 0.1833\n",
            "363/448, train_loss: 0.1464\n",
            "364/448, train_loss: 0.1403\n",
            "365/448, train_loss: 0.1752\n",
            "366/448, train_loss: 0.1430\n",
            "367/448, train_loss: 0.2572\n",
            "368/448, train_loss: 0.2342\n",
            "369/448, train_loss: 0.1389\n",
            "370/448, train_loss: 0.1320\n",
            "371/448, train_loss: 0.1611\n",
            "372/448, train_loss: 0.1991\n",
            "373/448, train_loss: 0.2107\n",
            "374/448, train_loss: 0.0976\n",
            "375/448, train_loss: 0.1601\n",
            "376/448, train_loss: 0.1766\n",
            "377/448, train_loss: 0.1415\n",
            "378/448, train_loss: 0.2165\n",
            "379/448, train_loss: 0.2217\n",
            "380/448, train_loss: 0.1243\n",
            "381/448, train_loss: 0.2439\n",
            "382/448, train_loss: 0.1385\n",
            "383/448, train_loss: 0.1982\n",
            "384/448, train_loss: 0.1734\n",
            "385/448, train_loss: 0.0785\n",
            "386/448, train_loss: 0.1867\n",
            "387/448, train_loss: 0.2131\n",
            "388/448, train_loss: 0.2672\n",
            "389/448, train_loss: 0.2792\n",
            "390/448, train_loss: 0.1455\n",
            "391/448, train_loss: 0.1131\n",
            "392/448, train_loss: 0.1373\n",
            "393/448, train_loss: 0.1541\n",
            "394/448, train_loss: 0.2427\n",
            "395/448, train_loss: 0.1642\n",
            "396/448, train_loss: 0.2467\n",
            "397/448, train_loss: 0.1382\n",
            "398/448, train_loss: 0.1214\n",
            "399/448, train_loss: 0.2060\n",
            "400/448, train_loss: 0.2447\n",
            "401/448, train_loss: 0.1523\n",
            "402/448, train_loss: 0.1563\n",
            "403/448, train_loss: 0.2031\n",
            "404/448, train_loss: 0.1716\n",
            "405/448, train_loss: 0.1737\n",
            "406/448, train_loss: 0.1862\n",
            "407/448, train_loss: 0.1374\n",
            "408/448, train_loss: 0.2635\n",
            "409/448, train_loss: 0.2284\n",
            "410/448, train_loss: 0.1571\n",
            "411/448, train_loss: 0.1492\n",
            "412/448, train_loss: 0.0644\n",
            "413/448, train_loss: 0.1508\n",
            "414/448, train_loss: 0.1711\n",
            "415/448, train_loss: 0.2623\n",
            "416/448, train_loss: 0.1655\n",
            "417/448, train_loss: 0.1668\n",
            "418/448, train_loss: 0.1283\n",
            "419/448, train_loss: 0.1334\n",
            "420/448, train_loss: 0.2822\n",
            "421/448, train_loss: 0.2067\n",
            "422/448, train_loss: 0.2622\n",
            "423/448, train_loss: 0.2072\n",
            "424/448, train_loss: 0.1434\n",
            "425/448, train_loss: 0.0978\n",
            "426/448, train_loss: 0.1878\n",
            "427/448, train_loss: 0.1499\n",
            "428/448, train_loss: 0.1626\n",
            "429/448, train_loss: 0.1496\n",
            "430/448, train_loss: 0.2010\n",
            "431/448, train_loss: 0.2049\n",
            "432/448, train_loss: 0.0748\n",
            "433/448, train_loss: 0.1631\n",
            "434/448, train_loss: 0.1489\n",
            "435/448, train_loss: 0.1721\n",
            "436/448, train_loss: 0.1636\n",
            "437/448, train_loss: 0.1515\n",
            "438/448, train_loss: 0.2280\n",
            "439/448, train_loss: 0.1194\n",
            "440/448, train_loss: 0.0960\n",
            "441/448, train_loss: 0.1763\n",
            "442/448, train_loss: 0.1924\n",
            "443/448, train_loss: 0.2126\n",
            "444/448, train_loss: 0.0697\n",
            "445/448, train_loss: 0.2019\n",
            "446/448, train_loss: 0.2163\n",
            "447/448, train_loss: 0.2191\n",
            "448/448, train_loss: 0.1385\n",
            "449/448, train_loss: 0.2575\n",
            "epoch 40 average loss: 0.1796\n",
            "current epoch: 40 current mean dice: 0.8332 best mean dice: 0.8333 at epoch: 29\n",
            "----------\n",
            "epoch 41/50\n",
            "1/448, train_loss: 0.1373\n",
            "2/448, train_loss: 0.1801\n",
            "3/448, train_loss: 0.1404\n",
            "4/448, train_loss: 0.1593\n",
            "5/448, train_loss: 0.2472\n",
            "6/448, train_loss: 0.1408\n",
            "7/448, train_loss: 0.1387\n",
            "8/448, train_loss: 0.1545\n",
            "9/448, train_loss: 0.2015\n",
            "10/448, train_loss: 0.1924\n",
            "11/448, train_loss: 0.1599\n",
            "12/448, train_loss: 0.1727\n",
            "13/448, train_loss: 0.1361\n",
            "14/448, train_loss: 0.1724\n",
            "15/448, train_loss: 0.1415\n",
            "16/448, train_loss: 0.4086\n",
            "17/448, train_loss: 0.1349\n",
            "18/448, train_loss: 0.1735\n",
            "19/448, train_loss: 0.2007\n",
            "20/448, train_loss: 0.2256\n",
            "21/448, train_loss: 0.0990\n",
            "22/448, train_loss: 0.1635\n",
            "23/448, train_loss: 0.2252\n",
            "24/448, train_loss: 0.2650\n",
            "25/448, train_loss: 0.3810\n",
            "26/448, train_loss: 0.1850\n",
            "27/448, train_loss: 0.1861\n",
            "28/448, train_loss: 0.1781\n",
            "29/448, train_loss: 0.1290\n",
            "30/448, train_loss: 0.1419\n",
            "31/448, train_loss: 0.1982\n",
            "32/448, train_loss: 0.2078\n",
            "33/448, train_loss: 0.1720\n",
            "34/448, train_loss: 0.1697\n",
            "35/448, train_loss: 0.1666\n",
            "36/448, train_loss: 0.1301\n",
            "37/448, train_loss: 0.1767\n",
            "38/448, train_loss: 0.1388\n",
            "39/448, train_loss: 0.1475\n",
            "40/448, train_loss: 0.0951\n",
            "41/448, train_loss: 0.2900\n",
            "42/448, train_loss: 0.1278\n",
            "43/448, train_loss: 0.2473\n",
            "44/448, train_loss: 0.1459\n",
            "45/448, train_loss: 0.1909\n",
            "46/448, train_loss: 0.1479\n",
            "47/448, train_loss: 0.1821\n",
            "48/448, train_loss: 0.1137\n",
            "49/448, train_loss: 0.1358\n",
            "50/448, train_loss: 0.2073\n",
            "51/448, train_loss: 0.2887\n",
            "52/448, train_loss: 0.1864\n",
            "53/448, train_loss: 0.2368\n",
            "54/448, train_loss: 0.2086\n",
            "55/448, train_loss: 0.2150\n",
            "56/448, train_loss: 0.3272\n",
            "57/448, train_loss: 0.1382\n",
            "58/448, train_loss: 0.1070\n",
            "59/448, train_loss: 0.2813\n",
            "60/448, train_loss: 0.1844\n",
            "61/448, train_loss: 0.1084\n",
            "62/448, train_loss: 0.1370\n",
            "63/448, train_loss: 0.2132\n",
            "64/448, train_loss: 0.1525\n",
            "65/448, train_loss: 0.2002\n",
            "66/448, train_loss: 0.1428\n",
            "67/448, train_loss: 0.1309\n",
            "68/448, train_loss: 0.1824\n",
            "69/448, train_loss: 0.1616\n",
            "70/448, train_loss: 0.2846\n",
            "71/448, train_loss: 0.2274\n",
            "72/448, train_loss: 0.2007\n",
            "73/448, train_loss: 0.1343\n",
            "74/448, train_loss: 0.2479\n",
            "75/448, train_loss: 0.1594\n",
            "76/448, train_loss: 0.2273\n",
            "77/448, train_loss: 0.1108\n",
            "78/448, train_loss: 0.1089\n",
            "79/448, train_loss: 0.1333\n",
            "80/448, train_loss: 0.1317\n",
            "81/448, train_loss: 0.2702\n",
            "82/448, train_loss: 0.1465\n",
            "83/448, train_loss: 0.1716\n",
            "84/448, train_loss: 0.1655\n",
            "85/448, train_loss: 0.1033\n",
            "86/448, train_loss: 0.1001\n",
            "87/448, train_loss: 0.1075\n",
            "88/448, train_loss: 0.1918\n",
            "89/448, train_loss: 0.2466\n",
            "90/448, train_loss: 0.2005\n",
            "91/448, train_loss: 0.2174\n",
            "92/448, train_loss: 0.1208\n",
            "93/448, train_loss: 0.1373\n",
            "94/448, train_loss: 0.1254\n",
            "95/448, train_loss: 0.1027\n",
            "96/448, train_loss: 0.1482\n",
            "97/448, train_loss: 0.1388\n",
            "98/448, train_loss: 0.2205\n",
            "99/448, train_loss: 0.5704\n",
            "100/448, train_loss: 0.1349\n",
            "101/448, train_loss: 0.1155\n",
            "102/448, train_loss: 0.1065\n",
            "103/448, train_loss: 0.1664\n",
            "104/448, train_loss: 0.1461\n",
            "105/448, train_loss: 0.1532\n",
            "106/448, train_loss: 0.1285\n",
            "107/448, train_loss: 0.3420\n",
            "108/448, train_loss: 0.2178\n",
            "109/448, train_loss: 0.1580\n",
            "110/448, train_loss: 0.1291\n",
            "111/448, train_loss: 0.0926\n",
            "112/448, train_loss: 0.1962\n",
            "113/448, train_loss: 0.2307\n",
            "114/448, train_loss: 0.1683\n",
            "115/448, train_loss: 0.1669\n",
            "116/448, train_loss: 0.1945\n",
            "117/448, train_loss: 0.1555\n",
            "118/448, train_loss: 0.1764\n",
            "119/448, train_loss: 0.1786\n",
            "120/448, train_loss: 0.2065\n",
            "121/448, train_loss: 0.3337\n",
            "122/448, train_loss: 0.1352\n",
            "123/448, train_loss: 0.2191\n",
            "124/448, train_loss: 0.1760\n",
            "125/448, train_loss: 0.1643\n",
            "126/448, train_loss: 0.1696\n",
            "127/448, train_loss: 0.2139\n",
            "128/448, train_loss: 0.0906\n",
            "129/448, train_loss: 0.2310\n",
            "130/448, train_loss: 0.1155\n",
            "131/448, train_loss: 0.1589\n",
            "132/448, train_loss: 0.2170\n",
            "133/448, train_loss: 0.1818\n",
            "134/448, train_loss: 0.1916\n",
            "135/448, train_loss: 0.3307\n",
            "136/448, train_loss: 0.1664\n",
            "137/448, train_loss: 0.1744\n",
            "138/448, train_loss: 0.1940\n",
            "139/448, train_loss: 0.4677\n",
            "140/448, train_loss: 0.1435\n",
            "141/448, train_loss: 0.2084\n",
            "142/448, train_loss: 0.1638\n",
            "143/448, train_loss: 0.2206\n",
            "144/448, train_loss: 0.1285\n",
            "145/448, train_loss: 0.1427\n",
            "146/448, train_loss: 0.1212\n",
            "147/448, train_loss: 0.2837\n",
            "148/448, train_loss: 0.2035\n",
            "149/448, train_loss: 0.1735\n",
            "150/448, train_loss: 0.1562\n",
            "151/448, train_loss: 0.1108\n",
            "152/448, train_loss: 0.2244\n",
            "153/448, train_loss: 0.2266\n",
            "154/448, train_loss: 0.1711\n",
            "155/448, train_loss: 0.2565\n",
            "156/448, train_loss: 0.4290\n",
            "157/448, train_loss: 0.1499\n",
            "158/448, train_loss: 0.1234\n",
            "159/448, train_loss: 0.1056\n",
            "160/448, train_loss: 0.2018\n",
            "161/448, train_loss: 0.3566\n",
            "162/448, train_loss: 0.1899\n",
            "163/448, train_loss: 0.1728\n",
            "164/448, train_loss: 0.1836\n",
            "165/448, train_loss: 0.1917\n",
            "166/448, train_loss: 0.1338\n",
            "167/448, train_loss: 0.1792\n",
            "168/448, train_loss: 0.1810\n",
            "169/448, train_loss: 0.0618\n",
            "170/448, train_loss: 0.2984\n",
            "171/448, train_loss: 0.0736\n",
            "172/448, train_loss: 0.0727\n",
            "173/448, train_loss: 0.2109\n",
            "174/448, train_loss: 0.1603\n",
            "175/448, train_loss: 0.1417\n",
            "176/448, train_loss: 0.3105\n",
            "177/448, train_loss: 0.2223\n",
            "178/448, train_loss: 0.1575\n",
            "179/448, train_loss: 0.0985\n",
            "180/448, train_loss: 0.1038\n",
            "181/448, train_loss: 0.1071\n",
            "182/448, train_loss: 0.1865\n",
            "183/448, train_loss: 0.1438\n",
            "184/448, train_loss: 0.1233\n",
            "185/448, train_loss: 0.1271\n",
            "186/448, train_loss: 0.2231\n",
            "187/448, train_loss: 0.2717\n",
            "188/448, train_loss: 0.1161\n",
            "189/448, train_loss: 0.1616\n",
            "190/448, train_loss: 0.1431\n",
            "191/448, train_loss: 0.1969\n",
            "192/448, train_loss: 0.1946\n",
            "193/448, train_loss: 0.1443\n",
            "194/448, train_loss: 0.1380\n",
            "195/448, train_loss: 0.1783\n",
            "196/448, train_loss: 0.1405\n",
            "197/448, train_loss: 0.1708\n",
            "198/448, train_loss: 0.2127\n",
            "199/448, train_loss: 0.2546\n",
            "200/448, train_loss: 0.1830\n",
            "201/448, train_loss: 0.1328\n",
            "202/448, train_loss: 0.1743\n",
            "203/448, train_loss: 0.2708\n",
            "204/448, train_loss: 0.1877\n",
            "205/448, train_loss: 0.1771\n",
            "206/448, train_loss: 0.2155\n",
            "207/448, train_loss: 0.1428\n",
            "208/448, train_loss: 0.1507\n",
            "209/448, train_loss: 0.2018\n",
            "210/448, train_loss: 0.1624\n",
            "211/448, train_loss: 0.1119\n",
            "212/448, train_loss: 0.1007\n",
            "213/448, train_loss: 0.4000\n",
            "214/448, train_loss: 0.1955\n",
            "215/448, train_loss: 0.2110\n",
            "216/448, train_loss: 0.1707\n",
            "217/448, train_loss: 0.1569\n",
            "218/448, train_loss: 0.2052\n",
            "219/448, train_loss: 0.1397\n",
            "220/448, train_loss: 0.1694\n",
            "221/448, train_loss: 0.1217\n",
            "222/448, train_loss: 0.1521\n",
            "223/448, train_loss: 0.3801\n",
            "224/448, train_loss: 0.1387\n",
            "225/448, train_loss: 0.2178\n",
            "226/448, train_loss: 0.0989\n",
            "227/448, train_loss: 0.1922\n",
            "228/448, train_loss: 0.1933\n",
            "229/448, train_loss: 0.1142\n",
            "230/448, train_loss: 0.3285\n",
            "231/448, train_loss: 0.2071\n",
            "232/448, train_loss: 0.0967\n",
            "233/448, train_loss: 0.0848\n",
            "234/448, train_loss: 0.2591\n",
            "235/448, train_loss: 0.2035\n",
            "236/448, train_loss: 0.1806\n",
            "237/448, train_loss: 0.1356\n",
            "238/448, train_loss: 0.1148\n",
            "239/448, train_loss: 0.1597\n",
            "240/448, train_loss: 0.2242\n",
            "241/448, train_loss: 0.1460\n",
            "242/448, train_loss: 0.1110\n",
            "243/448, train_loss: 0.1285\n",
            "244/448, train_loss: 0.1158\n",
            "245/448, train_loss: 0.1684\n",
            "246/448, train_loss: 0.1635\n",
            "247/448, train_loss: 0.2410\n",
            "248/448, train_loss: 0.1460\n",
            "249/448, train_loss: 0.1004\n",
            "250/448, train_loss: 0.2620\n",
            "251/448, train_loss: 0.1582\n",
            "252/448, train_loss: 0.1341\n",
            "253/448, train_loss: 0.1795\n",
            "254/448, train_loss: 0.1382\n",
            "255/448, train_loss: 0.1497\n",
            "256/448, train_loss: 0.1565\n",
            "257/448, train_loss: 0.1259\n",
            "258/448, train_loss: 0.1305\n",
            "259/448, train_loss: 0.1586\n",
            "260/448, train_loss: 0.1733\n",
            "261/448, train_loss: 0.1953\n",
            "262/448, train_loss: 0.1048\n",
            "263/448, train_loss: 0.0896\n",
            "264/448, train_loss: 0.0733\n",
            "265/448, train_loss: 0.1881\n",
            "266/448, train_loss: 0.1327\n",
            "267/448, train_loss: 0.1477\n",
            "268/448, train_loss: 0.0697\n",
            "269/448, train_loss: 0.3116\n",
            "270/448, train_loss: 0.1468\n",
            "271/448, train_loss: 0.1827\n",
            "272/448, train_loss: 0.2706\n",
            "273/448, train_loss: 0.1781\n",
            "274/448, train_loss: 0.1206\n",
            "275/448, train_loss: 0.2446\n",
            "276/448, train_loss: 0.2811\n",
            "277/448, train_loss: 0.1450\n",
            "278/448, train_loss: 0.2343\n",
            "279/448, train_loss: 0.1935\n",
            "280/448, train_loss: 0.1678\n",
            "281/448, train_loss: 0.1248\n",
            "282/448, train_loss: 0.1480\n",
            "283/448, train_loss: 0.1074\n",
            "284/448, train_loss: 0.0985\n",
            "285/448, train_loss: 0.1352\n",
            "286/448, train_loss: 0.1936\n",
            "287/448, train_loss: 0.0742\n",
            "288/448, train_loss: 0.1827\n",
            "289/448, train_loss: 0.2365\n",
            "290/448, train_loss: 0.1776\n",
            "291/448, train_loss: 0.1293\n",
            "292/448, train_loss: 0.1064\n",
            "293/448, train_loss: 0.5006\n",
            "294/448, train_loss: 0.1318\n",
            "295/448, train_loss: 0.2501\n",
            "296/448, train_loss: 0.2871\n",
            "297/448, train_loss: 0.2051\n",
            "298/448, train_loss: 0.2992\n",
            "299/448, train_loss: 0.1383\n",
            "300/448, train_loss: 0.1387\n",
            "301/448, train_loss: 0.2363\n",
            "302/448, train_loss: 0.1515\n",
            "303/448, train_loss: 0.0905\n",
            "304/448, train_loss: 0.1696\n",
            "305/448, train_loss: 0.1172\n",
            "306/448, train_loss: 0.1479\n",
            "307/448, train_loss: 0.1857\n",
            "308/448, train_loss: 0.2098\n",
            "309/448, train_loss: 0.2854\n",
            "310/448, train_loss: 0.1403\n",
            "311/448, train_loss: 0.1290\n",
            "312/448, train_loss: 0.1339\n",
            "313/448, train_loss: 0.1685\n",
            "314/448, train_loss: 0.2145\n",
            "315/448, train_loss: 0.1363\n",
            "316/448, train_loss: 0.1557\n",
            "317/448, train_loss: 0.0761\n",
            "318/448, train_loss: 0.1912\n",
            "319/448, train_loss: 0.0916\n",
            "320/448, train_loss: 0.1841\n",
            "321/448, train_loss: 0.1042\n",
            "322/448, train_loss: 0.1921\n",
            "323/448, train_loss: 0.2082\n",
            "324/448, train_loss: 0.1551\n",
            "325/448, train_loss: 0.1822\n",
            "326/448, train_loss: 0.1964\n",
            "327/448, train_loss: 0.1718\n",
            "328/448, train_loss: 0.1009\n",
            "329/448, train_loss: 0.2572\n",
            "330/448, train_loss: 0.0904\n",
            "331/448, train_loss: 0.2568\n",
            "332/448, train_loss: 0.1205\n",
            "333/448, train_loss: 0.1828\n",
            "334/448, train_loss: 0.1546\n",
            "335/448, train_loss: 0.1029\n",
            "336/448, train_loss: 0.1147\n",
            "337/448, train_loss: 0.1463\n",
            "338/448, train_loss: 0.1324\n",
            "339/448, train_loss: 0.1341\n",
            "340/448, train_loss: 0.1374\n",
            "341/448, train_loss: 0.1365\n",
            "342/448, train_loss: 0.1865\n",
            "343/448, train_loss: 0.3994\n",
            "344/448, train_loss: 0.3809\n",
            "345/448, train_loss: 0.1751\n",
            "346/448, train_loss: 0.1405\n",
            "347/448, train_loss: 0.1764\n",
            "348/448, train_loss: 0.1710\n",
            "349/448, train_loss: 0.1362\n",
            "350/448, train_loss: 0.1327\n",
            "351/448, train_loss: 0.1440\n",
            "352/448, train_loss: 0.1636\n",
            "353/448, train_loss: 0.3513\n",
            "354/448, train_loss: 0.2000\n",
            "355/448, train_loss: 0.1776\n",
            "356/448, train_loss: 0.1184\n",
            "357/448, train_loss: 0.1355\n",
            "358/448, train_loss: 0.2607\n",
            "359/448, train_loss: 0.1604\n",
            "360/448, train_loss: 0.1826\n",
            "361/448, train_loss: 0.1655\n",
            "362/448, train_loss: 0.1075\n",
            "363/448, train_loss: 0.1362\n",
            "364/448, train_loss: 0.1821\n",
            "365/448, train_loss: 0.0665\n",
            "366/448, train_loss: 0.1279\n",
            "367/448, train_loss: 0.1115\n",
            "368/448, train_loss: 0.1976\n",
            "369/448, train_loss: 0.1455\n",
            "370/448, train_loss: 0.1346\n",
            "371/448, train_loss: 0.2086\n",
            "372/448, train_loss: 0.1750\n",
            "373/448, train_loss: 0.1651\n",
            "374/448, train_loss: 0.0561\n",
            "375/448, train_loss: 0.1711\n",
            "376/448, train_loss: 0.2443\n",
            "377/448, train_loss: 0.1279\n",
            "378/448, train_loss: 0.1907\n",
            "379/448, train_loss: 0.1612\n",
            "380/448, train_loss: 0.1542\n",
            "381/448, train_loss: 0.1754\n",
            "382/448, train_loss: 0.1838\n",
            "383/448, train_loss: 0.1409\n",
            "384/448, train_loss: 0.1581\n",
            "385/448, train_loss: 0.2068\n",
            "386/448, train_loss: 0.4189\n",
            "387/448, train_loss: 0.1491\n",
            "388/448, train_loss: 0.1771\n",
            "389/448, train_loss: 0.0950\n",
            "390/448, train_loss: 0.1372\n",
            "391/448, train_loss: 0.1732\n",
            "392/448, train_loss: 0.1346\n",
            "393/448, train_loss: 0.1884\n",
            "394/448, train_loss: 0.2974\n",
            "395/448, train_loss: 0.1932\n",
            "396/448, train_loss: 0.1444\n",
            "397/448, train_loss: 0.2224\n",
            "398/448, train_loss: 0.2072\n",
            "399/448, train_loss: 0.2884\n",
            "400/448, train_loss: 0.1485\n",
            "401/448, train_loss: 0.1481\n",
            "402/448, train_loss: 0.1385\n",
            "403/448, train_loss: 0.1537\n",
            "404/448, train_loss: 0.2354\n",
            "405/448, train_loss: 0.1703\n",
            "406/448, train_loss: 0.1252\n",
            "407/448, train_loss: 0.2435\n",
            "408/448, train_loss: 0.1428\n",
            "409/448, train_loss: 0.1386\n",
            "410/448, train_loss: 0.0987\n",
            "411/448, train_loss: 0.1863\n",
            "412/448, train_loss: 0.2780\n",
            "413/448, train_loss: 0.1260\n",
            "414/448, train_loss: 0.2338\n",
            "415/448, train_loss: 0.1399\n",
            "416/448, train_loss: 0.1771\n",
            "417/448, train_loss: 0.2027\n",
            "418/448, train_loss: 0.2153\n",
            "419/448, train_loss: 0.1624\n",
            "420/448, train_loss: 0.1548\n",
            "421/448, train_loss: 0.1270\n",
            "422/448, train_loss: 0.1807\n",
            "423/448, train_loss: 0.2458\n",
            "424/448, train_loss: 0.1612\n",
            "425/448, train_loss: 0.1401\n",
            "426/448, train_loss: 0.1342\n",
            "427/448, train_loss: 0.1501\n",
            "428/448, train_loss: 0.1821\n",
            "429/448, train_loss: 0.2304\n",
            "430/448, train_loss: 0.2043\n",
            "431/448, train_loss: 0.1071\n",
            "432/448, train_loss: 0.2478\n",
            "433/448, train_loss: 0.1135\n",
            "434/448, train_loss: 0.1373\n",
            "435/448, train_loss: 0.2450\n",
            "436/448, train_loss: 0.1027\n",
            "437/448, train_loss: 0.2230\n",
            "438/448, train_loss: 0.0688\n",
            "439/448, train_loss: 0.0716\n",
            "440/448, train_loss: 0.1219\n",
            "441/448, train_loss: 0.1783\n",
            "442/448, train_loss: 0.2664\n",
            "443/448, train_loss: 0.1754\n",
            "444/448, train_loss: 0.1564\n",
            "445/448, train_loss: 0.1569\n",
            "446/448, train_loss: 0.2343\n",
            "447/448, train_loss: 0.2112\n",
            "448/448, train_loss: 0.2089\n",
            "449/448, train_loss: 0.1249\n",
            "epoch 41 average loss: 0.1769\n",
            "current epoch: 41 current mean dice: 0.8306 best mean dice: 0.8333 at epoch: 29\n",
            "----------\n",
            "epoch 42/50\n",
            "1/448, train_loss: 0.0954\n",
            "2/448, train_loss: 0.1023\n",
            "3/448, train_loss: 0.0986\n",
            "4/448, train_loss: 0.1288\n",
            "5/448, train_loss: 0.1854\n",
            "6/448, train_loss: 0.2595\n",
            "7/448, train_loss: 0.0963\n",
            "8/448, train_loss: 0.1687\n",
            "9/448, train_loss: 0.1707\n",
            "10/448, train_loss: 0.1340\n",
            "11/448, train_loss: 0.1283\n",
            "12/448, train_loss: 0.1367\n",
            "13/448, train_loss: 0.1349\n",
            "14/448, train_loss: 0.1848\n",
            "15/448, train_loss: 0.2220\n",
            "16/448, train_loss: 0.1363\n",
            "17/448, train_loss: 0.2649\n",
            "18/448, train_loss: 0.0738\n",
            "19/448, train_loss: 0.1994\n",
            "20/448, train_loss: 0.1923\n",
            "21/448, train_loss: 0.2266\n",
            "22/448, train_loss: 0.1309\n",
            "23/448, train_loss: 0.1538\n",
            "24/448, train_loss: 0.1663\n",
            "25/448, train_loss: 0.1571\n",
            "26/448, train_loss: 0.1310\n",
            "27/448, train_loss: 0.1222\n",
            "28/448, train_loss: 0.2133\n",
            "29/448, train_loss: 0.0631\n",
            "30/448, train_loss: 0.1670\n",
            "31/448, train_loss: 0.2655\n",
            "32/448, train_loss: 0.2716\n",
            "33/448, train_loss: 0.1638\n",
            "34/448, train_loss: 0.1862\n",
            "35/448, train_loss: 0.1120\n",
            "36/448, train_loss: 0.1659\n",
            "37/448, train_loss: 0.1540\n",
            "38/448, train_loss: 0.1598\n",
            "39/448, train_loss: 0.1538\n",
            "40/448, train_loss: 0.1314\n",
            "41/448, train_loss: 0.1558\n",
            "42/448, train_loss: 0.1784\n",
            "43/448, train_loss: 0.2645\n",
            "44/448, train_loss: 0.1334\n",
            "45/448, train_loss: 0.1589\n",
            "46/448, train_loss: 0.1291\n",
            "47/448, train_loss: 0.2297\n",
            "48/448, train_loss: 0.1684\n",
            "49/448, train_loss: 0.1375\n",
            "50/448, train_loss: 0.1311\n",
            "51/448, train_loss: 0.1059\n",
            "52/448, train_loss: 0.1213\n",
            "53/448, train_loss: 0.1515\n",
            "54/448, train_loss: 0.1473\n",
            "55/448, train_loss: 0.0610\n",
            "56/448, train_loss: 0.1687\n",
            "57/448, train_loss: 0.1558\n",
            "58/448, train_loss: 0.3298\n",
            "59/448, train_loss: 0.2053\n",
            "60/448, train_loss: 0.1481\n",
            "61/448, train_loss: 0.1870\n",
            "62/448, train_loss: 0.1301\n",
            "63/448, train_loss: 0.1778\n",
            "64/448, train_loss: 0.1352\n",
            "65/448, train_loss: 0.1345\n",
            "66/448, train_loss: 0.1506\n",
            "67/448, train_loss: 0.1322\n",
            "68/448, train_loss: 0.1734\n",
            "69/448, train_loss: 0.2371\n",
            "70/448, train_loss: 0.1420\n",
            "71/448, train_loss: 0.1860\n",
            "72/448, train_loss: 0.2063\n",
            "73/448, train_loss: 0.2619\n",
            "74/448, train_loss: 0.1499\n",
            "75/448, train_loss: 0.2256\n",
            "76/448, train_loss: 0.1613\n",
            "77/448, train_loss: 0.1767\n",
            "78/448, train_loss: 0.1681\n",
            "79/448, train_loss: 0.1061\n",
            "80/448, train_loss: 0.1588\n",
            "81/448, train_loss: 0.1611\n",
            "82/448, train_loss: 0.3308\n",
            "83/448, train_loss: 0.1873\n",
            "84/448, train_loss: 0.1748\n",
            "85/448, train_loss: 0.1558\n",
            "86/448, train_loss: 0.1272\n",
            "87/448, train_loss: 0.1966\n",
            "88/448, train_loss: 0.2348\n",
            "89/448, train_loss: 0.1392\n",
            "90/448, train_loss: 0.1300\n",
            "91/448, train_loss: 0.1391\n",
            "92/448, train_loss: 0.1434\n",
            "93/448, train_loss: 0.2440\n",
            "94/448, train_loss: 0.3577\n",
            "95/448, train_loss: 0.2876\n",
            "96/448, train_loss: 0.1706\n",
            "97/448, train_loss: 0.1826\n",
            "98/448, train_loss: 0.1974\n",
            "99/448, train_loss: 0.2527\n",
            "100/448, train_loss: 0.2333\n",
            "101/448, train_loss: 0.2145\n",
            "102/448, train_loss: 0.2296\n",
            "103/448, train_loss: 0.1367\n",
            "104/448, train_loss: 0.1382\n",
            "105/448, train_loss: 0.1439\n",
            "106/448, train_loss: 0.1478\n",
            "107/448, train_loss: 0.1708\n",
            "108/448, train_loss: 0.2309\n",
            "109/448, train_loss: 0.1373\n",
            "110/448, train_loss: 0.2306\n",
            "111/448, train_loss: 0.1694\n",
            "112/448, train_loss: 0.1570\n",
            "113/448, train_loss: 0.3134\n",
            "114/448, train_loss: 0.1936\n",
            "115/448, train_loss: 0.1019\n",
            "116/448, train_loss: 0.2001\n",
            "117/448, train_loss: 0.1785\n",
            "118/448, train_loss: 0.2204\n",
            "119/448, train_loss: 0.2250\n",
            "120/448, train_loss: 0.1677\n",
            "121/448, train_loss: 0.1474\n",
            "122/448, train_loss: 0.1680\n",
            "123/448, train_loss: 0.1706\n",
            "124/448, train_loss: 0.1631\n",
            "125/448, train_loss: 0.1697\n",
            "126/448, train_loss: 0.1441\n",
            "127/448, train_loss: 0.1749\n",
            "128/448, train_loss: 0.1445\n",
            "129/448, train_loss: 0.1723\n",
            "130/448, train_loss: 0.1807\n",
            "131/448, train_loss: 0.1497\n",
            "132/448, train_loss: 0.1464\n",
            "133/448, train_loss: 0.1292\n",
            "134/448, train_loss: 0.2043\n",
            "135/448, train_loss: 0.1686\n",
            "136/448, train_loss: 0.1633\n",
            "137/448, train_loss: 0.1928\n",
            "138/448, train_loss: 0.1621\n",
            "139/448, train_loss: 0.1489\n",
            "140/448, train_loss: 0.0936\n",
            "141/448, train_loss: 0.1665\n",
            "142/448, train_loss: 0.1006\n",
            "143/448, train_loss: 0.2712\n",
            "144/448, train_loss: 0.2245\n",
            "145/448, train_loss: 0.1084\n",
            "146/448, train_loss: 0.1048\n",
            "147/448, train_loss: 0.1222\n",
            "148/448, train_loss: 0.0974\n",
            "149/448, train_loss: 0.2260\n",
            "150/448, train_loss: 0.1666\n",
            "151/448, train_loss: 0.2195\n",
            "152/448, train_loss: 0.1447\n",
            "153/448, train_loss: 0.1326\n",
            "154/448, train_loss: 0.1791\n",
            "155/448, train_loss: 0.1378\n",
            "156/448, train_loss: 0.1400\n",
            "157/448, train_loss: 0.1913\n",
            "158/448, train_loss: 0.0963\n",
            "159/448, train_loss: 0.1448\n",
            "160/448, train_loss: 0.1206\n",
            "161/448, train_loss: 0.0981\n",
            "162/448, train_loss: 0.2345\n",
            "163/448, train_loss: 0.1532\n",
            "164/448, train_loss: 0.2278\n",
            "165/448, train_loss: 0.0940\n",
            "166/448, train_loss: 0.1679\n",
            "167/448, train_loss: 0.2366\n",
            "168/448, train_loss: 0.2363\n",
            "169/448, train_loss: 0.1859\n",
            "170/448, train_loss: 0.1351\n",
            "171/448, train_loss: 0.1533\n",
            "172/448, train_loss: 0.2103\n",
            "173/448, train_loss: 0.2363\n",
            "174/448, train_loss: 0.1303\n",
            "175/448, train_loss: 0.1806\n",
            "176/448, train_loss: 0.1605\n",
            "177/448, train_loss: 0.1447\n",
            "178/448, train_loss: 0.0907\n",
            "179/448, train_loss: 0.1921\n",
            "180/448, train_loss: 0.1377\n",
            "181/448, train_loss: 0.1284\n",
            "182/448, train_loss: 0.1302\n",
            "183/448, train_loss: 0.1408\n",
            "184/448, train_loss: 0.4418\n",
            "185/448, train_loss: 0.1655\n",
            "186/448, train_loss: 0.1559\n",
            "187/448, train_loss: 0.1996\n",
            "188/448, train_loss: 0.2465\n",
            "189/448, train_loss: 0.2505\n",
            "190/448, train_loss: 0.1723\n",
            "191/448, train_loss: 0.1848\n",
            "192/448, train_loss: 0.0566\n",
            "193/448, train_loss: 0.1292\n",
            "194/448, train_loss: 0.1342\n",
            "195/448, train_loss: 0.1814\n",
            "196/448, train_loss: 0.1610\n",
            "197/448, train_loss: 0.1371\n",
            "198/448, train_loss: 0.2075\n",
            "199/448, train_loss: 0.1627\n",
            "200/448, train_loss: 0.0911\n",
            "201/448, train_loss: 0.2637\n",
            "202/448, train_loss: 0.1313\n",
            "203/448, train_loss: 0.2471\n",
            "204/448, train_loss: 0.2120\n",
            "205/448, train_loss: 0.2273\n",
            "206/448, train_loss: 0.1255\n",
            "207/448, train_loss: 0.1648\n",
            "208/448, train_loss: 0.1371\n",
            "209/448, train_loss: 0.1448\n",
            "210/448, train_loss: 0.1757\n",
            "211/448, train_loss: 0.4760\n",
            "212/448, train_loss: 0.1357\n",
            "213/448, train_loss: 0.1630\n",
            "214/448, train_loss: 0.1679\n",
            "215/448, train_loss: 0.1584\n",
            "216/448, train_loss: 0.1023\n",
            "217/448, train_loss: 0.2337\n",
            "218/448, train_loss: 0.1539\n",
            "219/448, train_loss: 0.1350\n",
            "220/448, train_loss: 0.2493\n",
            "221/448, train_loss: 0.1321\n",
            "222/448, train_loss: 0.1083\n",
            "223/448, train_loss: 0.2579\n",
            "224/448, train_loss: 0.1565\n",
            "225/448, train_loss: 0.1380\n",
            "226/448, train_loss: 0.3296\n",
            "227/448, train_loss: 0.1968\n",
            "228/448, train_loss: 0.1629\n",
            "229/448, train_loss: 0.2074\n",
            "230/448, train_loss: 0.1789\n",
            "231/448, train_loss: 0.0879\n",
            "232/448, train_loss: 0.0908\n",
            "233/448, train_loss: 0.1790\n",
            "234/448, train_loss: 0.1717\n",
            "235/448, train_loss: 0.1355\n",
            "236/448, train_loss: 0.0620\n",
            "237/448, train_loss: 0.1289\n",
            "238/448, train_loss: 0.1042\n",
            "239/448, train_loss: 0.2345\n",
            "240/448, train_loss: 0.1332\n",
            "241/448, train_loss: 0.2025\n",
            "242/448, train_loss: 0.1786\n",
            "243/448, train_loss: 0.2143\n",
            "244/448, train_loss: 0.1915\n",
            "245/448, train_loss: 0.1539\n",
            "246/448, train_loss: 0.1322\n",
            "247/448, train_loss: 0.1404\n",
            "248/448, train_loss: 0.1780\n",
            "249/448, train_loss: 0.2726\n",
            "250/448, train_loss: 0.1559\n",
            "251/448, train_loss: 0.1761\n",
            "252/448, train_loss: 0.1393\n",
            "253/448, train_loss: 0.2890\n",
            "254/448, train_loss: 0.1337\n",
            "255/448, train_loss: 0.1357\n",
            "256/448, train_loss: 0.1576\n",
            "257/448, train_loss: 0.2570\n",
            "258/448, train_loss: 0.1265\n",
            "259/448, train_loss: 0.2040\n",
            "260/448, train_loss: 0.1028\n",
            "261/448, train_loss: 0.1656\n",
            "262/448, train_loss: 0.1455\n",
            "263/448, train_loss: 0.3058\n",
            "264/448, train_loss: 0.1642\n",
            "265/448, train_loss: 0.1546\n",
            "266/448, train_loss: 0.1415\n",
            "267/448, train_loss: 0.1574\n",
            "268/448, train_loss: 0.2345\n",
            "269/448, train_loss: 0.1784\n",
            "270/448, train_loss: 0.0827\n",
            "271/448, train_loss: 0.1591\n",
            "272/448, train_loss: 0.0775\n",
            "273/448, train_loss: 0.1364\n",
            "274/448, train_loss: 0.2473\n",
            "275/448, train_loss: 0.1348\n",
            "276/448, train_loss: 0.1550\n",
            "277/448, train_loss: 0.1428\n",
            "278/448, train_loss: 0.1325\n",
            "279/448, train_loss: 0.1000\n",
            "280/448, train_loss: 0.1729\n",
            "281/448, train_loss: 0.1509\n",
            "282/448, train_loss: 0.1010\n",
            "283/448, train_loss: 0.2493\n",
            "284/448, train_loss: 0.2305\n",
            "285/448, train_loss: 0.1896\n",
            "286/448, train_loss: 0.3557\n",
            "287/448, train_loss: 0.1678\n",
            "288/448, train_loss: 0.1540\n",
            "289/448, train_loss: 0.1651\n",
            "290/448, train_loss: 0.1180\n",
            "291/448, train_loss: 0.1594\n",
            "292/448, train_loss: 0.1710\n",
            "293/448, train_loss: 0.1903\n",
            "294/448, train_loss: 0.1890\n",
            "295/448, train_loss: 0.1666\n",
            "296/448, train_loss: 0.1278\n",
            "297/448, train_loss: 0.4014\n",
            "298/448, train_loss: 0.1567\n",
            "299/448, train_loss: 0.2059\n",
            "300/448, train_loss: 0.1755\n",
            "301/448, train_loss: 0.1166\n",
            "302/448, train_loss: 0.1328\n",
            "303/448, train_loss: 0.1129\n",
            "304/448, train_loss: 0.2288\n",
            "305/448, train_loss: 0.0984\n",
            "306/448, train_loss: 0.1331\n",
            "307/448, train_loss: 0.1241\n",
            "308/448, train_loss: 0.1780\n",
            "309/448, train_loss: 0.1394\n",
            "310/448, train_loss: 0.1713\n",
            "311/448, train_loss: 0.1785\n",
            "312/448, train_loss: 0.2138\n",
            "313/448, train_loss: 0.1397\n",
            "314/448, train_loss: 0.1285\n",
            "315/448, train_loss: 0.1996\n",
            "316/448, train_loss: 0.1517\n",
            "317/448, train_loss: 0.1428\n",
            "318/448, train_loss: 0.1646\n",
            "319/448, train_loss: 0.1237\n",
            "320/448, train_loss: 0.2684\n",
            "321/448, train_loss: 0.1711\n",
            "322/448, train_loss: 0.1095\n",
            "323/448, train_loss: 0.1132\n",
            "324/448, train_loss: 0.2103\n",
            "325/448, train_loss: 0.3596\n",
            "326/448, train_loss: 0.1272\n",
            "327/448, train_loss: 0.1349\n",
            "328/448, train_loss: 0.1325\n",
            "329/448, train_loss: 0.1803\n",
            "330/448, train_loss: 0.5103\n",
            "331/448, train_loss: 0.2734\n",
            "332/448, train_loss: 0.0941\n",
            "333/448, train_loss: 0.1404\n",
            "334/448, train_loss: 0.2473\n",
            "335/448, train_loss: 0.1364\n",
            "336/448, train_loss: 0.0937\n",
            "337/448, train_loss: 0.1133\n",
            "338/448, train_loss: 0.1853\n",
            "339/448, train_loss: 0.1581\n",
            "340/448, train_loss: 0.1468\n",
            "341/448, train_loss: 0.1672\n",
            "342/448, train_loss: 0.1636\n",
            "343/448, train_loss: 0.1451\n",
            "344/448, train_loss: 0.0643\n",
            "345/448, train_loss: 0.1404\n",
            "346/448, train_loss: 0.1298\n",
            "347/448, train_loss: 0.1000\n",
            "348/448, train_loss: 0.1444\n",
            "349/448, train_loss: 0.1603\n",
            "350/448, train_loss: 0.1953\n",
            "351/448, train_loss: 0.0948\n",
            "352/448, train_loss: 0.3696\n",
            "353/448, train_loss: 0.1189\n",
            "354/448, train_loss: 0.1918\n",
            "355/448, train_loss: 0.1488\n",
            "356/448, train_loss: 0.2468\n",
            "357/448, train_loss: 0.1816\n",
            "358/448, train_loss: 0.1283\n",
            "359/448, train_loss: 0.2161\n",
            "360/448, train_loss: 0.2348\n",
            "361/448, train_loss: 0.2124\n",
            "362/448, train_loss: 0.2343\n",
            "363/448, train_loss: 0.1860\n",
            "364/448, train_loss: 0.1374\n",
            "365/448, train_loss: 0.1522\n",
            "366/448, train_loss: 0.1459\n",
            "367/448, train_loss: 0.1403\n",
            "368/448, train_loss: 0.1843\n",
            "369/448, train_loss: 0.0759\n",
            "370/448, train_loss: 0.2130\n",
            "371/448, train_loss: 0.1001\n",
            "372/448, train_loss: 0.1545\n",
            "373/448, train_loss: 0.2089\n",
            "374/448, train_loss: 0.1563\n",
            "375/448, train_loss: 0.1787\n",
            "376/448, train_loss: 0.0943\n",
            "377/448, train_loss: 0.2077\n",
            "378/448, train_loss: 0.1527\n",
            "379/448, train_loss: 0.1606\n",
            "380/448, train_loss: 0.1341\n",
            "381/448, train_loss: 0.2329\n",
            "382/448, train_loss: 0.1663\n",
            "383/448, train_loss: 0.1642\n",
            "384/448, train_loss: 0.2456\n",
            "385/448, train_loss: 0.2117\n",
            "386/448, train_loss: 0.1615\n",
            "387/448, train_loss: 0.1261\n",
            "388/448, train_loss: 0.1713\n",
            "389/448, train_loss: 0.1626\n",
            "390/448, train_loss: 0.1350\n",
            "391/448, train_loss: 0.2080\n",
            "392/448, train_loss: 0.1635\n",
            "393/448, train_loss: 0.1288\n",
            "394/448, train_loss: 0.1779\n",
            "395/448, train_loss: 0.1439\n",
            "396/448, train_loss: 0.2212\n",
            "397/448, train_loss: 0.1368\n",
            "398/448, train_loss: 0.1879\n",
            "399/448, train_loss: 0.2113\n",
            "400/448, train_loss: 0.1163\n",
            "401/448, train_loss: 0.3832\n",
            "402/448, train_loss: 0.2141\n",
            "403/448, train_loss: 0.1631\n",
            "404/448, train_loss: 0.1356\n",
            "405/448, train_loss: 0.3897\n",
            "406/448, train_loss: 0.1626\n",
            "407/448, train_loss: 0.2220\n",
            "408/448, train_loss: 0.1182\n",
            "409/448, train_loss: 0.2852\n",
            "410/448, train_loss: 0.2523\n",
            "411/448, train_loss: 0.0910\n",
            "412/448, train_loss: 0.1415\n",
            "413/448, train_loss: 0.1517\n",
            "414/448, train_loss: 0.1102\n",
            "415/448, train_loss: 0.1788\n",
            "416/448, train_loss: 0.1334\n",
            "417/448, train_loss: 0.0980\n",
            "418/448, train_loss: 0.1700\n",
            "419/448, train_loss: 0.1768\n",
            "420/448, train_loss: 0.1082\n",
            "421/448, train_loss: 0.2656\n",
            "422/448, train_loss: 0.1094\n",
            "423/448, train_loss: 0.3646\n",
            "424/448, train_loss: 0.1887\n",
            "425/448, train_loss: 0.1530\n",
            "426/448, train_loss: 0.1843\n",
            "427/448, train_loss: 0.1737\n",
            "428/448, train_loss: 0.1291\n",
            "429/448, train_loss: 0.1855\n",
            "430/448, train_loss: 0.2746\n",
            "431/448, train_loss: 0.2818\n",
            "432/448, train_loss: 0.1531\n",
            "433/448, train_loss: 0.2542\n",
            "434/448, train_loss: 0.1303\n",
            "435/448, train_loss: 0.5559\n",
            "436/448, train_loss: 0.2122\n",
            "437/448, train_loss: 0.1334\n",
            "438/448, train_loss: 0.1997\n",
            "439/448, train_loss: 0.1286\n",
            "440/448, train_loss: 0.3033\n",
            "441/448, train_loss: 0.1016\n",
            "442/448, train_loss: 0.1386\n",
            "443/448, train_loss: 0.1641\n",
            "444/448, train_loss: 0.1562\n",
            "445/448, train_loss: 0.1615\n",
            "446/448, train_loss: 0.1215\n",
            "447/448, train_loss: 0.2300\n",
            "448/448, train_loss: 0.1282\n",
            "449/448, train_loss: 0.1285\n",
            "epoch 42 average loss: 0.1733\n",
            "current epoch: 42 current mean dice: 0.8313 best mean dice: 0.8333 at epoch: 29\n",
            "----------\n",
            "epoch 43/50\n",
            "1/448, train_loss: 0.2362\n",
            "2/448, train_loss: 0.1834\n",
            "3/448, train_loss: 0.1691\n",
            "4/448, train_loss: 0.2806\n",
            "5/448, train_loss: 0.2387\n",
            "6/448, train_loss: 0.1630\n",
            "7/448, train_loss: 0.1535\n",
            "8/448, train_loss: 0.2043\n",
            "9/448, train_loss: 0.1409\n",
            "10/448, train_loss: 0.1444\n",
            "11/448, train_loss: 0.1619\n",
            "12/448, train_loss: 0.2534\n",
            "13/448, train_loss: 0.2115\n",
            "14/448, train_loss: 0.3605\n",
            "15/448, train_loss: 0.1637\n",
            "16/448, train_loss: 0.1248\n",
            "17/448, train_loss: 0.1304\n",
            "18/448, train_loss: 0.1403\n",
            "19/448, train_loss: 0.1412\n",
            "20/448, train_loss: 0.1442\n",
            "21/448, train_loss: 0.0907\n",
            "22/448, train_loss: 0.1946\n",
            "23/448, train_loss: 0.2313\n",
            "24/448, train_loss: 0.1387\n",
            "25/448, train_loss: 0.1724\n",
            "26/448, train_loss: 0.1115\n",
            "27/448, train_loss: 0.1081\n",
            "28/448, train_loss: 0.1758\n",
            "29/448, train_loss: 0.2720\n",
            "30/448, train_loss: 0.0722\n",
            "31/448, train_loss: 0.1216\n",
            "32/448, train_loss: 0.1536\n",
            "33/448, train_loss: 0.2126\n",
            "34/448, train_loss: 0.4753\n",
            "35/448, train_loss: 0.1201\n",
            "36/448, train_loss: 0.1354\n",
            "37/448, train_loss: 0.1594\n",
            "38/448, train_loss: 0.1018\n",
            "39/448, train_loss: 0.1451\n",
            "40/448, train_loss: 0.2080\n",
            "41/448, train_loss: 0.1986\n",
            "42/448, train_loss: 0.1481\n",
            "43/448, train_loss: 0.0992\n",
            "44/448, train_loss: 0.1791\n",
            "45/448, train_loss: 0.1628\n",
            "46/448, train_loss: 0.4775\n",
            "47/448, train_loss: 0.1625\n",
            "48/448, train_loss: 0.1432\n",
            "49/448, train_loss: 0.1402\n",
            "50/448, train_loss: 0.2348\n",
            "51/448, train_loss: 0.1702\n",
            "52/448, train_loss: 0.1346\n",
            "53/448, train_loss: 0.2516\n",
            "54/448, train_loss: 0.1365\n",
            "55/448, train_loss: 0.1947\n",
            "56/448, train_loss: 0.2190\n",
            "57/448, train_loss: 0.1762\n",
            "58/448, train_loss: 0.2934\n",
            "59/448, train_loss: 0.1585\n",
            "60/448, train_loss: 0.1732\n",
            "61/448, train_loss: 0.2329\n",
            "62/448, train_loss: 0.1420\n",
            "63/448, train_loss: 0.0692\n",
            "64/448, train_loss: 0.1462\n",
            "65/448, train_loss: 0.1761\n",
            "66/448, train_loss: 0.2718\n",
            "67/448, train_loss: 0.1329\n",
            "68/448, train_loss: 0.0886\n",
            "69/448, train_loss: 0.3621\n",
            "70/448, train_loss: 0.1333\n",
            "71/448, train_loss: 0.2738\n",
            "72/448, train_loss: 0.1243\n",
            "73/448, train_loss: 0.1469\n",
            "74/448, train_loss: 0.2404\n",
            "75/448, train_loss: 0.1800\n",
            "76/448, train_loss: 0.1373\n",
            "77/448, train_loss: 0.1266\n",
            "78/448, train_loss: 0.1550\n",
            "79/448, train_loss: 0.3842\n",
            "80/448, train_loss: 0.1622\n",
            "81/448, train_loss: 0.0873\n",
            "82/448, train_loss: 0.1477\n",
            "83/448, train_loss: 0.2534\n",
            "84/448, train_loss: 0.2462\n",
            "85/448, train_loss: 0.2050\n",
            "86/448, train_loss: 0.1396\n",
            "87/448, train_loss: 0.1889\n",
            "88/448, train_loss: 0.1156\n",
            "89/448, train_loss: 0.1473\n",
            "90/448, train_loss: 0.1473\n",
            "91/448, train_loss: 0.1017\n",
            "92/448, train_loss: 0.1666\n",
            "93/448, train_loss: 0.2366\n",
            "94/448, train_loss: 0.1949\n",
            "95/448, train_loss: 0.1011\n",
            "96/448, train_loss: 0.1440\n",
            "97/448, train_loss: 0.1870\n",
            "98/448, train_loss: 0.2697\n",
            "99/448, train_loss: 0.0928\n",
            "100/448, train_loss: 0.2409\n",
            "101/448, train_loss: 0.1036\n",
            "102/448, train_loss: 0.1369\n",
            "103/448, train_loss: 0.1538\n",
            "104/448, train_loss: 0.1351\n",
            "105/448, train_loss: 0.2089\n",
            "106/448, train_loss: 0.1763\n",
            "107/448, train_loss: 0.1322\n",
            "108/448, train_loss: 0.1122\n",
            "109/448, train_loss: 0.1271\n",
            "110/448, train_loss: 0.1006\n",
            "111/448, train_loss: 0.1570\n",
            "112/448, train_loss: 0.2041\n",
            "113/448, train_loss: 0.1534\n",
            "114/448, train_loss: 0.1974\n",
            "115/448, train_loss: 0.2088\n",
            "116/448, train_loss: 0.1911\n",
            "117/448, train_loss: 0.0911\n",
            "118/448, train_loss: 0.1921\n",
            "119/448, train_loss: 0.1913\n",
            "120/448, train_loss: 0.1632\n",
            "121/448, train_loss: 0.1375\n",
            "122/448, train_loss: 0.1760\n",
            "123/448, train_loss: 0.1885\n",
            "124/448, train_loss: 0.4670\n",
            "125/448, train_loss: 0.1297\n",
            "126/448, train_loss: 0.2541\n",
            "127/448, train_loss: 0.2135\n",
            "128/448, train_loss: 0.1300\n",
            "129/448, train_loss: 0.1562\n",
            "130/448, train_loss: 0.2057\n",
            "131/448, train_loss: 0.1334\n",
            "132/448, train_loss: 0.0997\n",
            "133/448, train_loss: 0.2797\n",
            "134/448, train_loss: 0.1733\n",
            "135/448, train_loss: 0.1852\n",
            "136/448, train_loss: 0.3202\n",
            "137/448, train_loss: 0.2218\n",
            "138/448, train_loss: 0.1126\n",
            "139/448, train_loss: 0.1472\n",
            "140/448, train_loss: 0.1711\n",
            "141/448, train_loss: 0.2848\n",
            "142/448, train_loss: 0.2024\n",
            "143/448, train_loss: 0.1842\n",
            "144/448, train_loss: 0.1322\n",
            "145/448, train_loss: 0.0777\n",
            "146/448, train_loss: 0.1375\n",
            "147/448, train_loss: 0.1168\n",
            "148/448, train_loss: 0.1871\n",
            "149/448, train_loss: 0.1411\n",
            "150/448, train_loss: 0.2162\n",
            "151/448, train_loss: 0.1487\n",
            "152/448, train_loss: 0.0656\n",
            "153/448, train_loss: 0.1327\n",
            "154/448, train_loss: 0.1531\n",
            "155/448, train_loss: 0.1736\n",
            "156/448, train_loss: 0.1645\n",
            "157/448, train_loss: 0.1425\n",
            "158/448, train_loss: 0.0987\n",
            "159/448, train_loss: 0.0933\n",
            "160/448, train_loss: 0.1433\n",
            "161/448, train_loss: 0.2229\n",
            "162/448, train_loss: 0.2478\n",
            "163/448, train_loss: 0.2200\n",
            "164/448, train_loss: 0.1262\n",
            "165/448, train_loss: 0.2050\n",
            "166/448, train_loss: 0.1753\n",
            "167/448, train_loss: 0.1469\n",
            "168/448, train_loss: 0.1831\n",
            "169/448, train_loss: 0.1261\n",
            "170/448, train_loss: 0.2172\n",
            "171/448, train_loss: 0.2600\n",
            "172/448, train_loss: 0.1161\n",
            "173/448, train_loss: 0.1478\n",
            "174/448, train_loss: 0.1262\n",
            "175/448, train_loss: 0.1717\n",
            "176/448, train_loss: 0.2144\n",
            "177/448, train_loss: 0.2058\n",
            "178/448, train_loss: 0.1897\n",
            "179/448, train_loss: 0.1921\n",
            "180/448, train_loss: 0.1377\n",
            "181/448, train_loss: 0.1982\n",
            "182/448, train_loss: 0.1211\n",
            "183/448, train_loss: 0.1719\n",
            "184/448, train_loss: 0.2216\n",
            "185/448, train_loss: 0.1881\n",
            "186/448, train_loss: 0.1773\n",
            "187/448, train_loss: 0.1849\n",
            "188/448, train_loss: 0.1257\n",
            "189/448, train_loss: 0.1396\n",
            "190/448, train_loss: 0.1377\n",
            "191/448, train_loss: 0.2708\n",
            "192/448, train_loss: 0.1035\n",
            "193/448, train_loss: 0.1817\n",
            "194/448, train_loss: 0.1774\n",
            "195/448, train_loss: 0.1243\n",
            "196/448, train_loss: 0.1366\n",
            "197/448, train_loss: 0.1988\n",
            "198/448, train_loss: 0.0973\n",
            "199/448, train_loss: 0.1334\n",
            "200/448, train_loss: 0.0986\n",
            "201/448, train_loss: 0.1383\n",
            "202/448, train_loss: 0.1626\n",
            "203/448, train_loss: 0.1343\n",
            "204/448, train_loss: 0.1962\n",
            "205/448, train_loss: 0.2080\n",
            "206/448, train_loss: 0.3705\n",
            "207/448, train_loss: 0.1847\n",
            "208/448, train_loss: 0.1455\n",
            "209/448, train_loss: 0.1654\n",
            "210/448, train_loss: 0.2472\n",
            "211/448, train_loss: 0.2072\n",
            "212/448, train_loss: 0.1732\n",
            "213/448, train_loss: 0.0923\n",
            "214/448, train_loss: 0.1614\n",
            "215/448, train_loss: 0.1324\n",
            "216/448, train_loss: 0.1288\n",
            "217/448, train_loss: 0.1685\n",
            "218/448, train_loss: 0.1356\n",
            "219/448, train_loss: 0.1321\n",
            "220/448, train_loss: 0.2145\n",
            "221/448, train_loss: 0.1619\n",
            "222/448, train_loss: 0.1394\n",
            "223/448, train_loss: 0.1015\n",
            "224/448, train_loss: 0.2283\n",
            "225/448, train_loss: 0.1428\n",
            "226/448, train_loss: 0.1717\n",
            "227/448, train_loss: 0.2262\n",
            "228/448, train_loss: 0.1338\n",
            "229/448, train_loss: 0.1665\n",
            "230/448, train_loss: 0.1715\n",
            "231/448, train_loss: 0.2373\n",
            "232/448, train_loss: 0.2420\n",
            "233/448, train_loss: 0.1469\n",
            "234/448, train_loss: 0.2042\n",
            "235/448, train_loss: 0.1432\n",
            "236/448, train_loss: 0.1301\n",
            "237/448, train_loss: 0.1881\n",
            "238/448, train_loss: 0.1974\n",
            "239/448, train_loss: 0.2320\n",
            "240/448, train_loss: 0.1385\n",
            "241/448, train_loss: 0.1635\n",
            "242/448, train_loss: 0.1658\n",
            "243/448, train_loss: 0.4708\n",
            "244/448, train_loss: 0.0932\n",
            "245/448, train_loss: 0.1868\n",
            "246/448, train_loss: 0.1813\n",
            "247/448, train_loss: 0.1276\n",
            "248/448, train_loss: 0.1546\n",
            "249/448, train_loss: 0.1945\n",
            "250/448, train_loss: 0.2158\n",
            "251/448, train_loss: 0.1548\n",
            "252/448, train_loss: 0.1350\n",
            "253/448, train_loss: 0.2564\n",
            "254/448, train_loss: 0.1833\n",
            "255/448, train_loss: 0.1308\n",
            "256/448, train_loss: 0.1997\n",
            "257/448, train_loss: 0.0917\n",
            "258/448, train_loss: 0.2081\n",
            "259/448, train_loss: 0.1143\n",
            "260/448, train_loss: 0.3092\n",
            "261/448, train_loss: 0.1295\n",
            "262/448, train_loss: 0.1521\n",
            "263/448, train_loss: 0.2608\n",
            "264/448, train_loss: 0.1424\n",
            "265/448, train_loss: 0.1669\n",
            "266/448, train_loss: 0.2021\n",
            "267/448, train_loss: 0.1079\n",
            "268/448, train_loss: 0.1895\n",
            "269/448, train_loss: 0.1707\n",
            "270/448, train_loss: 0.2677\n",
            "271/448, train_loss: 0.1066\n",
            "272/448, train_loss: 0.1803\n",
            "273/448, train_loss: 0.3500\n",
            "274/448, train_loss: 0.1666\n",
            "275/448, train_loss: 0.2042\n",
            "276/448, train_loss: 0.2032\n",
            "277/448, train_loss: 0.1944\n",
            "278/448, train_loss: 0.2088\n",
            "279/448, train_loss: 0.4115\n",
            "280/448, train_loss: 0.1746\n",
            "281/448, train_loss: 0.0935\n",
            "282/448, train_loss: 0.1782\n",
            "283/448, train_loss: 0.2248\n",
            "284/448, train_loss: 0.1590\n",
            "285/448, train_loss: 0.1934\n",
            "286/448, train_loss: 0.1629\n",
            "287/448, train_loss: 0.1591\n",
            "288/448, train_loss: 0.1571\n",
            "289/448, train_loss: 0.1362\n",
            "290/448, train_loss: 0.1356\n",
            "291/448, train_loss: 0.1781\n",
            "292/448, train_loss: 0.1892\n",
            "293/448, train_loss: 0.2616\n",
            "294/448, train_loss: 0.1385\n",
            "295/448, train_loss: 0.0998\n",
            "296/448, train_loss: 0.1306\n",
            "297/448, train_loss: 0.1468\n",
            "298/448, train_loss: 0.1666\n",
            "299/448, train_loss: 0.2990\n",
            "300/448, train_loss: 0.1430\n",
            "301/448, train_loss: 0.1748\n",
            "302/448, train_loss: 0.1301\n",
            "303/448, train_loss: 0.1055\n",
            "304/448, train_loss: 0.1209\n",
            "305/448, train_loss: 0.2248\n",
            "306/448, train_loss: 0.3464\n",
            "307/448, train_loss: 0.1419\n",
            "308/448, train_loss: 0.2090\n",
            "309/448, train_loss: 0.1712\n",
            "310/448, train_loss: 0.0991\n",
            "311/448, train_loss: 0.1602\n",
            "312/448, train_loss: 0.1713\n",
            "313/448, train_loss: 0.2504\n",
            "314/448, train_loss: 0.1847\n",
            "315/448, train_loss: 0.1546\n",
            "316/448, train_loss: 0.1923\n",
            "317/448, train_loss: 0.2405\n",
            "318/448, train_loss: 0.1591\n",
            "319/448, train_loss: 0.1934\n",
            "320/448, train_loss: 0.1461\n",
            "321/448, train_loss: 0.1757\n",
            "322/448, train_loss: 0.3060\n",
            "323/448, train_loss: 0.3365\n",
            "324/448, train_loss: 0.2386\n",
            "325/448, train_loss: 0.0917\n",
            "326/448, train_loss: 0.1253\n",
            "327/448, train_loss: 0.1857\n",
            "328/448, train_loss: 0.4701\n",
            "329/448, train_loss: 0.1477\n",
            "330/448, train_loss: 0.2216\n",
            "331/448, train_loss: 0.1342\n",
            "332/448, train_loss: 0.1006\n",
            "333/448, train_loss: 0.1826\n",
            "334/448, train_loss: 0.0706\n",
            "335/448, train_loss: 0.2964\n",
            "336/448, train_loss: 0.1604\n",
            "337/448, train_loss: 0.2898\n",
            "338/448, train_loss: 0.1073\n",
            "339/448, train_loss: 0.1326\n",
            "340/448, train_loss: 0.1386\n",
            "341/448, train_loss: 0.2029\n",
            "342/448, train_loss: 0.2062\n",
            "343/448, train_loss: 0.1655\n",
            "344/448, train_loss: 0.0888\n",
            "345/448, train_loss: 0.1715\n",
            "346/448, train_loss: 0.1458\n",
            "347/448, train_loss: 0.1319\n",
            "348/448, train_loss: 0.0978\n",
            "349/448, train_loss: 0.0732\n",
            "350/448, train_loss: 0.0951\n",
            "351/448, train_loss: 0.2394\n",
            "352/448, train_loss: 0.2144\n",
            "353/448, train_loss: 0.1550\n",
            "354/448, train_loss: 0.0955\n",
            "355/448, train_loss: 0.4183\n",
            "356/448, train_loss: 0.1734\n",
            "357/448, train_loss: 0.2326\n",
            "358/448, train_loss: 0.1006\n",
            "359/448, train_loss: 0.0747\n",
            "360/448, train_loss: 0.1321\n",
            "361/448, train_loss: 0.1378\n",
            "362/448, train_loss: 0.1582\n",
            "363/448, train_loss: 0.2012\n",
            "364/448, train_loss: 0.1117\n",
            "365/448, train_loss: 0.1435\n",
            "366/448, train_loss: 0.1995\n",
            "367/448, train_loss: 0.1237\n",
            "368/448, train_loss: 0.1424\n",
            "369/448, train_loss: 0.1140\n",
            "370/448, train_loss: 0.2600\n",
            "371/448, train_loss: 0.0977\n",
            "372/448, train_loss: 0.1330\n",
            "373/448, train_loss: 0.2455\n",
            "374/448, train_loss: 0.0953\n",
            "375/448, train_loss: 0.1444\n",
            "376/448, train_loss: 0.2784\n",
            "377/448, train_loss: 0.1679\n",
            "378/448, train_loss: 0.0575\n",
            "379/448, train_loss: 0.1329\n",
            "380/448, train_loss: 0.2068\n",
            "381/448, train_loss: 0.1753\n",
            "382/448, train_loss: 0.1587\n",
            "383/448, train_loss: 0.1759\n",
            "384/448, train_loss: 0.1450\n",
            "385/448, train_loss: 0.1202\n",
            "386/448, train_loss: 0.1841\n",
            "387/448, train_loss: 0.1613\n",
            "388/448, train_loss: 0.1378\n",
            "389/448, train_loss: 0.1655\n",
            "390/448, train_loss: 0.0903\n",
            "391/448, train_loss: 0.2834\n",
            "392/448, train_loss: 0.0982\n",
            "393/448, train_loss: 0.1433\n",
            "394/448, train_loss: 0.1942\n",
            "395/448, train_loss: 0.1097\n",
            "396/448, train_loss: 0.1284\n",
            "397/448, train_loss: 0.1373\n",
            "398/448, train_loss: 0.1045\n",
            "399/448, train_loss: 0.1706\n",
            "400/448, train_loss: 0.1649\n",
            "401/448, train_loss: 0.1222\n",
            "402/448, train_loss: 0.1771\n",
            "403/448, train_loss: 0.1803\n",
            "404/448, train_loss: 0.1633\n",
            "405/448, train_loss: 0.3532\n",
            "406/448, train_loss: 0.2467\n",
            "407/448, train_loss: 0.1336\n",
            "408/448, train_loss: 0.1937\n",
            "409/448, train_loss: 0.1925\n",
            "410/448, train_loss: 0.2584\n",
            "411/448, train_loss: 0.1473\n",
            "412/448, train_loss: 0.2299\n",
            "413/448, train_loss: 0.1681\n",
            "414/448, train_loss: 0.2640\n",
            "415/448, train_loss: 0.2098\n",
            "416/448, train_loss: 0.1320\n",
            "417/448, train_loss: 0.0690\n",
            "418/448, train_loss: 0.2194\n",
            "419/448, train_loss: 0.0997\n",
            "420/448, train_loss: 0.0926\n",
            "421/448, train_loss: 0.1663\n",
            "422/448, train_loss: 0.4737\n",
            "423/448, train_loss: 0.0865\n",
            "424/448, train_loss: 0.0952\n",
            "425/448, train_loss: 0.1628\n",
            "426/448, train_loss: 0.1362\n",
            "427/448, train_loss: 0.2083\n",
            "428/448, train_loss: 0.1299\n",
            "429/448, train_loss: 0.1706\n",
            "430/448, train_loss: 0.1882\n",
            "431/448, train_loss: 0.1877\n",
            "432/448, train_loss: 0.1888\n",
            "433/448, train_loss: 0.1477\n",
            "434/448, train_loss: 0.1819\n",
            "435/448, train_loss: 0.1773\n",
            "436/448, train_loss: 0.3862\n",
            "437/448, train_loss: 0.1288\n",
            "438/448, train_loss: 0.1303\n",
            "439/448, train_loss: 0.1349\n",
            "440/448, train_loss: 0.1283\n",
            "441/448, train_loss: 0.2448\n",
            "442/448, train_loss: 0.2063\n",
            "443/448, train_loss: 0.0974\n",
            "444/448, train_loss: 0.1853\n",
            "445/448, train_loss: 0.0984\n",
            "446/448, train_loss: 0.1539\n",
            "447/448, train_loss: 0.1669\n",
            "448/448, train_loss: 0.0933\n",
            "449/448, train_loss: 0.2704\n",
            "epoch 43 average loss: 0.1753\n",
            "current epoch: 43 current mean dice: 0.8317 best mean dice: 0.8333 at epoch: 29\n",
            "----------\n",
            "epoch 44/50\n",
            "1/448, train_loss: 0.1193\n",
            "2/448, train_loss: 0.1334\n",
            "3/448, train_loss: 0.2076\n",
            "4/448, train_loss: 0.2444\n",
            "5/448, train_loss: 0.1590\n",
            "6/448, train_loss: 0.4358\n",
            "7/448, train_loss: 0.1274\n",
            "8/448, train_loss: 0.0722\n",
            "9/448, train_loss: 0.1733\n",
            "10/448, train_loss: 0.2113\n",
            "11/448, train_loss: 0.1126\n",
            "12/448, train_loss: 0.1660\n",
            "13/448, train_loss: 0.1620\n",
            "14/448, train_loss: 0.1631\n",
            "15/448, train_loss: 0.1647\n",
            "16/448, train_loss: 0.1624\n",
            "17/448, train_loss: 0.2865\n",
            "18/448, train_loss: 0.1968\n",
            "19/448, train_loss: 0.2035\n",
            "20/448, train_loss: 0.2049\n",
            "21/448, train_loss: 0.1060\n",
            "22/448, train_loss: 0.1486\n",
            "23/448, train_loss: 0.2508\n",
            "24/448, train_loss: 0.1216\n",
            "25/448, train_loss: 0.1865\n",
            "26/448, train_loss: 0.2345\n",
            "27/448, train_loss: 0.1314\n",
            "28/448, train_loss: 0.1912\n",
            "29/448, train_loss: 0.1672\n",
            "30/448, train_loss: 0.1649\n",
            "31/448, train_loss: 0.1383\n",
            "32/448, train_loss: 0.1255\n",
            "33/448, train_loss: 0.1106\n",
            "34/448, train_loss: 0.0979\n",
            "35/448, train_loss: 0.2974\n",
            "36/448, train_loss: 0.0692\n",
            "37/448, train_loss: 0.1479\n",
            "38/448, train_loss: 0.3593\n",
            "39/448, train_loss: 0.1783\n",
            "40/448, train_loss: 0.1753\n",
            "41/448, train_loss: 0.1459\n",
            "42/448, train_loss: 0.1827\n",
            "43/448, train_loss: 0.1826\n",
            "44/448, train_loss: 0.1582\n",
            "45/448, train_loss: 0.3329\n",
            "46/448, train_loss: 0.1285\n",
            "47/448, train_loss: 0.0955\n",
            "48/448, train_loss: 0.1351\n",
            "49/448, train_loss: 0.1641\n",
            "50/448, train_loss: 0.1165\n",
            "51/448, train_loss: 0.1837\n",
            "52/448, train_loss: 0.1750\n",
            "53/448, train_loss: 0.1634\n",
            "54/448, train_loss: 0.2149\n",
            "55/448, train_loss: 0.1079\n",
            "56/448, train_loss: 0.1321\n",
            "57/448, train_loss: 0.1827\n",
            "58/448, train_loss: 0.1857\n",
            "59/448, train_loss: 0.1764\n",
            "60/448, train_loss: 0.1788\n",
            "61/448, train_loss: 0.1927\n",
            "62/448, train_loss: 0.1880\n",
            "63/448, train_loss: 0.1664\n",
            "64/448, train_loss: 0.0865\n",
            "65/448, train_loss: 0.2323\n",
            "66/448, train_loss: 0.1573\n",
            "67/448, train_loss: 0.2144\n",
            "68/448, train_loss: 0.1319\n",
            "69/448, train_loss: 0.1656\n",
            "70/448, train_loss: 0.1463\n",
            "71/448, train_loss: 0.1781\n",
            "72/448, train_loss: 0.1672\n",
            "73/448, train_loss: 0.2269\n",
            "74/448, train_loss: 0.1418\n",
            "75/448, train_loss: 0.1674\n",
            "76/448, train_loss: 0.1350\n",
            "77/448, train_loss: 0.1468\n",
            "78/448, train_loss: 0.1459\n",
            "79/448, train_loss: 0.1551\n",
            "80/448, train_loss: 0.1776\n",
            "81/448, train_loss: 0.1731\n",
            "82/448, train_loss: 0.1683\n",
            "83/448, train_loss: 0.1427\n",
            "84/448, train_loss: 0.1673\n",
            "85/448, train_loss: 0.1435\n",
            "86/448, train_loss: 0.1752\n",
            "87/448, train_loss: 0.1305\n",
            "88/448, train_loss: 0.0926\n",
            "89/448, train_loss: 0.1494\n",
            "90/448, train_loss: 0.2087\n",
            "91/448, train_loss: 0.1614\n",
            "92/448, train_loss: 0.1117\n",
            "93/448, train_loss: 0.2533\n",
            "94/448, train_loss: 0.2621\n",
            "95/448, train_loss: 0.1056\n",
            "96/448, train_loss: 0.1167\n",
            "97/448, train_loss: 0.1931\n",
            "98/448, train_loss: 0.1264\n",
            "99/448, train_loss: 0.1781\n",
            "100/448, train_loss: 0.1323\n",
            "101/448, train_loss: 0.1631\n",
            "102/448, train_loss: 0.1332\n",
            "103/448, train_loss: 0.1666\n",
            "104/448, train_loss: 0.1429\n",
            "105/448, train_loss: 0.3105\n",
            "106/448, train_loss: 0.2604\n",
            "107/448, train_loss: 0.2051\n",
            "108/448, train_loss: 0.1615\n",
            "109/448, train_loss: 0.2418\n",
            "110/448, train_loss: 0.1703\n",
            "111/448, train_loss: 0.1824\n",
            "112/448, train_loss: 0.1442\n",
            "113/448, train_loss: 0.0888\n",
            "114/448, train_loss: 0.1856\n",
            "115/448, train_loss: 0.1490\n",
            "116/448, train_loss: 0.3179\n",
            "117/448, train_loss: 0.1591\n",
            "118/448, train_loss: 0.1110\n",
            "119/448, train_loss: 0.1893\n",
            "120/448, train_loss: 0.1845\n",
            "121/448, train_loss: 0.1118\n",
            "122/448, train_loss: 0.2017\n",
            "123/448, train_loss: 0.1290\n",
            "124/448, train_loss: 0.1295\n",
            "125/448, train_loss: 0.2083\n",
            "126/448, train_loss: 0.1699\n",
            "127/448, train_loss: 0.2073\n",
            "128/448, train_loss: 0.1966\n",
            "129/448, train_loss: 0.0956\n",
            "130/448, train_loss: 0.0980\n",
            "131/448, train_loss: 0.2138\n",
            "132/448, train_loss: 0.0940\n",
            "133/448, train_loss: 0.1954\n",
            "134/448, train_loss: 0.1872\n",
            "135/448, train_loss: 0.2381\n",
            "136/448, train_loss: 0.2725\n",
            "137/448, train_loss: 0.1308\n",
            "138/448, train_loss: 0.1421\n",
            "139/448, train_loss: 0.0989\n",
            "140/448, train_loss: 0.0984\n",
            "141/448, train_loss: 0.1730\n",
            "142/448, train_loss: 0.1047\n",
            "143/448, train_loss: 0.0955\n",
            "144/448, train_loss: 0.1524\n",
            "145/448, train_loss: 0.1539\n",
            "146/448, train_loss: 0.4920\n",
            "147/448, train_loss: 0.2174\n",
            "148/448, train_loss: 0.1297\n",
            "149/448, train_loss: 0.1662\n",
            "150/448, train_loss: 0.1062\n",
            "151/448, train_loss: 0.1805\n",
            "152/448, train_loss: 0.0780\n",
            "153/448, train_loss: 0.2638\n",
            "154/448, train_loss: 0.1779\n",
            "155/448, train_loss: 0.1246\n",
            "156/448, train_loss: 0.2122\n",
            "157/448, train_loss: 0.1627\n",
            "158/448, train_loss: 0.1389\n",
            "159/448, train_loss: 0.2106\n",
            "160/448, train_loss: 0.2124\n",
            "161/448, train_loss: 0.1941\n",
            "162/448, train_loss: 0.1810\n",
            "163/448, train_loss: 0.1249\n",
            "164/448, train_loss: 0.1279\n",
            "165/448, train_loss: 0.2197\n",
            "166/448, train_loss: 0.1603\n",
            "167/448, train_loss: 0.3197\n",
            "168/448, train_loss: 0.0834\n",
            "169/448, train_loss: 0.2525\n",
            "170/448, train_loss: 0.1474\n",
            "171/448, train_loss: 0.2171\n",
            "172/448, train_loss: 0.1723\n",
            "173/448, train_loss: 0.1220\n",
            "174/448, train_loss: 0.2240\n",
            "175/448, train_loss: 0.1150\n",
            "176/448, train_loss: 0.1319\n",
            "177/448, train_loss: 0.1729\n",
            "178/448, train_loss: 0.1182\n",
            "179/448, train_loss: 0.1884\n",
            "180/448, train_loss: 0.1372\n",
            "181/448, train_loss: 0.1283\n",
            "182/448, train_loss: 0.1040\n",
            "183/448, train_loss: 0.1318\n",
            "184/448, train_loss: 0.1335\n",
            "185/448, train_loss: 0.1118\n",
            "186/448, train_loss: 0.1695\n",
            "187/448, train_loss: 0.1262\n",
            "188/448, train_loss: 0.1913\n",
            "189/448, train_loss: 0.1421\n",
            "190/448, train_loss: 0.0562\n",
            "191/448, train_loss: 0.1977\n",
            "192/448, train_loss: 0.2345\n",
            "193/448, train_loss: 0.2430\n",
            "194/448, train_loss: 0.2113\n",
            "195/448, train_loss: 0.2006\n",
            "196/448, train_loss: 0.2150\n",
            "197/448, train_loss: 0.1447\n",
            "198/448, train_loss: 0.1078\n",
            "199/448, train_loss: 0.1515\n",
            "200/448, train_loss: 0.1611\n",
            "201/448, train_loss: 0.1771\n",
            "202/448, train_loss: 0.1282\n",
            "203/448, train_loss: 0.2040\n",
            "204/448, train_loss: 0.1804\n",
            "205/448, train_loss: 0.2090\n",
            "206/448, train_loss: 0.1724\n",
            "207/448, train_loss: 0.1697\n",
            "208/448, train_loss: 0.1232\n",
            "209/448, train_loss: 0.2206\n",
            "210/448, train_loss: 0.2185\n",
            "211/448, train_loss: 0.1649\n",
            "212/448, train_loss: 0.1905\n",
            "213/448, train_loss: 0.1370\n",
            "214/448, train_loss: 0.1320\n",
            "215/448, train_loss: 0.1430\n",
            "216/448, train_loss: 0.1458\n",
            "217/448, train_loss: 0.1873\n",
            "218/448, train_loss: 0.1315\n",
            "219/448, train_loss: 0.1133\n",
            "220/448, train_loss: 0.1422\n",
            "221/448, train_loss: 0.1281\n",
            "222/448, train_loss: 0.0939\n",
            "223/448, train_loss: 0.1786\n",
            "224/448, train_loss: 0.1530\n",
            "225/448, train_loss: 0.2536\n",
            "226/448, train_loss: 0.1952\n",
            "227/448, train_loss: 0.1498\n",
            "228/448, train_loss: 0.2176\n",
            "229/448, train_loss: 0.1705\n",
            "230/448, train_loss: 0.2481\n",
            "231/448, train_loss: 0.1365\n",
            "232/448, train_loss: 0.1745\n",
            "233/448, train_loss: 0.3866\n",
            "234/448, train_loss: 0.1139\n",
            "235/448, train_loss: 0.2920\n",
            "236/448, train_loss: 0.1317\n",
            "237/448, train_loss: 0.2595\n",
            "238/448, train_loss: 0.1979\n",
            "239/448, train_loss: 0.0968\n",
            "240/448, train_loss: 0.2157\n",
            "241/448, train_loss: 0.1643\n",
            "242/448, train_loss: 0.1601\n",
            "243/448, train_loss: 0.1997\n",
            "244/448, train_loss: 0.1317\n",
            "245/448, train_loss: 0.1850\n",
            "246/448, train_loss: 0.3228\n",
            "247/448, train_loss: 0.1269\n",
            "248/448, train_loss: 0.1575\n",
            "249/448, train_loss: 0.1730\n",
            "250/448, train_loss: 0.1321\n",
            "251/448, train_loss: 0.2782\n",
            "252/448, train_loss: 0.1797\n",
            "253/448, train_loss: 0.0541\n",
            "254/448, train_loss: 0.2376\n",
            "255/448, train_loss: 0.2451\n",
            "256/448, train_loss: 0.1802\n",
            "257/448, train_loss: 0.2337\n",
            "258/448, train_loss: 0.1856\n",
            "259/448, train_loss: 0.2041\n",
            "260/448, train_loss: 0.0702\n",
            "261/448, train_loss: 0.1211\n",
            "262/448, train_loss: 0.0927\n",
            "263/448, train_loss: 0.2243\n",
            "264/448, train_loss: 0.1716\n",
            "265/448, train_loss: 0.1909\n",
            "266/448, train_loss: 0.1688\n",
            "267/448, train_loss: 0.2084\n",
            "268/448, train_loss: 0.1326\n",
            "269/448, train_loss: 0.1329\n",
            "270/448, train_loss: 0.1578\n",
            "271/448, train_loss: 0.1004\n",
            "272/448, train_loss: 0.1817\n",
            "273/448, train_loss: 0.2118\n",
            "274/448, train_loss: 0.1397\n",
            "275/448, train_loss: 0.1617\n",
            "276/448, train_loss: 0.0669\n",
            "277/448, train_loss: 0.2447\n",
            "278/448, train_loss: 0.2028\n",
            "279/448, train_loss: 0.1330\n",
            "280/448, train_loss: 0.1673\n",
            "281/448, train_loss: 0.2068\n",
            "282/448, train_loss: 0.1386\n",
            "283/448, train_loss: 0.3002\n",
            "284/448, train_loss: 0.1346\n",
            "285/448, train_loss: 0.2312\n",
            "286/448, train_loss: 0.0979\n",
            "287/448, train_loss: 0.1168\n",
            "288/448, train_loss: 0.1333\n",
            "289/448, train_loss: 0.2530\n",
            "290/448, train_loss: 0.1757\n",
            "291/448, train_loss: 0.1024\n",
            "292/448, train_loss: 0.1872\n",
            "293/448, train_loss: 0.1790\n",
            "294/448, train_loss: 0.1472\n",
            "295/448, train_loss: 0.2219\n",
            "296/448, train_loss: 0.1816\n",
            "297/448, train_loss: 0.0922\n",
            "298/448, train_loss: 0.2298\n",
            "299/448, train_loss: 0.1914\n",
            "300/448, train_loss: 0.2506\n",
            "301/448, train_loss: 0.0996\n",
            "302/448, train_loss: 0.1666\n",
            "303/448, train_loss: 0.2063\n",
            "304/448, train_loss: 0.0846\n",
            "305/448, train_loss: 0.1699\n",
            "306/448, train_loss: 0.2332\n",
            "307/448, train_loss: 0.1219\n",
            "308/448, train_loss: 0.0608\n",
            "309/448, train_loss: 0.2711\n",
            "310/448, train_loss: 0.1382\n",
            "311/448, train_loss: 0.1633\n",
            "312/448, train_loss: 0.1478\n",
            "313/448, train_loss: 0.1987\n",
            "314/448, train_loss: 0.1035\n",
            "315/448, train_loss: 0.1018\n",
            "316/448, train_loss: 0.3001\n",
            "317/448, train_loss: 0.2989\n",
            "318/448, train_loss: 0.1466\n",
            "319/448, train_loss: 0.2053\n",
            "320/448, train_loss: 0.2280\n",
            "321/448, train_loss: 0.4720\n",
            "322/448, train_loss: 0.1295\n",
            "323/448, train_loss: 0.1217\n",
            "324/448, train_loss: 0.1415\n",
            "325/448, train_loss: 0.1691\n",
            "326/448, train_loss: 0.2048\n",
            "327/448, train_loss: 0.1583\n",
            "328/448, train_loss: 0.1816\n",
            "329/448, train_loss: 0.1764\n",
            "330/448, train_loss: 0.1397\n",
            "331/448, train_loss: 0.1636\n",
            "332/448, train_loss: 0.1695\n",
            "333/448, train_loss: 0.2097\n",
            "334/448, train_loss: 0.1745\n",
            "335/448, train_loss: 0.1470\n",
            "336/448, train_loss: 0.2740\n",
            "337/448, train_loss: 0.1764\n",
            "338/448, train_loss: 0.1051\n",
            "339/448, train_loss: 0.1852\n",
            "340/448, train_loss: 0.1631\n",
            "341/448, train_loss: 0.1003\n",
            "342/448, train_loss: 0.1134\n",
            "343/448, train_loss: 0.1448\n",
            "344/448, train_loss: 0.1532\n",
            "345/448, train_loss: 0.1535\n",
            "346/448, train_loss: 0.1565\n",
            "347/448, train_loss: 0.1534\n",
            "348/448, train_loss: 0.1321\n",
            "349/448, train_loss: 0.2908\n",
            "350/448, train_loss: 0.0661\n",
            "351/448, train_loss: 0.2150\n",
            "352/448, train_loss: 0.1578\n",
            "353/448, train_loss: 0.1358\n",
            "354/448, train_loss: 0.1262\n",
            "355/448, train_loss: 0.2078\n",
            "356/448, train_loss: 0.1231\n",
            "357/448, train_loss: 0.2596\n",
            "358/448, train_loss: 0.2819\n",
            "359/448, train_loss: 0.0918\n",
            "360/448, train_loss: 0.1422\n",
            "361/448, train_loss: 0.1456\n",
            "362/448, train_loss: 0.1465\n",
            "363/448, train_loss: 0.1811\n",
            "364/448, train_loss: 0.1437\n",
            "365/448, train_loss: 0.2819\n",
            "366/448, train_loss: 0.1449\n",
            "367/448, train_loss: 0.1730\n",
            "368/448, train_loss: 0.0787\n",
            "369/448, train_loss: 0.1747\n",
            "370/448, train_loss: 0.2835\n",
            "371/448, train_loss: 0.1323\n",
            "372/448, train_loss: 0.1994\n",
            "373/448, train_loss: 0.2199\n",
            "374/448, train_loss: 0.1774\n",
            "375/448, train_loss: 0.1685\n",
            "376/448, train_loss: 0.1315\n",
            "377/448, train_loss: 0.2496\n",
            "378/448, train_loss: 0.1906\n",
            "379/448, train_loss: 0.2568\n",
            "380/448, train_loss: 0.1627\n",
            "381/448, train_loss: 0.1804\n",
            "382/448, train_loss: 0.1219\n",
            "383/448, train_loss: 0.1254\n",
            "384/448, train_loss: 0.1455\n",
            "385/448, train_loss: 0.1879\n",
            "386/448, train_loss: 0.1611\n",
            "387/448, train_loss: 0.4472\n",
            "388/448, train_loss: 0.0891\n",
            "389/448, train_loss: 0.1800\n",
            "390/448, train_loss: 0.1354\n",
            "391/448, train_loss: 0.0669\n",
            "392/448, train_loss: 0.2708\n",
            "393/448, train_loss: 0.2084\n",
            "394/448, train_loss: 0.2551\n",
            "395/448, train_loss: 0.1080\n",
            "396/448, train_loss: 0.1096\n",
            "397/448, train_loss: 0.1938\n",
            "398/448, train_loss: 0.2633\n",
            "399/448, train_loss: 0.1038\n",
            "400/448, train_loss: 0.2214\n",
            "401/448, train_loss: 0.0992\n",
            "402/448, train_loss: 0.1739\n",
            "403/448, train_loss: 0.1539\n",
            "404/448, train_loss: 0.1692\n",
            "405/448, train_loss: 0.1019\n",
            "406/448, train_loss: 0.2144\n",
            "407/448, train_loss: 0.2312\n",
            "408/448, train_loss: 0.2514\n",
            "409/448, train_loss: 0.2944\n",
            "410/448, train_loss: 0.1229\n",
            "411/448, train_loss: 0.1428\n",
            "412/448, train_loss: 0.1822\n",
            "413/448, train_loss: 0.1839\n",
            "414/448, train_loss: 0.1268\n",
            "415/448, train_loss: 0.1714\n",
            "416/448, train_loss: 0.2528\n",
            "417/448, train_loss: 0.1428\n",
            "418/448, train_loss: 0.2164\n",
            "419/448, train_loss: 0.4616\n",
            "420/448, train_loss: 0.1373\n",
            "421/448, train_loss: 0.1675\n",
            "422/448, train_loss: 0.1723\n",
            "423/448, train_loss: 0.1424\n",
            "424/448, train_loss: 0.1006\n",
            "425/448, train_loss: 0.1764\n",
            "426/448, train_loss: 0.1766\n",
            "427/448, train_loss: 0.2844\n",
            "428/448, train_loss: 0.1027\n",
            "429/448, train_loss: 0.1388\n",
            "430/448, train_loss: 0.1777\n",
            "431/448, train_loss: 0.2850\n",
            "432/448, train_loss: 0.1999\n",
            "433/448, train_loss: 0.1982\n",
            "434/448, train_loss: 0.1669\n",
            "435/448, train_loss: 0.1393\n",
            "436/448, train_loss: 0.1982\n",
            "437/448, train_loss: 0.1879\n",
            "438/448, train_loss: 0.2595\n",
            "439/448, train_loss: 0.2229\n",
            "440/448, train_loss: 0.1815\n",
            "441/448, train_loss: 0.1516\n",
            "442/448, train_loss: 0.1428\n",
            "443/448, train_loss: 0.1089\n",
            "444/448, train_loss: 0.1564\n",
            "445/448, train_loss: 0.1897\n",
            "446/448, train_loss: 0.1093\n",
            "447/448, train_loss: 0.2555\n",
            "448/448, train_loss: 0.1438\n",
            "449/448, train_loss: 0.1843\n",
            "epoch 44 average loss: 0.1736\n",
            "saved new best metric model\n",
            "current epoch: 44 current mean dice: 0.8339 best mean dice: 0.8339 at epoch: 44\n",
            "----------\n",
            "epoch 45/50\n",
            "1/448, train_loss: 0.1710\n",
            "2/448, train_loss: 0.1329\n",
            "3/448, train_loss: 0.1705\n",
            "4/448, train_loss: 0.1624\n",
            "5/448, train_loss: 0.1711\n",
            "6/448, train_loss: 0.1737\n",
            "7/448, train_loss: 0.1942\n",
            "8/448, train_loss: 0.1562\n",
            "9/448, train_loss: 0.2869\n",
            "10/448, train_loss: 0.3844\n",
            "11/448, train_loss: 0.1475\n",
            "12/448, train_loss: 0.1350\n",
            "13/448, train_loss: 0.2439\n",
            "14/448, train_loss: 0.1569\n",
            "15/448, train_loss: 0.1883\n",
            "16/448, train_loss: 0.1887\n",
            "17/448, train_loss: 0.1522\n",
            "18/448, train_loss: 0.1799\n",
            "19/448, train_loss: 0.1652\n",
            "20/448, train_loss: 0.1263\n",
            "21/448, train_loss: 0.1614\n",
            "22/448, train_loss: 0.1169\n",
            "23/448, train_loss: 0.0972\n",
            "24/448, train_loss: 0.2558\n",
            "25/448, train_loss: 0.0778\n",
            "26/448, train_loss: 0.4301\n",
            "27/448, train_loss: 0.1361\n",
            "28/448, train_loss: 0.1312\n",
            "29/448, train_loss: 0.1943\n",
            "30/448, train_loss: 0.1848\n",
            "31/448, train_loss: 0.1852\n",
            "32/448, train_loss: 0.1506\n",
            "33/448, train_loss: 0.0993\n",
            "34/448, train_loss: 0.1929\n",
            "35/448, train_loss: 0.1593\n",
            "36/448, train_loss: 0.2434\n",
            "37/448, train_loss: 0.2503\n",
            "38/448, train_loss: 0.1689\n",
            "39/448, train_loss: 0.1613\n",
            "40/448, train_loss: 0.1141\n",
            "41/448, train_loss: 0.1079\n",
            "42/448, train_loss: 0.1879\n",
            "43/448, train_loss: 0.1236\n",
            "44/448, train_loss: 0.2363\n",
            "45/448, train_loss: 0.1300\n",
            "46/448, train_loss: 0.1680\n",
            "47/448, train_loss: 0.1748\n",
            "48/448, train_loss: 0.1812\n",
            "49/448, train_loss: 0.2319\n",
            "50/448, train_loss: 0.1305\n",
            "51/448, train_loss: 0.1505\n",
            "52/448, train_loss: 0.1677\n",
            "53/448, train_loss: 0.1024\n",
            "54/448, train_loss: 0.1139\n",
            "55/448, train_loss: 0.1372\n",
            "56/448, train_loss: 0.1006\n",
            "57/448, train_loss: 0.1387\n",
            "58/448, train_loss: 0.1289\n",
            "59/448, train_loss: 0.1487\n",
            "60/448, train_loss: 0.1495\n",
            "61/448, train_loss: 0.1289\n",
            "62/448, train_loss: 0.0543\n",
            "63/448, train_loss: 0.1887\n",
            "64/448, train_loss: 0.1643\n",
            "65/448, train_loss: 0.1774\n",
            "66/448, train_loss: 0.1477\n",
            "67/448, train_loss: 0.0745\n",
            "68/448, train_loss: 0.1115\n",
            "69/448, train_loss: 0.1522\n",
            "70/448, train_loss: 0.1158\n",
            "71/448, train_loss: 0.2113\n",
            "72/448, train_loss: 0.4021\n",
            "73/448, train_loss: 0.1362\n",
            "74/448, train_loss: 0.1547\n",
            "75/448, train_loss: 0.1432\n",
            "76/448, train_loss: 0.1328\n",
            "77/448, train_loss: 0.2478\n",
            "78/448, train_loss: 0.0626\n",
            "79/448, train_loss: 0.1603\n",
            "80/448, train_loss: 0.1341\n",
            "81/448, train_loss: 0.0963\n",
            "82/448, train_loss: 0.1353\n",
            "83/448, train_loss: 0.1450\n",
            "84/448, train_loss: 0.1822\n",
            "85/448, train_loss: 0.1709\n",
            "86/448, train_loss: 0.1618\n",
            "87/448, train_loss: 0.2719\n",
            "88/448, train_loss: 0.1320\n",
            "89/448, train_loss: 0.1800\n",
            "90/448, train_loss: 0.1517\n",
            "91/448, train_loss: 0.1739\n",
            "92/448, train_loss: 0.1703\n",
            "93/448, train_loss: 0.2710\n",
            "94/448, train_loss: 0.2090\n",
            "95/448, train_loss: 0.1551\n",
            "96/448, train_loss: 0.2360\n",
            "97/448, train_loss: 0.1735\n",
            "98/448, train_loss: 0.2025\n",
            "99/448, train_loss: 0.1074\n",
            "100/448, train_loss: 0.2203\n",
            "101/448, train_loss: 0.1761\n",
            "102/448, train_loss: 0.1728\n",
            "103/448, train_loss: 0.1908\n",
            "104/448, train_loss: 0.2421\n",
            "105/448, train_loss: 0.1909\n",
            "106/448, train_loss: 0.2336\n",
            "107/448, train_loss: 0.1274\n",
            "108/448, train_loss: 0.1407\n",
            "109/448, train_loss: 0.2092\n",
            "110/448, train_loss: 0.1313\n",
            "111/448, train_loss: 0.1722\n",
            "112/448, train_loss: 0.2465\n",
            "113/448, train_loss: 0.1617\n",
            "114/448, train_loss: 0.2091\n",
            "115/448, train_loss: 0.2342\n",
            "116/448, train_loss: 0.2408\n",
            "117/448, train_loss: 0.2132\n",
            "118/448, train_loss: 0.1414\n",
            "119/448, train_loss: 0.1724\n",
            "120/448, train_loss: 0.2206\n",
            "121/448, train_loss: 0.2301\n",
            "122/448, train_loss: 0.1484\n",
            "123/448, train_loss: 0.1249\n",
            "124/448, train_loss: 0.1675\n",
            "125/448, train_loss: 0.1376\n",
            "126/448, train_loss: 0.1251\n",
            "127/448, train_loss: 0.1532\n",
            "128/448, train_loss: 0.0863\n",
            "129/448, train_loss: 0.2084\n",
            "130/448, train_loss: 0.1696\n",
            "131/448, train_loss: 0.1798\n",
            "132/448, train_loss: 0.1512\n",
            "133/448, train_loss: 0.2616\n",
            "134/448, train_loss: 0.2396\n",
            "135/448, train_loss: 0.1383\n",
            "136/448, train_loss: 0.0981\n",
            "137/448, train_loss: 0.1391\n",
            "138/448, train_loss: 0.2035\n",
            "139/448, train_loss: 0.1834\n",
            "140/448, train_loss: 0.0856\n",
            "141/448, train_loss: 0.1032\n",
            "142/448, train_loss: 0.1729\n",
            "143/448, train_loss: 0.0931\n",
            "144/448, train_loss: 0.1778\n",
            "145/448, train_loss: 0.1654\n",
            "146/448, train_loss: 0.1772\n",
            "147/448, train_loss: 0.1405\n",
            "148/448, train_loss: 0.1421\n",
            "149/448, train_loss: 0.1418\n",
            "150/448, train_loss: 0.1473\n",
            "151/448, train_loss: 0.1749\n",
            "152/448, train_loss: 0.2039\n",
            "153/448, train_loss: 0.1687\n",
            "154/448, train_loss: 0.1750\n",
            "155/448, train_loss: 0.1615\n",
            "156/448, train_loss: 0.2761\n",
            "157/448, train_loss: 0.1480\n",
            "158/448, train_loss: 0.1002\n",
            "159/448, train_loss: 0.1794\n",
            "160/448, train_loss: 0.1817\n",
            "161/448, train_loss: 0.1556\n",
            "162/448, train_loss: 0.1381\n",
            "163/448, train_loss: 0.1459\n",
            "164/448, train_loss: 0.1524\n",
            "165/448, train_loss: 0.2113\n",
            "166/448, train_loss: 0.1579\n",
            "167/448, train_loss: 0.1680\n",
            "168/448, train_loss: 0.1599\n",
            "169/448, train_loss: 0.1218\n",
            "170/448, train_loss: 0.1660\n",
            "171/448, train_loss: 0.1397\n",
            "172/448, train_loss: 0.1196\n",
            "173/448, train_loss: 0.1576\n",
            "174/448, train_loss: 0.3637\n",
            "175/448, train_loss: 0.1846\n",
            "176/448, train_loss: 0.1373\n",
            "177/448, train_loss: 0.0904\n",
            "178/448, train_loss: 0.1314\n",
            "179/448, train_loss: 0.1673\n",
            "180/448, train_loss: 0.2132\n",
            "181/448, train_loss: 0.1419\n",
            "182/448, train_loss: 0.1712\n",
            "183/448, train_loss: 0.1272\n",
            "184/448, train_loss: 0.1323\n",
            "185/448, train_loss: 0.1872\n",
            "186/448, train_loss: 0.2116\n",
            "187/448, train_loss: 0.1287\n",
            "188/448, train_loss: 0.1684\n",
            "189/448, train_loss: 0.1007\n",
            "190/448, train_loss: 0.1696\n",
            "191/448, train_loss: 0.3023\n",
            "192/448, train_loss: 0.1167\n",
            "193/448, train_loss: 0.1122\n",
            "194/448, train_loss: 0.1246\n",
            "195/448, train_loss: 0.0919\n",
            "196/448, train_loss: 0.1657\n",
            "197/448, train_loss: 0.1060\n",
            "198/448, train_loss: 0.1918\n",
            "199/448, train_loss: 0.2470\n",
            "200/448, train_loss: 0.2938\n",
            "201/448, train_loss: 0.1328\n",
            "202/448, train_loss: 0.1215\n",
            "203/448, train_loss: 0.1728\n",
            "204/448, train_loss: 0.1383\n",
            "205/448, train_loss: 0.1466\n",
            "206/448, train_loss: 0.1683\n",
            "207/448, train_loss: 0.2265\n",
            "208/448, train_loss: 0.1814\n",
            "209/448, train_loss: 0.1315\n",
            "210/448, train_loss: 0.2033\n",
            "211/448, train_loss: 0.1895\n",
            "212/448, train_loss: 0.1604\n",
            "213/448, train_loss: 0.1029\n",
            "214/448, train_loss: 0.2388\n",
            "215/448, train_loss: 0.0713\n",
            "216/448, train_loss: 0.1857\n",
            "217/448, train_loss: 0.1930\n",
            "218/448, train_loss: 0.2822\n",
            "219/448, train_loss: 0.2023\n",
            "220/448, train_loss: 0.1806\n",
            "221/448, train_loss: 0.1719\n",
            "222/448, train_loss: 0.1414\n",
            "223/448, train_loss: 0.2201\n",
            "224/448, train_loss: 0.1327\n",
            "225/448, train_loss: 0.1916\n",
            "226/448, train_loss: 0.1598\n",
            "227/448, train_loss: 0.1855\n",
            "228/448, train_loss: 0.2558\n",
            "229/448, train_loss: 0.2119\n",
            "230/448, train_loss: 0.2003\n",
            "231/448, train_loss: 0.1377\n",
            "232/448, train_loss: 0.1334\n",
            "233/448, train_loss: 0.1281\n",
            "234/448, train_loss: 0.1361\n",
            "235/448, train_loss: 0.2672\n",
            "236/448, train_loss: 0.1515\n",
            "237/448, train_loss: 0.1065\n",
            "238/448, train_loss: 0.1641\n",
            "239/448, train_loss: 0.1690\n",
            "240/448, train_loss: 0.1388\n",
            "241/448, train_loss: 0.1230\n",
            "242/448, train_loss: 0.1398\n",
            "243/448, train_loss: 0.1532\n",
            "244/448, train_loss: 0.1709\n",
            "245/448, train_loss: 0.1336\n",
            "246/448, train_loss: 0.2281\n",
            "247/448, train_loss: 0.1712\n",
            "248/448, train_loss: 0.1634\n",
            "249/448, train_loss: 0.2141\n",
            "250/448, train_loss: 0.2230\n",
            "251/448, train_loss: 0.2465\n",
            "252/448, train_loss: 0.1550\n",
            "253/448, train_loss: 0.2585\n",
            "254/448, train_loss: 0.1402\n",
            "255/448, train_loss: 0.1312\n",
            "256/448, train_loss: 0.2887\n",
            "257/448, train_loss: 0.2178\n",
            "258/448, train_loss: 0.2035\n",
            "259/448, train_loss: 0.1976\n",
            "260/448, train_loss: 0.1423\n",
            "261/448, train_loss: 0.1975\n",
            "262/448, train_loss: 0.1281\n",
            "263/448, train_loss: 0.0936\n",
            "264/448, train_loss: 0.1405\n",
            "265/448, train_loss: 0.0991\n",
            "266/448, train_loss: 0.1967\n",
            "267/448, train_loss: 0.1284\n",
            "268/448, train_loss: 0.1525\n",
            "269/448, train_loss: 0.3554\n",
            "270/448, train_loss: 0.1396\n",
            "271/448, train_loss: 0.2818\n",
            "272/448, train_loss: 0.1978\n",
            "273/448, train_loss: 0.1809\n",
            "274/448, train_loss: 0.2080\n",
            "275/448, train_loss: 0.1433\n",
            "276/448, train_loss: 0.1657\n",
            "277/448, train_loss: 0.2656\n",
            "278/448, train_loss: 0.2191\n",
            "279/448, train_loss: 0.1808\n",
            "280/448, train_loss: 0.1719\n",
            "281/448, train_loss: 0.1606\n",
            "282/448, train_loss: 0.1335\n",
            "283/448, train_loss: 0.1684\n",
            "284/448, train_loss: 0.0556\n",
            "285/448, train_loss: 0.1435\n",
            "286/448, train_loss: 0.2522\n",
            "287/448, train_loss: 0.1636\n",
            "288/448, train_loss: 0.1844\n",
            "289/448, train_loss: 0.1729\n",
            "290/448, train_loss: 0.1881\n",
            "291/448, train_loss: 0.2725\n",
            "292/448, train_loss: 0.1622\n",
            "293/448, train_loss: 0.0975\n",
            "294/448, train_loss: 0.1513\n",
            "295/448, train_loss: 0.3203\n",
            "296/448, train_loss: 0.1511\n",
            "297/448, train_loss: 0.1629\n",
            "298/448, train_loss: 0.0793\n",
            "299/448, train_loss: 0.1350\n",
            "300/448, train_loss: 0.1915\n",
            "301/448, train_loss: 0.1352\n",
            "302/448, train_loss: 0.1566\n",
            "303/448, train_loss: 0.2021\n",
            "304/448, train_loss: 0.1386\n",
            "305/448, train_loss: 0.2004\n",
            "306/448, train_loss: 0.1152\n",
            "307/448, train_loss: 0.2104\n",
            "308/448, train_loss: 0.2722\n",
            "309/448, train_loss: 0.1658\n",
            "310/448, train_loss: 0.1686\n",
            "311/448, train_loss: 0.1932\n",
            "312/448, train_loss: 0.1112\n",
            "313/448, train_loss: 0.0968\n",
            "314/448, train_loss: 0.1768\n",
            "315/448, train_loss: 0.1774\n",
            "316/448, train_loss: 0.0916\n",
            "317/448, train_loss: 0.2139\n",
            "318/448, train_loss: 0.1401\n",
            "319/448, train_loss: 0.1472\n",
            "320/448, train_loss: 0.2644\n",
            "321/448, train_loss: 0.1893\n",
            "322/448, train_loss: 0.1934\n",
            "323/448, train_loss: 0.2575\n",
            "324/448, train_loss: 0.1668\n",
            "325/448, train_loss: 0.1712\n",
            "326/448, train_loss: 0.1598\n",
            "327/448, train_loss: 0.1344\n",
            "328/448, train_loss: 0.1477\n",
            "329/448, train_loss: 0.0671\n",
            "330/448, train_loss: 0.1325\n",
            "331/448, train_loss: 0.1355\n",
            "332/448, train_loss: 0.1435\n",
            "333/448, train_loss: 0.1360\n",
            "334/448, train_loss: 0.1597\n",
            "335/448, train_loss: 0.2345\n",
            "336/448, train_loss: 0.2120\n",
            "337/448, train_loss: 0.1301\n",
            "338/448, train_loss: 0.0918\n",
            "339/448, train_loss: 0.1137\n",
            "340/448, train_loss: 0.1418\n",
            "341/448, train_loss: 0.1015\n",
            "342/448, train_loss: 0.1533\n",
            "343/448, train_loss: 0.2026\n",
            "344/448, train_loss: 0.1768\n",
            "345/448, train_loss: 0.1537\n",
            "346/448, train_loss: 0.1888\n",
            "347/448, train_loss: 0.1107\n",
            "348/448, train_loss: 0.1948\n",
            "349/448, train_loss: 0.2653\n",
            "350/448, train_loss: 0.1785\n",
            "351/448, train_loss: 0.1239\n",
            "352/448, train_loss: 0.2137\n",
            "353/448, train_loss: 0.4246\n",
            "354/448, train_loss: 0.1792\n",
            "355/448, train_loss: 0.1448\n",
            "356/448, train_loss: 0.1684\n",
            "357/448, train_loss: 0.1694\n",
            "358/448, train_loss: 0.2056\n",
            "359/448, train_loss: 0.0973\n",
            "360/448, train_loss: 0.2284\n",
            "361/448, train_loss: 0.1339\n",
            "362/448, train_loss: 0.1576\n",
            "363/448, train_loss: 0.1680\n",
            "364/448, train_loss: 0.1294\n",
            "365/448, train_loss: 0.2132\n",
            "366/448, train_loss: 0.1482\n",
            "367/448, train_loss: 0.1334\n",
            "368/448, train_loss: 0.1817\n",
            "369/448, train_loss: 0.1497\n",
            "370/448, train_loss: 0.1218\n",
            "371/448, train_loss: 0.2147\n",
            "372/448, train_loss: 0.1733\n",
            "373/448, train_loss: 0.1293\n",
            "374/448, train_loss: 0.1885\n",
            "375/448, train_loss: 0.3932\n",
            "376/448, train_loss: 0.1634\n",
            "377/448, train_loss: 0.1756\n",
            "378/448, train_loss: 0.3199\n",
            "379/448, train_loss: 0.1944\n",
            "380/448, train_loss: 0.0990\n",
            "381/448, train_loss: 0.1711\n",
            "382/448, train_loss: 0.1419\n",
            "383/448, train_loss: 0.5019\n",
            "384/448, train_loss: 0.1237\n",
            "385/448, train_loss: 0.1229\n",
            "386/448, train_loss: 0.1719\n",
            "387/448, train_loss: 0.1743\n",
            "388/448, train_loss: 0.1868\n",
            "389/448, train_loss: 0.1390\n",
            "390/448, train_loss: 0.1223\n",
            "391/448, train_loss: 0.1343\n",
            "392/448, train_loss: 0.1872\n",
            "393/448, train_loss: 0.1633\n",
            "394/448, train_loss: 0.2390\n",
            "395/448, train_loss: 0.3531\n",
            "396/448, train_loss: 0.1104\n",
            "397/448, train_loss: 0.4225\n",
            "398/448, train_loss: 0.0644\n",
            "399/448, train_loss: 0.1821\n",
            "400/448, train_loss: 0.1639\n",
            "401/448, train_loss: 0.5280\n",
            "402/448, train_loss: 0.2094\n",
            "403/448, train_loss: 0.1473\n",
            "404/448, train_loss: 0.1290\n",
            "405/448, train_loss: 0.1631\n",
            "406/448, train_loss: 0.0962\n",
            "407/448, train_loss: 0.1598\n",
            "408/448, train_loss: 0.2842\n",
            "409/448, train_loss: 0.1227\n",
            "410/448, train_loss: 0.1438\n",
            "411/448, train_loss: 0.3433\n",
            "412/448, train_loss: 0.2054\n",
            "413/448, train_loss: 0.1913\n",
            "414/448, train_loss: 0.1355\n",
            "415/448, train_loss: 0.1753\n",
            "416/448, train_loss: 0.0624\n",
            "417/448, train_loss: 0.1518\n",
            "418/448, train_loss: 0.2067\n",
            "419/448, train_loss: 0.1907\n",
            "420/448, train_loss: 0.1333\n",
            "421/448, train_loss: 0.2218\n",
            "422/448, train_loss: 0.1403\n",
            "423/448, train_loss: 0.1050\n",
            "424/448, train_loss: 0.1618\n",
            "425/448, train_loss: 0.1875\n",
            "426/448, train_loss: 0.1694\n",
            "427/448, train_loss: 0.2652\n",
            "428/448, train_loss: 0.0963\n",
            "429/448, train_loss: 0.3447\n",
            "430/448, train_loss: 0.2365\n",
            "431/448, train_loss: 0.1110\n",
            "432/448, train_loss: 0.1708\n",
            "433/448, train_loss: 0.2226\n",
            "434/448, train_loss: 0.0926\n",
            "435/448, train_loss: 0.1279\n",
            "436/448, train_loss: 0.1813\n",
            "437/448, train_loss: 0.1916\n",
            "438/448, train_loss: 0.1358\n",
            "439/448, train_loss: 0.1923\n",
            "440/448, train_loss: 0.1844\n",
            "441/448, train_loss: 0.0632\n",
            "442/448, train_loss: 0.1717\n",
            "443/448, train_loss: 0.1281\n",
            "444/448, train_loss: 0.3330\n",
            "445/448, train_loss: 0.1757\n",
            "446/448, train_loss: 0.1256\n",
            "447/448, train_loss: 0.0726\n",
            "448/448, train_loss: 0.1632\n",
            "449/448, train_loss: 0.0766\n",
            "epoch 45 average loss: 0.1726\n",
            "current epoch: 45 current mean dice: 0.8313 best mean dice: 0.8339 at epoch: 44\n",
            "----------\n",
            "epoch 46/50\n",
            "1/448, train_loss: 0.1184\n",
            "2/448, train_loss: 0.1712\n",
            "3/448, train_loss: 0.1382\n",
            "4/448, train_loss: 0.2216\n",
            "5/448, train_loss: 0.1690\n",
            "6/448, train_loss: 0.1652\n",
            "7/448, train_loss: 0.1003\n",
            "8/448, train_loss: 0.1643\n",
            "9/448, train_loss: 0.2475\n",
            "10/448, train_loss: 0.1377\n",
            "11/448, train_loss: 0.1221\n",
            "12/448, train_loss: 0.4055\n",
            "13/448, train_loss: 0.1341\n",
            "14/448, train_loss: 0.2237\n",
            "15/448, train_loss: 0.1339\n",
            "16/448, train_loss: 0.1561\n",
            "17/448, train_loss: 0.1808\n",
            "18/448, train_loss: 0.1812\n",
            "19/448, train_loss: 0.0760\n",
            "20/448, train_loss: 0.1746\n",
            "21/448, train_loss: 0.1144\n",
            "22/448, train_loss: 0.1004\n",
            "23/448, train_loss: 0.1431\n",
            "24/448, train_loss: 0.2019\n",
            "25/448, train_loss: 0.2602\n",
            "26/448, train_loss: 0.1017\n",
            "27/448, train_loss: 0.1760\n",
            "28/448, train_loss: 0.1234\n",
            "29/448, train_loss: 0.1344\n",
            "30/448, train_loss: 0.2089\n",
            "31/448, train_loss: 0.3256\n",
            "32/448, train_loss: 0.1631\n",
            "33/448, train_loss: 0.1052\n",
            "34/448, train_loss: 0.1444\n",
            "35/448, train_loss: 0.1335\n",
            "36/448, train_loss: 0.1312\n",
            "37/448, train_loss: 0.1933\n",
            "38/448, train_loss: 0.2193\n",
            "39/448, train_loss: 0.2485\n",
            "40/448, train_loss: 0.1062\n",
            "41/448, train_loss: 0.1740\n",
            "42/448, train_loss: 0.1321\n",
            "43/448, train_loss: 0.1836\n",
            "44/448, train_loss: 0.1562\n",
            "45/448, train_loss: 0.1491\n",
            "46/448, train_loss: 0.1346\n",
            "47/448, train_loss: 0.2362\n",
            "48/448, train_loss: 0.1256\n",
            "49/448, train_loss: 0.1986\n",
            "50/448, train_loss: 0.1371\n",
            "51/448, train_loss: 0.2216\n",
            "52/448, train_loss: 0.1223\n",
            "53/448, train_loss: 0.1340\n",
            "54/448, train_loss: 0.2025\n",
            "55/448, train_loss: 0.1696\n",
            "56/448, train_loss: 0.1500\n",
            "57/448, train_loss: 0.1427\n",
            "58/448, train_loss: 0.1629\n",
            "59/448, train_loss: 0.1470\n",
            "60/448, train_loss: 0.1695\n",
            "61/448, train_loss: 0.1802\n",
            "62/448, train_loss: 0.1048\n",
            "63/448, train_loss: 0.1981\n",
            "64/448, train_loss: 0.1566\n",
            "65/448, train_loss: 0.1543\n",
            "66/448, train_loss: 0.1563\n",
            "67/448, train_loss: 0.1969\n",
            "68/448, train_loss: 0.3167\n",
            "69/448, train_loss: 0.4784\n",
            "70/448, train_loss: 0.1718\n",
            "71/448, train_loss: 0.2818\n",
            "72/448, train_loss: 0.0910\n",
            "73/448, train_loss: 0.0913\n",
            "74/448, train_loss: 0.2020\n",
            "75/448, train_loss: 0.1291\n",
            "76/448, train_loss: 0.1651\n",
            "77/448, train_loss: 0.2397\n",
            "78/448, train_loss: 0.2478\n",
            "79/448, train_loss: 0.2295\n",
            "80/448, train_loss: 0.1696\n",
            "81/448, train_loss: 0.0921\n",
            "82/448, train_loss: 0.4256\n",
            "83/448, train_loss: 0.1797\n",
            "84/448, train_loss: 0.2660\n",
            "85/448, train_loss: 0.2937\n",
            "86/448, train_loss: 0.1564\n",
            "87/448, train_loss: 0.1338\n",
            "88/448, train_loss: 0.1544\n",
            "89/448, train_loss: 0.1490\n",
            "90/448, train_loss: 0.1436\n",
            "91/448, train_loss: 0.1705\n",
            "92/448, train_loss: 0.1622\n",
            "93/448, train_loss: 0.1544\n",
            "94/448, train_loss: 0.1722\n",
            "95/448, train_loss: 0.1658\n",
            "96/448, train_loss: 0.3268\n",
            "97/448, train_loss: 0.0948\n",
            "98/448, train_loss: 0.3769\n",
            "99/448, train_loss: 0.2359\n",
            "100/448, train_loss: 0.1632\n",
            "101/448, train_loss: 0.2058\n",
            "102/448, train_loss: 0.1722\n",
            "103/448, train_loss: 0.2754\n",
            "104/448, train_loss: 0.1727\n",
            "105/448, train_loss: 0.3451\n",
            "106/448, train_loss: 0.2047\n",
            "107/448, train_loss: 0.1880\n",
            "108/448, train_loss: 0.1642\n",
            "109/448, train_loss: 0.1888\n",
            "110/448, train_loss: 0.2008\n",
            "111/448, train_loss: 0.2097\n",
            "112/448, train_loss: 0.0864\n",
            "113/448, train_loss: 0.0920\n",
            "114/448, train_loss: 0.1542\n",
            "115/448, train_loss: 0.0704\n",
            "116/448, train_loss: 0.1670\n",
            "117/448, train_loss: 0.1681\n",
            "118/448, train_loss: 0.1287\n",
            "119/448, train_loss: 0.1424\n",
            "120/448, train_loss: 0.1835\n",
            "121/448, train_loss: 0.1369\n",
            "122/448, train_loss: 0.1028\n",
            "123/448, train_loss: 0.1214\n",
            "124/448, train_loss: 0.1950\n",
            "125/448, train_loss: 0.1632\n",
            "126/448, train_loss: 0.1261\n",
            "127/448, train_loss: 0.1728\n",
            "128/448, train_loss: 0.2342\n",
            "129/448, train_loss: 0.1027\n",
            "130/448, train_loss: 0.1771\n",
            "131/448, train_loss: 0.2236\n",
            "132/448, train_loss: 0.1951\n",
            "133/448, train_loss: 0.1470\n",
            "134/448, train_loss: 0.1037\n",
            "135/448, train_loss: 0.1542\n",
            "136/448, train_loss: 0.1382\n",
            "137/448, train_loss: 0.1543\n",
            "138/448, train_loss: 0.1686\n",
            "139/448, train_loss: 0.1131\n",
            "140/448, train_loss: 0.1404\n",
            "141/448, train_loss: 0.1447\n",
            "142/448, train_loss: 0.1496\n",
            "143/448, train_loss: 0.1245\n",
            "144/448, train_loss: 0.1974\n",
            "145/448, train_loss: 0.1340\n",
            "146/448, train_loss: 0.1355\n",
            "147/448, train_loss: 0.1539\n",
            "148/448, train_loss: 0.1647\n",
            "149/448, train_loss: 0.2800\n",
            "150/448, train_loss: 0.1238\n",
            "151/448, train_loss: 0.2453\n",
            "152/448, train_loss: 0.1944\n",
            "153/448, train_loss: 0.1846\n",
            "154/448, train_loss: 0.3194\n",
            "155/448, train_loss: 0.2214\n",
            "156/448, train_loss: 0.1925\n",
            "157/448, train_loss: 0.1677\n",
            "158/448, train_loss: 0.1935\n",
            "159/448, train_loss: 0.1747\n",
            "160/448, train_loss: 0.3455\n",
            "161/448, train_loss: 0.1281\n",
            "162/448, train_loss: 0.1699\n",
            "163/448, train_loss: 0.1424\n",
            "164/448, train_loss: 0.2204\n",
            "165/448, train_loss: 0.1266\n",
            "166/448, train_loss: 0.1423\n",
            "167/448, train_loss: 0.1349\n",
            "168/448, train_loss: 0.1575\n",
            "169/448, train_loss: 0.1624\n",
            "170/448, train_loss: 0.1227\n",
            "171/448, train_loss: 0.1465\n",
            "172/448, train_loss: 0.3675\n",
            "173/448, train_loss: 0.1596\n",
            "174/448, train_loss: 0.1984\n",
            "175/448, train_loss: 0.1576\n",
            "176/448, train_loss: 0.1230\n",
            "177/448, train_loss: 0.2346\n",
            "178/448, train_loss: 0.1000\n",
            "179/448, train_loss: 0.1344\n",
            "180/448, train_loss: 0.1819\n",
            "181/448, train_loss: 0.2390\n",
            "182/448, train_loss: 0.1349\n",
            "183/448, train_loss: 0.1375\n",
            "184/448, train_loss: 0.1989\n",
            "185/448, train_loss: 0.1615\n",
            "186/448, train_loss: 0.1868\n",
            "187/448, train_loss: 0.1003\n",
            "188/448, train_loss: 0.1376\n",
            "189/448, train_loss: 0.1879\n",
            "190/448, train_loss: 0.2823\n",
            "191/448, train_loss: 0.1280\n",
            "192/448, train_loss: 0.1269\n",
            "193/448, train_loss: 0.2305\n",
            "194/448, train_loss: 0.2371\n",
            "195/448, train_loss: 0.1248\n",
            "196/448, train_loss: 0.1236\n",
            "197/448, train_loss: 0.1617\n",
            "198/448, train_loss: 0.2501\n",
            "199/448, train_loss: 0.2308\n",
            "200/448, train_loss: 0.1928\n",
            "201/448, train_loss: 0.1359\n",
            "202/448, train_loss: 0.1618\n",
            "203/448, train_loss: 0.1539\n",
            "204/448, train_loss: 0.2056\n",
            "205/448, train_loss: 0.1450\n",
            "206/448, train_loss: 0.1660\n",
            "207/448, train_loss: 0.1628\n",
            "208/448, train_loss: 0.1307\n",
            "209/448, train_loss: 0.3108\n",
            "210/448, train_loss: 0.1413\n",
            "211/448, train_loss: 0.1717\n",
            "212/448, train_loss: 0.2085\n",
            "213/448, train_loss: 0.0951\n",
            "214/448, train_loss: 0.1844\n",
            "215/448, train_loss: 0.1882\n",
            "216/448, train_loss: 0.1471\n",
            "217/448, train_loss: 0.1294\n",
            "218/448, train_loss: 0.1467\n",
            "219/448, train_loss: 0.1087\n",
            "220/448, train_loss: 0.1603\n",
            "221/448, train_loss: 0.2040\n",
            "222/448, train_loss: 0.1242\n",
            "223/448, train_loss: 0.1674\n",
            "224/448, train_loss: 0.2141\n",
            "225/448, train_loss: 0.1833\n",
            "226/448, train_loss: 0.1025\n",
            "227/448, train_loss: 0.1392\n",
            "228/448, train_loss: 0.5376\n",
            "229/448, train_loss: 0.1582\n",
            "230/448, train_loss: 0.1389\n",
            "231/448, train_loss: 0.1853\n",
            "232/448, train_loss: 0.0716\n",
            "233/448, train_loss: 0.1522\n",
            "234/448, train_loss: 0.1299\n",
            "235/448, train_loss: 0.1231\n",
            "236/448, train_loss: 0.2741\n",
            "237/448, train_loss: 0.1261\n",
            "238/448, train_loss: 0.1103\n",
            "239/448, train_loss: 0.2088\n",
            "240/448, train_loss: 0.1387\n",
            "241/448, train_loss: 0.1755\n",
            "242/448, train_loss: 0.1752\n",
            "243/448, train_loss: 0.1265\n",
            "244/448, train_loss: 0.2213\n",
            "245/448, train_loss: 0.1720\n",
            "246/448, train_loss: 0.1766\n",
            "247/448, train_loss: 0.1085\n",
            "248/448, train_loss: 0.1572\n",
            "249/448, train_loss: 0.1930\n",
            "250/448, train_loss: 0.2083\n",
            "251/448, train_loss: 0.2864\n",
            "252/448, train_loss: 0.1610\n",
            "253/448, train_loss: 0.1278\n",
            "254/448, train_loss: 0.1668\n",
            "255/448, train_loss: 0.1724\n",
            "256/448, train_loss: 0.3814\n",
            "257/448, train_loss: 0.1415\n",
            "258/448, train_loss: 0.1272\n",
            "259/448, train_loss: 0.1409\n",
            "260/448, train_loss: 0.1698\n",
            "261/448, train_loss: 0.2029\n",
            "262/448, train_loss: 0.2820\n",
            "263/448, train_loss: 0.1366\n",
            "264/448, train_loss: 0.2174\n",
            "265/448, train_loss: 0.1397\n",
            "266/448, train_loss: 0.1380\n",
            "267/448, train_loss: 0.1452\n",
            "268/448, train_loss: 0.1575\n",
            "269/448, train_loss: 0.1516\n",
            "270/448, train_loss: 0.0924\n",
            "271/448, train_loss: 0.1666\n",
            "272/448, train_loss: 0.1227\n",
            "273/448, train_loss: 0.1508\n",
            "274/448, train_loss: 0.0888\n",
            "275/448, train_loss: 0.1778\n",
            "276/448, train_loss: 0.1942\n",
            "277/448, train_loss: 0.1284\n",
            "278/448, train_loss: 0.1655\n",
            "279/448, train_loss: 0.3570\n",
            "280/448, train_loss: 0.1709\n",
            "281/448, train_loss: 0.1932\n",
            "282/448, train_loss: 0.0954\n",
            "283/448, train_loss: 0.1067\n",
            "284/448, train_loss: 0.1433\n",
            "285/448, train_loss: 0.2053\n",
            "286/448, train_loss: 0.1908\n",
            "287/448, train_loss: 0.1282\n",
            "288/448, train_loss: 0.1379\n",
            "289/448, train_loss: 0.1293\n",
            "290/448, train_loss: 0.1667\n",
            "291/448, train_loss: 0.1847\n",
            "292/448, train_loss: 0.1332\n",
            "293/448, train_loss: 0.1406\n",
            "294/448, train_loss: 0.1222\n",
            "295/448, train_loss: 0.1054\n",
            "296/448, train_loss: 0.1293\n",
            "297/448, train_loss: 0.1408\n",
            "298/448, train_loss: 0.0983\n",
            "299/448, train_loss: 0.1383\n",
            "300/448, train_loss: 0.1555\n",
            "301/448, train_loss: 0.0975\n",
            "302/448, train_loss: 0.1808\n",
            "303/448, train_loss: 0.1682\n",
            "304/448, train_loss: 0.2038\n",
            "305/448, train_loss: 0.1900\n",
            "306/448, train_loss: 0.1686\n",
            "307/448, train_loss: 0.1221\n",
            "308/448, train_loss: 0.1794\n",
            "309/448, train_loss: 0.1079\n",
            "310/448, train_loss: 0.1382\n",
            "311/448, train_loss: 0.1819\n",
            "312/448, train_loss: 0.1326\n",
            "313/448, train_loss: 0.1807\n",
            "314/448, train_loss: 0.1467\n",
            "315/448, train_loss: 0.1988\n",
            "316/448, train_loss: 0.0927\n",
            "317/448, train_loss: 0.2089\n",
            "318/448, train_loss: 0.1080\n",
            "319/448, train_loss: 0.0957\n",
            "320/448, train_loss: 0.2147\n",
            "321/448, train_loss: 0.2002\n",
            "322/448, train_loss: 0.1940\n",
            "323/448, train_loss: 0.1646\n",
            "324/448, train_loss: 0.1305\n",
            "325/448, train_loss: 0.1279\n",
            "326/448, train_loss: 0.1331\n",
            "327/448, train_loss: 0.2053\n",
            "328/448, train_loss: 0.1354\n",
            "329/448, train_loss: 0.1449\n",
            "330/448, train_loss: 0.1937\n",
            "331/448, train_loss: 0.1806\n",
            "332/448, train_loss: 0.1495\n",
            "333/448, train_loss: 0.2128\n",
            "334/448, train_loss: 0.1729\n",
            "335/448, train_loss: 0.2816\n",
            "336/448, train_loss: 0.3997\n",
            "337/448, train_loss: 0.0942\n",
            "338/448, train_loss: 0.1435\n",
            "339/448, train_loss: 0.1474\n",
            "340/448, train_loss: 0.1698\n",
            "341/448, train_loss: 0.1982\n",
            "342/448, train_loss: 0.1757\n",
            "343/448, train_loss: 0.1881\n",
            "344/448, train_loss: 0.2621\n",
            "345/448, train_loss: 0.1655\n",
            "346/448, train_loss: 0.1884\n",
            "347/448, train_loss: 0.1465\n",
            "348/448, train_loss: 0.2068\n",
            "349/448, train_loss: 0.1076\n",
            "350/448, train_loss: 0.1837\n",
            "351/448, train_loss: 0.1092\n",
            "352/448, train_loss: 0.1975\n",
            "353/448, train_loss: 0.1350\n",
            "354/448, train_loss: 0.1586\n",
            "355/448, train_loss: 0.0974\n",
            "356/448, train_loss: 0.1299\n",
            "357/448, train_loss: 0.1606\n",
            "358/448, train_loss: 0.1407\n",
            "359/448, train_loss: 0.1821\n",
            "360/448, train_loss: 0.1292\n",
            "361/448, train_loss: 0.1722\n",
            "362/448, train_loss: 0.1659\n",
            "363/448, train_loss: 0.3332\n",
            "364/448, train_loss: 0.1594\n",
            "365/448, train_loss: 0.2152\n",
            "366/448, train_loss: 0.2420\n",
            "367/448, train_loss: 0.2248\n",
            "368/448, train_loss: 0.1000\n",
            "369/448, train_loss: 0.1601\n",
            "370/448, train_loss: 0.1482\n",
            "371/448, train_loss: 0.1546\n",
            "372/448, train_loss: 0.2340\n",
            "373/448, train_loss: 0.1556\n",
            "374/448, train_loss: 0.1691\n",
            "375/448, train_loss: 0.1489\n",
            "376/448, train_loss: 0.2589\n",
            "377/448, train_loss: 0.1343\n",
            "378/448, train_loss: 0.1030\n",
            "379/448, train_loss: 0.1645\n",
            "380/448, train_loss: 0.1330\n",
            "381/448, train_loss: 0.0919\n",
            "382/448, train_loss: 0.0712\n",
            "383/448, train_loss: 0.2233\n",
            "384/448, train_loss: 0.1553\n",
            "385/448, train_loss: 0.4729\n",
            "386/448, train_loss: 0.2182\n",
            "387/448, train_loss: 0.2135\n",
            "388/448, train_loss: 0.1089\n",
            "389/448, train_loss: 0.0858\n",
            "390/448, train_loss: 0.1154\n",
            "391/448, train_loss: 0.1073\n",
            "392/448, train_loss: 0.1987\n",
            "393/448, train_loss: 0.1133\n",
            "394/448, train_loss: 0.1493\n",
            "395/448, train_loss: 0.1372\n",
            "396/448, train_loss: 0.0868\n",
            "397/448, train_loss: 0.1421\n",
            "398/448, train_loss: 0.1419\n",
            "399/448, train_loss: 0.1885\n",
            "400/448, train_loss: 0.1643\n",
            "401/448, train_loss: 0.2288\n",
            "402/448, train_loss: 0.1663\n",
            "403/448, train_loss: 0.1697\n",
            "404/448, train_loss: 0.2543\n",
            "405/448, train_loss: 0.1708\n",
            "406/448, train_loss: 0.2541\n",
            "407/448, train_loss: 0.1355\n",
            "408/448, train_loss: 0.1255\n",
            "409/448, train_loss: 0.1604\n",
            "410/448, train_loss: 0.1852\n",
            "411/448, train_loss: 0.2030\n",
            "412/448, train_loss: 0.2213\n",
            "413/448, train_loss: 0.1952\n",
            "414/448, train_loss: 0.2112\n",
            "415/448, train_loss: 0.1509\n",
            "416/448, train_loss: 0.3009\n",
            "417/448, train_loss: 0.2777\n",
            "418/448, train_loss: 0.1122\n",
            "419/448, train_loss: 0.1830\n",
            "420/448, train_loss: 0.1682\n",
            "421/448, train_loss: 0.1245\n",
            "422/448, train_loss: 0.1509\n",
            "423/448, train_loss: 0.2247\n",
            "424/448, train_loss: 0.1255\n",
            "425/448, train_loss: 0.3482\n",
            "426/448, train_loss: 0.1128\n",
            "427/448, train_loss: 0.1920\n",
            "428/448, train_loss: 0.1354\n",
            "429/448, train_loss: 0.1545\n",
            "430/448, train_loss: 0.2156\n",
            "431/448, train_loss: 0.1394\n",
            "432/448, train_loss: 0.1247\n",
            "433/448, train_loss: 0.1485\n",
            "434/448, train_loss: 0.1878\n",
            "435/448, train_loss: 0.1453\n",
            "436/448, train_loss: 0.1327\n",
            "437/448, train_loss: 0.1291\n",
            "438/448, train_loss: 0.3125\n",
            "439/448, train_loss: 0.1587\n",
            "440/448, train_loss: 0.0976\n",
            "441/448, train_loss: 0.1819\n",
            "442/448, train_loss: 0.1489\n",
            "443/448, train_loss: 0.1366\n",
            "444/448, train_loss: 0.1800\n",
            "445/448, train_loss: 0.1694\n",
            "446/448, train_loss: 0.1540\n",
            "447/448, train_loss: 0.1597\n",
            "448/448, train_loss: 0.1252\n",
            "449/448, train_loss: 0.2840\n",
            "epoch 46 average loss: 0.1723\n",
            "current epoch: 46 current mean dice: 0.8321 best mean dice: 0.8339 at epoch: 44\n",
            "----------\n",
            "epoch 47/50\n",
            "1/448, train_loss: 0.1211\n",
            "2/448, train_loss: 0.1314\n",
            "3/448, train_loss: 0.1148\n",
            "4/448, train_loss: 0.1785\n",
            "5/448, train_loss: 0.1441\n",
            "6/448, train_loss: 0.1544\n",
            "7/448, train_loss: 0.1697\n",
            "8/448, train_loss: 0.1285\n",
            "9/448, train_loss: 0.1272\n",
            "10/448, train_loss: 0.1324\n",
            "11/448, train_loss: 0.1646\n",
            "12/448, train_loss: 0.1719\n",
            "13/448, train_loss: 0.0744\n",
            "14/448, train_loss: 0.1262\n",
            "15/448, train_loss: 0.0728\n",
            "16/448, train_loss: 0.0995\n",
            "17/448, train_loss: 0.1971\n",
            "18/448, train_loss: 0.2574\n",
            "19/448, train_loss: 0.1331\n",
            "20/448, train_loss: 0.3088\n",
            "21/448, train_loss: 0.1309\n",
            "22/448, train_loss: 0.1937\n",
            "23/448, train_loss: 0.2417\n",
            "24/448, train_loss: 0.1681\n",
            "25/448, train_loss: 0.2059\n",
            "26/448, train_loss: 0.0954\n",
            "27/448, train_loss: 0.1573\n",
            "28/448, train_loss: 0.2053\n",
            "29/448, train_loss: 0.0923\n",
            "30/448, train_loss: 0.1264\n",
            "31/448, train_loss: 0.1220\n",
            "32/448, train_loss: 0.1289\n",
            "33/448, train_loss: 0.1719\n",
            "34/448, train_loss: 0.1729\n",
            "35/448, train_loss: 0.1405\n",
            "36/448, train_loss: 0.0918\n",
            "37/448, train_loss: 0.1731\n",
            "38/448, train_loss: 0.1985\n",
            "39/448, train_loss: 0.2103\n",
            "40/448, train_loss: 0.0993\n",
            "41/448, train_loss: 0.2255\n",
            "42/448, train_loss: 0.2194\n",
            "43/448, train_loss: 0.1247\n",
            "44/448, train_loss: 0.1961\n",
            "45/448, train_loss: 0.1300\n",
            "46/448, train_loss: 0.1380\n",
            "47/448, train_loss: 0.1789\n",
            "48/448, train_loss: 0.1647\n",
            "49/448, train_loss: 0.1696\n",
            "50/448, train_loss: 0.1303\n",
            "51/448, train_loss: 0.1273\n",
            "52/448, train_loss: 0.1003\n",
            "53/448, train_loss: 0.1315\n",
            "54/448, train_loss: 0.1033\n",
            "55/448, train_loss: 0.2541\n",
            "56/448, train_loss: 0.1774\n",
            "57/448, train_loss: 0.1961\n",
            "58/448, train_loss: 0.2855\n",
            "59/448, train_loss: 0.1612\n",
            "60/448, train_loss: 0.1965\n",
            "61/448, train_loss: 0.0838\n",
            "62/448, train_loss: 0.1435\n",
            "63/448, train_loss: 0.2655\n",
            "64/448, train_loss: 0.1984\n",
            "65/448, train_loss: 0.2328\n",
            "66/448, train_loss: 0.0995\n",
            "67/448, train_loss: 0.1277\n",
            "68/448, train_loss: 0.1851\n",
            "69/448, train_loss: 0.2838\n",
            "70/448, train_loss: 0.0969\n",
            "71/448, train_loss: 0.1583\n",
            "72/448, train_loss: 0.2250\n",
            "73/448, train_loss: 0.1769\n",
            "74/448, train_loss: 0.0989\n",
            "75/448, train_loss: 0.1583\n",
            "76/448, train_loss: 0.1663\n",
            "77/448, train_loss: 0.1953\n",
            "78/448, train_loss: 0.1803\n",
            "79/448, train_loss: 0.1760\n",
            "80/448, train_loss: 0.1237\n",
            "81/448, train_loss: 0.2616\n",
            "82/448, train_loss: 0.1554\n",
            "83/448, train_loss: 0.2250\n",
            "84/448, train_loss: 0.2541\n",
            "85/448, train_loss: 0.2351\n",
            "86/448, train_loss: 0.1265\n",
            "87/448, train_loss: 0.1297\n",
            "88/448, train_loss: 0.1574\n",
            "89/448, train_loss: 0.2501\n",
            "90/448, train_loss: 0.3464\n",
            "91/448, train_loss: 0.2043\n",
            "92/448, train_loss: 0.2732\n",
            "93/448, train_loss: 0.1665\n",
            "94/448, train_loss: 0.2925\n",
            "95/448, train_loss: 0.2445\n",
            "96/448, train_loss: 0.4109\n",
            "97/448, train_loss: 0.1918\n",
            "98/448, train_loss: 0.2181\n",
            "99/448, train_loss: 0.1631\n",
            "100/448, train_loss: 0.1277\n",
            "101/448, train_loss: 0.1731\n",
            "102/448, train_loss: 0.3083\n",
            "103/448, train_loss: 0.1298\n",
            "104/448, train_loss: 0.1625\n",
            "105/448, train_loss: 0.1606\n",
            "106/448, train_loss: 0.0923\n",
            "107/448, train_loss: 0.1702\n",
            "108/448, train_loss: 0.2422\n",
            "109/448, train_loss: 0.2114\n",
            "110/448, train_loss: 0.2084\n",
            "111/448, train_loss: 0.2051\n",
            "112/448, train_loss: 0.1662\n",
            "113/448, train_loss: 0.1570\n",
            "114/448, train_loss: 0.1961\n",
            "115/448, train_loss: 0.0909\n",
            "116/448, train_loss: 0.0963\n",
            "117/448, train_loss: 0.2277\n",
            "118/448, train_loss: 0.1176\n",
            "119/448, train_loss: 0.1306\n",
            "120/448, train_loss: 0.1322\n",
            "121/448, train_loss: 0.0979\n",
            "122/448, train_loss: 0.1342\n",
            "123/448, train_loss: 0.2050\n",
            "124/448, train_loss: 0.2279\n",
            "125/448, train_loss: 0.1729\n",
            "126/448, train_loss: 0.2042\n",
            "127/448, train_loss: 0.1565\n",
            "128/448, train_loss: 0.1402\n",
            "129/448, train_loss: 0.1754\n",
            "130/448, train_loss: 0.1364\n",
            "131/448, train_loss: 0.1892\n",
            "132/448, train_loss: 0.1585\n",
            "133/448, train_loss: 0.1529\n",
            "134/448, train_loss: 0.1210\n",
            "135/448, train_loss: 0.1857\n",
            "136/448, train_loss: 0.1302\n",
            "137/448, train_loss: 0.1072\n",
            "138/448, train_loss: 0.1610\n",
            "139/448, train_loss: 0.1654\n",
            "140/448, train_loss: 0.0948\n",
            "141/448, train_loss: 0.1639\n",
            "142/448, train_loss: 0.2295\n",
            "143/448, train_loss: 0.1421\n",
            "144/448, train_loss: 0.1013\n",
            "145/448, train_loss: 0.1732\n",
            "146/448, train_loss: 0.1062\n",
            "147/448, train_loss: 0.1246\n",
            "148/448, train_loss: 0.1944\n",
            "149/448, train_loss: 0.1393\n",
            "150/448, train_loss: 0.1162\n",
            "151/448, train_loss: 0.1663\n",
            "152/448, train_loss: 0.1937\n",
            "153/448, train_loss: 0.2007\n",
            "154/448, train_loss: 0.1985\n",
            "155/448, train_loss: 0.1826\n",
            "156/448, train_loss: 0.2441\n",
            "157/448, train_loss: 0.1583\n",
            "158/448, train_loss: 0.2136\n",
            "159/448, train_loss: 0.1411\n",
            "160/448, train_loss: 0.1372\n",
            "161/448, train_loss: 0.2095\n",
            "162/448, train_loss: 0.1388\n",
            "163/448, train_loss: 0.1055\n",
            "164/448, train_loss: 0.1426\n",
            "165/448, train_loss: 0.2352\n",
            "166/448, train_loss: 0.1293\n",
            "167/448, train_loss: 0.1335\n",
            "168/448, train_loss: 0.2184\n",
            "169/448, train_loss: 0.1777\n",
            "170/448, train_loss: 0.1316\n",
            "171/448, train_loss: 0.1387\n",
            "172/448, train_loss: 0.2608\n",
            "173/448, train_loss: 0.2766\n",
            "174/448, train_loss: 0.1892\n",
            "175/448, train_loss: 0.1673\n",
            "176/448, train_loss: 0.1457\n",
            "177/448, train_loss: 0.2143\n",
            "178/448, train_loss: 0.2189\n",
            "179/448, train_loss: 0.1738\n",
            "180/448, train_loss: 0.1940\n",
            "181/448, train_loss: 0.1355\n",
            "182/448, train_loss: 0.1347\n",
            "183/448, train_loss: 0.1657\n",
            "184/448, train_loss: 0.1411\n",
            "185/448, train_loss: 0.1297\n",
            "186/448, train_loss: 0.1769\n",
            "187/448, train_loss: 0.1943\n",
            "188/448, train_loss: 0.2003\n",
            "189/448, train_loss: 0.2280\n",
            "190/448, train_loss: 0.1509\n",
            "191/448, train_loss: 0.3079\n",
            "192/448, train_loss: 0.1306\n",
            "193/448, train_loss: 0.2006\n",
            "194/448, train_loss: 0.1496\n",
            "195/448, train_loss: 0.1369\n",
            "196/448, train_loss: 0.5039\n",
            "197/448, train_loss: 0.1797\n",
            "198/448, train_loss: 0.1270\n",
            "199/448, train_loss: 0.2074\n",
            "200/448, train_loss: 0.1687\n",
            "201/448, train_loss: 0.2467\n",
            "202/448, train_loss: 0.2044\n",
            "203/448, train_loss: 0.2345\n",
            "204/448, train_loss: 0.1748\n",
            "205/448, train_loss: 0.1590\n",
            "206/448, train_loss: 0.1447\n",
            "207/448, train_loss: 0.1302\n",
            "208/448, train_loss: 0.1780\n",
            "209/448, train_loss: 0.1731\n",
            "210/448, train_loss: 0.1773\n",
            "211/448, train_loss: 0.1406\n",
            "212/448, train_loss: 0.4539\n",
            "213/448, train_loss: 0.1332\n",
            "214/448, train_loss: 0.0956\n",
            "215/448, train_loss: 0.0850\n",
            "216/448, train_loss: 0.1762\n",
            "217/448, train_loss: 0.2236\n",
            "218/448, train_loss: 0.4636\n",
            "219/448, train_loss: 0.1582\n",
            "220/448, train_loss: 0.2754\n",
            "221/448, train_loss: 0.1601\n",
            "222/448, train_loss: 0.1688\n",
            "223/448, train_loss: 0.2470\n",
            "224/448, train_loss: 0.1400\n",
            "225/448, train_loss: 0.1330\n",
            "226/448, train_loss: 0.1154\n",
            "227/448, train_loss: 0.1045\n",
            "228/448, train_loss: 0.3488\n",
            "229/448, train_loss: 0.1275\n",
            "230/448, train_loss: 0.1693\n",
            "231/448, train_loss: 0.0984\n",
            "232/448, train_loss: 0.2104\n",
            "233/448, train_loss: 0.1471\n",
            "234/448, train_loss: 0.2278\n",
            "235/448, train_loss: 0.0954\n",
            "236/448, train_loss: 0.1861\n",
            "237/448, train_loss: 0.1756\n",
            "238/448, train_loss: 0.2086\n",
            "239/448, train_loss: 0.1509\n",
            "240/448, train_loss: 0.1102\n",
            "241/448, train_loss: 0.2456\n",
            "242/448, train_loss: 0.2374\n",
            "243/448, train_loss: 0.1342\n",
            "244/448, train_loss: 0.1706\n",
            "245/448, train_loss: 0.1047\n",
            "246/448, train_loss: 0.1264\n",
            "247/448, train_loss: 0.1254\n",
            "248/448, train_loss: 0.1697\n",
            "249/448, train_loss: 0.0821\n",
            "250/448, train_loss: 0.1744\n",
            "251/448, train_loss: 0.2160\n",
            "252/448, train_loss: 0.1703\n",
            "253/448, train_loss: 0.1384\n",
            "254/448, train_loss: 0.1943\n",
            "255/448, train_loss: 0.3719\n",
            "256/448, train_loss: 0.1485\n",
            "257/448, train_loss: 0.1556\n",
            "258/448, train_loss: 0.1354\n",
            "259/448, train_loss: 0.1670\n",
            "260/448, train_loss: 0.1283\n",
            "261/448, train_loss: 0.1640\n",
            "262/448, train_loss: 0.1725\n",
            "263/448, train_loss: 0.1408\n",
            "264/448, train_loss: 0.1688\n",
            "265/448, train_loss: 0.1385\n",
            "266/448, train_loss: 0.1297\n",
            "267/448, train_loss: 0.1593\n",
            "268/448, train_loss: 0.1292\n",
            "269/448, train_loss: 0.1972\n",
            "270/448, train_loss: 0.1300\n",
            "271/448, train_loss: 0.2111\n",
            "272/448, train_loss: 0.2159\n",
            "273/448, train_loss: 0.1835\n",
            "274/448, train_loss: 0.1646\n",
            "275/448, train_loss: 0.1111\n",
            "276/448, train_loss: 0.1750\n",
            "277/448, train_loss: 0.1312\n",
            "278/448, train_loss: 0.3287\n",
            "279/448, train_loss: 0.1224\n",
            "280/448, train_loss: 0.1624\n",
            "281/448, train_loss: 0.2220\n",
            "282/448, train_loss: 0.1012\n",
            "283/448, train_loss: 0.0555\n",
            "284/448, train_loss: 0.1778\n",
            "285/448, train_loss: 0.1580\n",
            "286/448, train_loss: 0.2243\n",
            "287/448, train_loss: 0.1579\n",
            "288/448, train_loss: 0.1491\n",
            "289/448, train_loss: 0.1369\n",
            "290/448, train_loss: 0.1780\n",
            "291/448, train_loss: 0.0992\n",
            "292/448, train_loss: 0.2281\n",
            "293/448, train_loss: 0.1541\n",
            "294/448, train_loss: 0.1593\n",
            "295/448, train_loss: 0.0658\n",
            "296/448, train_loss: 0.1107\n",
            "297/448, train_loss: 0.1872\n",
            "298/448, train_loss: 0.2220\n",
            "299/448, train_loss: 0.1068\n",
            "300/448, train_loss: 0.1604\n",
            "301/448, train_loss: 0.1428\n",
            "302/448, train_loss: 0.2410\n",
            "303/448, train_loss: 0.1596\n",
            "304/448, train_loss: 0.1737\n",
            "305/448, train_loss: 0.1526\n",
            "306/448, train_loss: 0.1596\n",
            "307/448, train_loss: 0.1401\n",
            "308/448, train_loss: 0.1245\n",
            "309/448, train_loss: 0.1535\n",
            "310/448, train_loss: 0.1344\n",
            "311/448, train_loss: 0.2039\n",
            "312/448, train_loss: 0.1228\n",
            "313/448, train_loss: 0.1038\n",
            "314/448, train_loss: 0.1601\n",
            "315/448, train_loss: 0.1259\n",
            "316/448, train_loss: 0.1485\n",
            "317/448, train_loss: 0.2142\n",
            "318/448, train_loss: 0.1119\n",
            "319/448, train_loss: 0.1237\n",
            "320/448, train_loss: 0.1263\n",
            "321/448, train_loss: 0.2319\n",
            "322/448, train_loss: 0.1789\n",
            "323/448, train_loss: 0.0949\n",
            "324/448, train_loss: 0.1655\n",
            "325/448, train_loss: 0.1498\n",
            "326/448, train_loss: 0.1733\n",
            "327/448, train_loss: 0.0984\n",
            "328/448, train_loss: 0.1362\n",
            "329/448, train_loss: 0.1503\n",
            "330/448, train_loss: 0.1474\n",
            "331/448, train_loss: 0.1731\n",
            "332/448, train_loss: 0.1717\n",
            "333/448, train_loss: 0.1671\n",
            "334/448, train_loss: 0.1803\n",
            "335/448, train_loss: 0.1310\n",
            "336/448, train_loss: 0.1539\n",
            "337/448, train_loss: 0.1763\n",
            "338/448, train_loss: 0.1904\n",
            "339/448, train_loss: 0.1241\n",
            "340/448, train_loss: 0.1305\n",
            "341/448, train_loss: 0.1339\n",
            "342/448, train_loss: 0.1262\n",
            "343/448, train_loss: 0.1849\n",
            "344/448, train_loss: 0.1915\n",
            "345/448, train_loss: 0.1115\n",
            "346/448, train_loss: 0.2149\n",
            "347/448, train_loss: 0.3205\n",
            "348/448, train_loss: 0.0874\n",
            "349/448, train_loss: 0.1419\n",
            "350/448, train_loss: 0.1737\n",
            "351/448, train_loss: 0.1855\n",
            "352/448, train_loss: 0.1488\n",
            "353/448, train_loss: 0.1582\n",
            "354/448, train_loss: 0.0542\n",
            "355/448, train_loss: 0.3115\n",
            "356/448, train_loss: 0.1381\n",
            "357/448, train_loss: 0.1348\n",
            "358/448, train_loss: 0.1845\n",
            "359/448, train_loss: 0.1316\n",
            "360/448, train_loss: 0.2139\n",
            "361/448, train_loss: 0.4157\n",
            "362/448, train_loss: 0.2445\n",
            "363/448, train_loss: 0.1811\n",
            "364/448, train_loss: 0.0957\n",
            "365/448, train_loss: 0.1744\n",
            "366/448, train_loss: 0.1856\n",
            "367/448, train_loss: 0.1707\n",
            "368/448, train_loss: 0.4281\n",
            "369/448, train_loss: 0.1857\n",
            "370/448, train_loss: 0.2196\n",
            "371/448, train_loss: 0.1729\n",
            "372/448, train_loss: 0.0975\n",
            "373/448, train_loss: 0.1291\n",
            "374/448, train_loss: 0.1498\n",
            "375/448, train_loss: 0.1250\n",
            "376/448, train_loss: 0.0924\n",
            "377/448, train_loss: 0.2081\n",
            "378/448, train_loss: 0.1567\n",
            "379/448, train_loss: 0.1401\n",
            "380/448, train_loss: 0.1854\n",
            "381/448, train_loss: 0.1304\n",
            "382/448, train_loss: 0.1837\n",
            "383/448, train_loss: 0.2154\n",
            "384/448, train_loss: 0.1944\n",
            "385/448, train_loss: 0.1926\n",
            "386/448, train_loss: 0.1325\n",
            "387/448, train_loss: 0.1334\n",
            "388/448, train_loss: 0.1425\n",
            "389/448, train_loss: 0.2765\n",
            "390/448, train_loss: 0.0967\n",
            "391/448, train_loss: 0.1236\n",
            "392/448, train_loss: 0.1542\n",
            "393/448, train_loss: 0.1522\n",
            "394/448, train_loss: 0.1238\n",
            "395/448, train_loss: 0.1883\n",
            "396/448, train_loss: 0.1561\n",
            "397/448, train_loss: 0.2487\n",
            "398/448, train_loss: 0.4055\n",
            "399/448, train_loss: 0.2023\n",
            "400/448, train_loss: 0.2390\n",
            "401/448, train_loss: 0.0977\n",
            "402/448, train_loss: 0.1967\n",
            "403/448, train_loss: 0.1675\n",
            "404/448, train_loss: 0.2734\n",
            "405/448, train_loss: 0.1095\n",
            "406/448, train_loss: 0.1652\n",
            "407/448, train_loss: 0.2789\n",
            "408/448, train_loss: 0.1846\n",
            "409/448, train_loss: 0.1626\n",
            "410/448, train_loss: 0.1281\n",
            "411/448, train_loss: 0.1336\n",
            "412/448, train_loss: 0.2442\n",
            "413/448, train_loss: 0.1809\n",
            "414/448, train_loss: 0.5609\n",
            "415/448, train_loss: 0.1388\n",
            "416/448, train_loss: 0.0931\n",
            "417/448, train_loss: 0.1497\n",
            "418/448, train_loss: 0.1953\n",
            "419/448, train_loss: 0.1308\n",
            "420/448, train_loss: 0.2397\n",
            "421/448, train_loss: 0.1264\n",
            "422/448, train_loss: 0.0925\n",
            "423/448, train_loss: 0.1034\n",
            "424/448, train_loss: 0.2816\n",
            "425/448, train_loss: 0.0677\n",
            "426/448, train_loss: 0.0892\n",
            "427/448, train_loss: 0.1050\n",
            "428/448, train_loss: 0.1650\n",
            "429/448, train_loss: 0.2114\n",
            "430/448, train_loss: 0.2071\n",
            "431/448, train_loss: 0.3028\n",
            "432/448, train_loss: 0.1372\n",
            "433/448, train_loss: 0.1915\n",
            "434/448, train_loss: 0.1516\n",
            "435/448, train_loss: 0.1675\n",
            "436/448, train_loss: 0.1445\n",
            "437/448, train_loss: 0.2011\n",
            "438/448, train_loss: 0.1393\n",
            "439/448, train_loss: 0.1367\n",
            "440/448, train_loss: 0.1519\n",
            "441/448, train_loss: 0.2178\n",
            "442/448, train_loss: 0.1392\n",
            "443/448, train_loss: 0.1302\n",
            "444/448, train_loss: 0.1419\n",
            "445/448, train_loss: 0.1340\n",
            "446/448, train_loss: 0.1314\n",
            "447/448, train_loss: 0.1520\n",
            "448/448, train_loss: 0.1517\n",
            "449/448, train_loss: 0.1903\n",
            "epoch 47 average loss: 0.1715\n",
            "current epoch: 47 current mean dice: 0.8337 best mean dice: 0.8339 at epoch: 44\n",
            "----------\n",
            "epoch 48/50\n",
            "1/448, train_loss: 0.2059\n",
            "2/448, train_loss: 0.1338\n",
            "3/448, train_loss: 0.1154\n",
            "4/448, train_loss: 0.1193\n",
            "5/448, train_loss: 0.1387\n",
            "6/448, train_loss: 0.1243\n",
            "7/448, train_loss: 0.1241\n",
            "8/448, train_loss: 0.3114\n",
            "9/448, train_loss: 0.2207\n",
            "10/448, train_loss: 0.1567\n",
            "11/448, train_loss: 0.1583\n",
            "12/448, train_loss: 0.1719\n",
            "13/448, train_loss: 0.1152\n",
            "14/448, train_loss: 0.1769\n",
            "15/448, train_loss: 0.1681\n",
            "16/448, train_loss: 0.1239\n",
            "17/448, train_loss: 0.1519\n",
            "18/448, train_loss: 0.3054\n",
            "19/448, train_loss: 0.0993\n",
            "20/448, train_loss: 0.1360\n",
            "21/448, train_loss: 0.2563\n",
            "22/448, train_loss: 0.1890\n",
            "23/448, train_loss: 0.1614\n",
            "24/448, train_loss: 0.1401\n",
            "25/448, train_loss: 0.0942\n",
            "26/448, train_loss: 0.1715\n",
            "27/448, train_loss: 0.2188\n",
            "28/448, train_loss: 0.0790\n",
            "29/448, train_loss: 0.1021\n",
            "30/448, train_loss: 0.1624\n",
            "31/448, train_loss: 0.1555\n",
            "32/448, train_loss: 0.0701\n",
            "33/448, train_loss: 0.2174\n",
            "34/448, train_loss: 0.1838\n",
            "35/448, train_loss: 0.2254\n",
            "36/448, train_loss: 0.1669\n",
            "37/448, train_loss: 0.1766\n",
            "38/448, train_loss: 0.3234\n",
            "39/448, train_loss: 0.4943\n",
            "40/448, train_loss: 0.2457\n",
            "41/448, train_loss: 0.1795\n",
            "42/448, train_loss: 0.1262\n",
            "43/448, train_loss: 0.2135\n",
            "44/448, train_loss: 0.1239\n",
            "45/448, train_loss: 0.2111\n",
            "46/448, train_loss: 0.2473\n",
            "47/448, train_loss: 0.0934\n",
            "48/448, train_loss: 0.2675\n",
            "49/448, train_loss: 0.1785\n",
            "50/448, train_loss: 0.2403\n",
            "51/448, train_loss: 0.1439\n",
            "52/448, train_loss: 0.1489\n",
            "53/448, train_loss: 0.1381\n",
            "54/448, train_loss: 0.1546\n",
            "55/448, train_loss: 0.3648\n",
            "56/448, train_loss: 0.1424\n",
            "57/448, train_loss: 0.1807\n",
            "58/448, train_loss: 0.1509\n",
            "59/448, train_loss: 0.1435\n",
            "60/448, train_loss: 0.2127\n",
            "61/448, train_loss: 0.1737\n",
            "62/448, train_loss: 0.1664\n",
            "63/448, train_loss: 0.1648\n",
            "64/448, train_loss: 0.1101\n",
            "65/448, train_loss: 0.2715\n",
            "66/448, train_loss: 0.0998\n",
            "67/448, train_loss: 0.2279\n",
            "68/448, train_loss: 0.1969\n",
            "69/448, train_loss: 0.0723\n",
            "70/448, train_loss: 0.1009\n",
            "71/448, train_loss: 0.1634\n",
            "72/448, train_loss: 0.1725\n",
            "73/448, train_loss: 0.1254\n",
            "74/448, train_loss: 0.1627\n",
            "75/448, train_loss: 0.1442\n",
            "76/448, train_loss: 0.1392\n",
            "77/448, train_loss: 0.1180\n",
            "78/448, train_loss: 0.1133\n",
            "79/448, train_loss: 0.0559\n",
            "80/448, train_loss: 0.1395\n",
            "81/448, train_loss: 0.1485\n",
            "82/448, train_loss: 0.1776\n",
            "83/448, train_loss: 0.3255\n",
            "84/448, train_loss: 0.1026\n",
            "85/448, train_loss: 0.1432\n",
            "86/448, train_loss: 0.1660\n",
            "87/448, train_loss: 0.1292\n",
            "88/448, train_loss: 0.3156\n",
            "89/448, train_loss: 0.1855\n",
            "90/448, train_loss: 0.1065\n",
            "91/448, train_loss: 0.1083\n",
            "92/448, train_loss: 0.2074\n",
            "93/448, train_loss: 0.2063\n",
            "94/448, train_loss: 0.2961\n",
            "95/448, train_loss: 0.4652\n",
            "96/448, train_loss: 0.1696\n",
            "97/448, train_loss: 0.2645\n",
            "98/448, train_loss: 0.1327\n",
            "99/448, train_loss: 0.1466\n",
            "100/448, train_loss: 0.1554\n",
            "101/448, train_loss: 0.2500\n",
            "102/448, train_loss: 0.4974\n",
            "103/448, train_loss: 0.1556\n",
            "104/448, train_loss: 0.0984\n",
            "105/448, train_loss: 0.1817\n",
            "106/448, train_loss: 0.1810\n",
            "107/448, train_loss: 0.1786\n",
            "108/448, train_loss: 0.1213\n",
            "109/448, train_loss: 0.1940\n",
            "110/448, train_loss: 0.2368\n",
            "111/448, train_loss: 0.1727\n",
            "112/448, train_loss: 0.1636\n",
            "113/448, train_loss: 0.1358\n",
            "114/448, train_loss: 0.3229\n",
            "115/448, train_loss: 0.4787\n",
            "116/448, train_loss: 0.1380\n",
            "117/448, train_loss: 0.1713\n",
            "118/448, train_loss: 0.1578\n",
            "119/448, train_loss: 0.1528\n",
            "120/448, train_loss: 0.1609\n",
            "121/448, train_loss: 0.1370\n",
            "122/448, train_loss: 0.1078\n",
            "123/448, train_loss: 0.2078\n",
            "124/448, train_loss: 0.2239\n",
            "125/448, train_loss: 0.1803\n",
            "126/448, train_loss: 0.1795\n",
            "127/448, train_loss: 0.1435\n",
            "128/448, train_loss: 0.1875\n",
            "129/448, train_loss: 0.1546\n",
            "130/448, train_loss: 0.2128\n",
            "131/448, train_loss: 0.1482\n",
            "132/448, train_loss: 0.1252\n",
            "133/448, train_loss: 0.1815\n",
            "134/448, train_loss: 0.1869\n",
            "135/448, train_loss: 0.1852\n",
            "136/448, train_loss: 0.1033\n",
            "137/448, train_loss: 0.1319\n",
            "138/448, train_loss: 0.1701\n",
            "139/448, train_loss: 0.1283\n",
            "140/448, train_loss: 0.1931\n",
            "141/448, train_loss: 0.0994\n",
            "142/448, train_loss: 0.1258\n",
            "143/448, train_loss: 0.2084\n",
            "144/448, train_loss: 0.1273\n",
            "145/448, train_loss: 0.1267\n",
            "146/448, train_loss: 0.2164\n",
            "147/448, train_loss: 0.1588\n",
            "148/448, train_loss: 0.2345\n",
            "149/448, train_loss: 0.0941\n",
            "150/448, train_loss: 0.1388\n",
            "151/448, train_loss: 0.2295\n",
            "152/448, train_loss: 0.2524\n",
            "153/448, train_loss: 0.1796\n",
            "154/448, train_loss: 0.2489\n",
            "155/448, train_loss: 0.1517\n",
            "156/448, train_loss: 0.1320\n",
            "157/448, train_loss: 0.2027\n",
            "158/448, train_loss: 0.2105\n",
            "159/448, train_loss: 0.1906\n",
            "160/448, train_loss: 0.2185\n",
            "161/448, train_loss: 0.1340\n",
            "162/448, train_loss: 0.1747\n",
            "163/448, train_loss: 0.1898\n",
            "164/448, train_loss: 0.1680\n",
            "165/448, train_loss: 0.1705\n",
            "166/448, train_loss: 0.2196\n",
            "167/448, train_loss: 0.0956\n",
            "168/448, train_loss: 0.1528\n",
            "169/448, train_loss: 0.1657\n",
            "170/448, train_loss: 0.1487\n",
            "171/448, train_loss: 0.0885\n",
            "172/448, train_loss: 0.1690\n",
            "173/448, train_loss: 0.1370\n",
            "174/448, train_loss: 0.1218\n",
            "175/448, train_loss: 0.1290\n",
            "176/448, train_loss: 0.3499\n",
            "177/448, train_loss: 0.2297\n",
            "178/448, train_loss: 0.1757\n",
            "179/448, train_loss: 0.1057\n",
            "180/448, train_loss: 0.1908\n",
            "181/448, train_loss: 0.1337\n",
            "182/448, train_loss: 0.1300\n",
            "183/448, train_loss: 0.2399\n",
            "184/448, train_loss: 0.1736\n",
            "185/448, train_loss: 0.1488\n",
            "186/448, train_loss: 0.1531\n",
            "187/448, train_loss: 0.2119\n",
            "188/448, train_loss: 0.1674\n",
            "189/448, train_loss: 0.2655\n",
            "190/448, train_loss: 0.1305\n",
            "191/448, train_loss: 0.1750\n",
            "192/448, train_loss: 0.1336\n",
            "193/448, train_loss: 0.0912\n",
            "194/448, train_loss: 0.2428\n",
            "195/448, train_loss: 0.0798\n",
            "196/448, train_loss: 0.1358\n",
            "197/448, train_loss: 0.1374\n",
            "198/448, train_loss: 0.1106\n",
            "199/448, train_loss: 0.0916\n",
            "200/448, train_loss: 0.2630\n",
            "201/448, train_loss: 0.1411\n",
            "202/448, train_loss: 0.4470\n",
            "203/448, train_loss: 0.1365\n",
            "204/448, train_loss: 0.1751\n",
            "205/448, train_loss: 0.1397\n",
            "206/448, train_loss: 0.0977\n",
            "207/448, train_loss: 0.2490\n",
            "208/448, train_loss: 0.1091\n",
            "209/448, train_loss: 0.1682\n",
            "210/448, train_loss: 0.2146\n",
            "211/448, train_loss: 0.1886\n",
            "212/448, train_loss: 0.1302\n",
            "213/448, train_loss: 0.1589\n",
            "214/448, train_loss: 0.1618\n",
            "215/448, train_loss: 0.2696\n",
            "216/448, train_loss: 0.1027\n",
            "217/448, train_loss: 0.1415\n",
            "218/448, train_loss: 0.1764\n",
            "219/448, train_loss: 0.1686\n",
            "220/448, train_loss: 0.1454\n",
            "221/448, train_loss: 0.1495\n",
            "222/448, train_loss: 0.1351\n",
            "223/448, train_loss: 0.1641\n",
            "224/448, train_loss: 0.1453\n",
            "225/448, train_loss: 0.1293\n",
            "226/448, train_loss: 0.2394\n",
            "227/448, train_loss: 0.1395\n",
            "228/448, train_loss: 0.2797\n",
            "229/448, train_loss: 0.0572\n",
            "230/448, train_loss: 0.1981\n",
            "231/448, train_loss: 0.1465\n",
            "232/448, train_loss: 0.1214\n",
            "233/448, train_loss: 0.0672\n",
            "234/448, train_loss: 0.1429\n",
            "235/448, train_loss: 0.1770\n",
            "236/448, train_loss: 0.0911\n",
            "237/448, train_loss: 0.2704\n",
            "238/448, train_loss: 0.1850\n",
            "239/448, train_loss: 0.1287\n",
            "240/448, train_loss: 0.1620\n",
            "241/448, train_loss: 0.2303\n",
            "242/448, train_loss: 0.1598\n",
            "243/448, train_loss: 0.1051\n",
            "244/448, train_loss: 0.1959\n",
            "245/448, train_loss: 0.1741\n",
            "246/448, train_loss: 0.1335\n",
            "247/448, train_loss: 0.1387\n",
            "248/448, train_loss: 0.1368\n",
            "249/448, train_loss: 0.1829\n",
            "250/448, train_loss: 0.1291\n",
            "251/448, train_loss: 0.1290\n",
            "252/448, train_loss: 0.1880\n",
            "253/448, train_loss: 0.1777\n",
            "254/448, train_loss: 0.2086\n",
            "255/448, train_loss: 0.1306\n",
            "256/448, train_loss: 0.2322\n",
            "257/448, train_loss: 0.1264\n",
            "258/448, train_loss: 0.2157\n",
            "259/448, train_loss: 0.1318\n",
            "260/448, train_loss: 0.1283\n",
            "261/448, train_loss: 0.2904\n",
            "262/448, train_loss: 0.1519\n",
            "263/448, train_loss: 0.1712\n",
            "264/448, train_loss: 0.1360\n",
            "265/448, train_loss: 0.1667\n",
            "266/448, train_loss: 0.1466\n",
            "267/448, train_loss: 0.2141\n",
            "268/448, train_loss: 0.1348\n",
            "269/448, train_loss: 0.2067\n",
            "270/448, train_loss: 0.2021\n",
            "271/448, train_loss: 0.1790\n",
            "272/448, train_loss: 0.3874\n",
            "273/448, train_loss: 0.1236\n",
            "274/448, train_loss: 0.0933\n",
            "275/448, train_loss: 0.0970\n",
            "276/448, train_loss: 0.2504\n",
            "277/448, train_loss: 0.1972\n",
            "278/448, train_loss: 0.1363\n",
            "279/448, train_loss: 0.0980\n",
            "280/448, train_loss: 0.2095\n",
            "281/448, train_loss: 0.1282\n",
            "282/448, train_loss: 0.1322\n",
            "283/448, train_loss: 0.1645\n",
            "284/448, train_loss: 0.1368\n",
            "285/448, train_loss: 0.1261\n",
            "286/448, train_loss: 0.2499\n",
            "287/448, train_loss: 0.1715\n",
            "288/448, train_loss: 0.1028\n",
            "289/448, train_loss: 0.0992\n",
            "290/448, train_loss: 0.0920\n",
            "291/448, train_loss: 0.2806\n",
            "292/448, train_loss: 0.1150\n",
            "293/448, train_loss: 0.1733\n",
            "294/448, train_loss: 0.1357\n",
            "295/448, train_loss: 0.1546\n",
            "296/448, train_loss: 0.1851\n",
            "297/448, train_loss: 0.2522\n",
            "298/448, train_loss: 0.2217\n",
            "299/448, train_loss: 0.1352\n",
            "300/448, train_loss: 0.1269\n",
            "301/448, train_loss: 0.1848\n",
            "302/448, train_loss: 0.1655\n",
            "303/448, train_loss: 0.1361\n",
            "304/448, train_loss: 0.1672\n",
            "305/448, train_loss: 0.2028\n",
            "306/448, train_loss: 0.1066\n",
            "307/448, train_loss: 0.0969\n",
            "308/448, train_loss: 0.1362\n",
            "309/448, train_loss: 0.1948\n",
            "310/448, train_loss: 0.1558\n",
            "311/448, train_loss: 0.2189\n",
            "312/448, train_loss: 0.1255\n",
            "313/448, train_loss: 0.1441\n",
            "314/448, train_loss: 0.1040\n",
            "315/448, train_loss: 0.2042\n",
            "316/448, train_loss: 0.1317\n",
            "317/448, train_loss: 0.1788\n",
            "318/448, train_loss: 0.2085\n",
            "319/448, train_loss: 0.2089\n",
            "320/448, train_loss: 0.1697\n",
            "321/448, train_loss: 0.1354\n",
            "322/448, train_loss: 0.1894\n",
            "323/448, train_loss: 0.1101\n",
            "324/448, train_loss: 0.1371\n",
            "325/448, train_loss: 0.1644\n",
            "326/448, train_loss: 0.1700\n",
            "327/448, train_loss: 0.2346\n",
            "328/448, train_loss: 0.1010\n",
            "329/448, train_loss: 0.1680\n",
            "330/448, train_loss: 0.1764\n",
            "331/448, train_loss: 0.2108\n",
            "332/448, train_loss: 0.1379\n",
            "333/448, train_loss: 0.1372\n",
            "334/448, train_loss: 0.2046\n",
            "335/448, train_loss: 0.1525\n",
            "336/448, train_loss: 0.1481\n",
            "337/448, train_loss: 0.1924\n",
            "338/448, train_loss: 0.1005\n",
            "339/448, train_loss: 0.0662\n",
            "340/448, train_loss: 0.1592\n",
            "341/448, train_loss: 0.1883\n",
            "342/448, train_loss: 0.1360\n",
            "343/448, train_loss: 0.1865\n",
            "344/448, train_loss: 0.1516\n",
            "345/448, train_loss: 0.1513\n",
            "346/448, train_loss: 0.0985\n",
            "347/448, train_loss: 0.1783\n",
            "348/448, train_loss: 0.0785\n",
            "349/448, train_loss: 0.3180\n",
            "350/448, train_loss: 0.1418\n",
            "351/448, train_loss: 0.1806\n",
            "352/448, train_loss: 0.0951\n",
            "353/448, train_loss: 0.3098\n",
            "354/448, train_loss: 0.1826\n",
            "355/448, train_loss: 0.1002\n",
            "356/448, train_loss: 0.1544\n",
            "357/448, train_loss: 0.1705\n",
            "358/448, train_loss: 0.2125\n",
            "359/448, train_loss: 0.1654\n",
            "360/448, train_loss: 0.1406\n",
            "361/448, train_loss: 0.1127\n",
            "362/448, train_loss: 0.0923\n",
            "363/448, train_loss: 0.2147\n",
            "364/448, train_loss: 0.1700\n",
            "365/448, train_loss: 0.2233\n",
            "366/448, train_loss: 0.1415\n",
            "367/448, train_loss: 0.1295\n",
            "368/448, train_loss: 0.2057\n",
            "369/448, train_loss: 0.1769\n",
            "370/448, train_loss: 0.0892\n",
            "371/448, train_loss: 0.1909\n",
            "372/448, train_loss: 0.1305\n",
            "373/448, train_loss: 0.1238\n",
            "374/448, train_loss: 0.1699\n",
            "375/448, train_loss: 0.1284\n",
            "376/448, train_loss: 0.1382\n",
            "377/448, train_loss: 0.1667\n",
            "378/448, train_loss: 0.1382\n",
            "379/448, train_loss: 0.1965\n",
            "380/448, train_loss: 0.1803\n",
            "381/448, train_loss: 0.1397\n",
            "382/448, train_loss: 0.0902\n",
            "383/448, train_loss: 0.1854\n",
            "384/448, train_loss: 0.1277\n",
            "385/448, train_loss: 0.2174\n",
            "386/448, train_loss: 0.1732\n",
            "387/448, train_loss: 0.1037\n",
            "388/448, train_loss: 0.3418\n",
            "389/448, train_loss: 0.1436\n",
            "390/448, train_loss: 0.0610\n",
            "391/448, train_loss: 0.1260\n",
            "392/448, train_loss: 0.0941\n",
            "393/448, train_loss: 0.1017\n",
            "394/448, train_loss: 0.1033\n",
            "395/448, train_loss: 0.1099\n",
            "396/448, train_loss: 0.2317\n",
            "397/448, train_loss: 0.0951\n",
            "398/448, train_loss: 0.2561\n",
            "399/448, train_loss: 0.3147\n",
            "400/448, train_loss: 0.1705\n",
            "401/448, train_loss: 0.1697\n",
            "402/448, train_loss: 0.2200\n",
            "403/448, train_loss: 0.1254\n",
            "404/448, train_loss: 0.2142\n",
            "405/448, train_loss: 0.0867\n",
            "406/448, train_loss: 0.1323\n",
            "407/448, train_loss: 0.1610\n",
            "408/448, train_loss: 0.1348\n",
            "409/448, train_loss: 0.1380\n",
            "410/448, train_loss: 0.2172\n",
            "411/448, train_loss: 0.0981\n",
            "412/448, train_loss: 0.1310\n",
            "413/448, train_loss: 0.1954\n",
            "414/448, train_loss: 0.1239\n",
            "415/448, train_loss: 0.1262\n",
            "416/448, train_loss: 0.1406\n",
            "417/448, train_loss: 0.1728\n",
            "418/448, train_loss: 0.1840\n",
            "419/448, train_loss: 0.1738\n",
            "420/448, train_loss: 0.0937\n",
            "421/448, train_loss: 0.1610\n",
            "422/448, train_loss: 0.1614\n",
            "423/448, train_loss: 0.2180\n",
            "424/448, train_loss: 0.1453\n",
            "425/448, train_loss: 0.2676\n",
            "426/448, train_loss: 0.1308\n",
            "427/448, train_loss: 0.1727\n",
            "428/448, train_loss: 0.1203\n",
            "429/448, train_loss: 0.2818\n",
            "430/448, train_loss: 0.1306\n",
            "431/448, train_loss: 0.0686\n",
            "432/448, train_loss: 0.0949\n",
            "433/448, train_loss: 0.2555\n",
            "434/448, train_loss: 0.1257\n",
            "435/448, train_loss: 0.1527\n",
            "436/448, train_loss: 0.2583\n",
            "437/448, train_loss: 0.1086\n",
            "438/448, train_loss: 0.2332\n",
            "439/448, train_loss: 0.1421\n",
            "440/448, train_loss: 0.2092\n",
            "441/448, train_loss: 0.2092\n",
            "442/448, train_loss: 0.1705\n",
            "443/448, train_loss: 0.1865\n",
            "444/448, train_loss: 0.1976\n",
            "445/448, train_loss: 0.1316\n",
            "446/448, train_loss: 0.1686\n",
            "447/448, train_loss: 0.3114\n",
            "448/448, train_loss: 0.2425\n",
            "449/448, train_loss: 0.2031\n",
            "epoch 48 average loss: 0.1695\n",
            "current epoch: 48 current mean dice: 0.8311 best mean dice: 0.8339 at epoch: 44\n",
            "----------\n",
            "epoch 49/50\n",
            "1/448, train_loss: 0.1538\n",
            "2/448, train_loss: 0.2594\n",
            "3/448, train_loss: 0.2439\n",
            "4/448, train_loss: 0.1299\n",
            "5/448, train_loss: 0.1751\n",
            "6/448, train_loss: 0.2258\n",
            "7/448, train_loss: 0.0966\n",
            "8/448, train_loss: 0.3220\n",
            "9/448, train_loss: 0.2253\n",
            "10/448, train_loss: 0.1750\n",
            "11/448, train_loss: 0.1792\n",
            "12/448, train_loss: 0.1724\n",
            "13/448, train_loss: 0.2104\n",
            "14/448, train_loss: 0.1914\n",
            "15/448, train_loss: 0.1761\n",
            "16/448, train_loss: 0.1598\n",
            "17/448, train_loss: 0.1737\n",
            "18/448, train_loss: 0.2057\n",
            "19/448, train_loss: 0.1706\n",
            "20/448, train_loss: 0.1447\n",
            "21/448, train_loss: 0.1078\n",
            "22/448, train_loss: 0.2145\n",
            "23/448, train_loss: 0.1166\n",
            "24/448, train_loss: 0.1156\n",
            "25/448, train_loss: 0.1667\n",
            "26/448, train_loss: 0.0963\n",
            "27/448, train_loss: 0.1254\n",
            "28/448, train_loss: 0.2575\n",
            "29/448, train_loss: 0.1932\n",
            "30/448, train_loss: 0.3752\n",
            "31/448, train_loss: 0.0995\n",
            "32/448, train_loss: 0.1657\n",
            "33/448, train_loss: 0.1711\n",
            "34/448, train_loss: 0.2343\n",
            "35/448, train_loss: 0.2020\n",
            "36/448, train_loss: 0.1781\n",
            "37/448, train_loss: 0.1924\n",
            "38/448, train_loss: 0.2020\n",
            "39/448, train_loss: 0.0902\n",
            "40/448, train_loss: 0.2282\n",
            "41/448, train_loss: 0.1404\n",
            "42/448, train_loss: 0.1448\n",
            "43/448, train_loss: 0.1485\n",
            "44/448, train_loss: 0.0570\n",
            "45/448, train_loss: 0.1262\n",
            "46/448, train_loss: 0.1145\n",
            "47/448, train_loss: 0.1947\n",
            "48/448, train_loss: 0.1276\n",
            "49/448, train_loss: 0.1640\n",
            "50/448, train_loss: 0.1656\n",
            "51/448, train_loss: 0.1544\n",
            "52/448, train_loss: 0.4009\n",
            "53/448, train_loss: 0.1550\n",
            "54/448, train_loss: 0.0992\n",
            "55/448, train_loss: 0.1005\n",
            "56/448, train_loss: 0.2689\n",
            "57/448, train_loss: 0.1525\n",
            "58/448, train_loss: 0.1963\n",
            "59/448, train_loss: 0.1355\n",
            "60/448, train_loss: 0.1599\n",
            "61/448, train_loss: 0.1745\n",
            "62/448, train_loss: 0.2074\n",
            "63/448, train_loss: 0.2170\n",
            "64/448, train_loss: 0.1275\n",
            "65/448, train_loss: 0.1803\n",
            "66/448, train_loss: 0.1342\n",
            "67/448, train_loss: 0.2332\n",
            "68/448, train_loss: 0.1688\n",
            "69/448, train_loss: 0.1795\n",
            "70/448, train_loss: 0.1007\n",
            "71/448, train_loss: 0.1587\n",
            "72/448, train_loss: 0.2094\n",
            "73/448, train_loss: 0.0987\n",
            "74/448, train_loss: 0.1798\n",
            "75/448, train_loss: 0.1347\n",
            "76/448, train_loss: 0.1603\n",
            "77/448, train_loss: 0.1726\n",
            "78/448, train_loss: 0.3034\n",
            "79/448, train_loss: 0.1425\n",
            "80/448, train_loss: 0.1014\n",
            "81/448, train_loss: 0.1474\n",
            "82/448, train_loss: 0.1322\n",
            "83/448, train_loss: 0.1702\n",
            "84/448, train_loss: 0.4574\n",
            "85/448, train_loss: 0.2302\n",
            "86/448, train_loss: 0.1035\n",
            "87/448, train_loss: 0.1937\n",
            "88/448, train_loss: 0.0816\n",
            "89/448, train_loss: 0.1229\n",
            "90/448, train_loss: 0.1387\n",
            "91/448, train_loss: 0.1464\n",
            "92/448, train_loss: 0.1955\n",
            "93/448, train_loss: 0.0979\n",
            "94/448, train_loss: 0.1231\n",
            "95/448, train_loss: 0.1917\n",
            "96/448, train_loss: 0.2036\n",
            "97/448, train_loss: 0.1350\n",
            "98/448, train_loss: 0.1587\n",
            "99/448, train_loss: 0.1323\n",
            "100/448, train_loss: 0.1160\n",
            "101/448, train_loss: 0.1018\n",
            "102/448, train_loss: 0.1908\n",
            "103/448, train_loss: 0.2236\n",
            "104/448, train_loss: 0.0893\n",
            "105/448, train_loss: 0.1648\n",
            "106/448, train_loss: 0.0967\n",
            "107/448, train_loss: 0.1623\n",
            "108/448, train_loss: 0.1485\n",
            "109/448, train_loss: 0.1374\n",
            "110/448, train_loss: 0.2402\n",
            "111/448, train_loss: 0.1620\n",
            "112/448, train_loss: 0.1396\n",
            "113/448, train_loss: 0.1219\n",
            "114/448, train_loss: 0.1811\n",
            "115/448, train_loss: 0.2535\n",
            "116/448, train_loss: 0.2906\n",
            "117/448, train_loss: 0.1376\n",
            "118/448, train_loss: 0.1195\n",
            "119/448, train_loss: 0.1433\n",
            "120/448, train_loss: 0.2300\n",
            "121/448, train_loss: 0.2367\n",
            "122/448, train_loss: 0.0980\n",
            "123/448, train_loss: 0.1368\n",
            "124/448, train_loss: 0.2515\n",
            "125/448, train_loss: 0.2752\n",
            "126/448, train_loss: 0.1983\n",
            "127/448, train_loss: 0.1658\n",
            "128/448, train_loss: 0.2034\n",
            "129/448, train_loss: 0.1440\n",
            "130/448, train_loss: 0.2289\n",
            "131/448, train_loss: 0.1750\n",
            "132/448, train_loss: 0.2215\n",
            "133/448, train_loss: 0.1428\n",
            "134/448, train_loss: 0.1605\n",
            "135/448, train_loss: 0.3714\n",
            "136/448, train_loss: 0.1695\n",
            "137/448, train_loss: 0.1829\n",
            "138/448, train_loss: 0.1024\n",
            "139/448, train_loss: 0.1575\n",
            "140/448, train_loss: 0.1108\n",
            "141/448, train_loss: 0.1334\n",
            "142/448, train_loss: 0.2345\n",
            "143/448, train_loss: 0.1816\n",
            "144/448, train_loss: 0.1584\n",
            "145/448, train_loss: 0.2379\n",
            "146/448, train_loss: 0.1759\n",
            "147/448, train_loss: 0.3144\n",
            "148/448, train_loss: 0.1376\n",
            "149/448, train_loss: 0.1344\n",
            "150/448, train_loss: 0.0997\n",
            "151/448, train_loss: 0.1421\n",
            "152/448, train_loss: 0.1284\n",
            "153/448, train_loss: 0.1375\n",
            "154/448, train_loss: 0.1496\n",
            "155/448, train_loss: 0.1646\n",
            "156/448, train_loss: 0.1264\n",
            "157/448, train_loss: 0.1468\n",
            "158/448, train_loss: 0.2439\n",
            "159/448, train_loss: 0.1074\n",
            "160/448, train_loss: 0.2325\n",
            "161/448, train_loss: 0.1382\n",
            "162/448, train_loss: 0.1470\n",
            "163/448, train_loss: 0.2186\n",
            "164/448, train_loss: 0.2072\n",
            "165/448, train_loss: 0.1627\n",
            "166/448, train_loss: 0.1328\n",
            "167/448, train_loss: 0.0905\n",
            "168/448, train_loss: 0.1565\n",
            "169/448, train_loss: 0.2056\n",
            "170/448, train_loss: 0.1999\n",
            "171/448, train_loss: 0.1359\n",
            "172/448, train_loss: 0.1060\n",
            "173/448, train_loss: 0.1716\n",
            "174/448, train_loss: 0.1268\n",
            "175/448, train_loss: 0.1321\n",
            "176/448, train_loss: 0.1289\n",
            "177/448, train_loss: 0.0630\n",
            "178/448, train_loss: 0.1583\n",
            "179/448, train_loss: 0.2337\n",
            "180/448, train_loss: 0.2315\n",
            "181/448, train_loss: 0.1355\n",
            "182/448, train_loss: 0.1340\n",
            "183/448, train_loss: 0.1358\n",
            "184/448, train_loss: 0.1247\n",
            "185/448, train_loss: 0.1654\n",
            "186/448, train_loss: 0.2126\n",
            "187/448, train_loss: 0.1469\n",
            "188/448, train_loss: 0.1944\n",
            "189/448, train_loss: 0.1845\n",
            "190/448, train_loss: 0.2041\n",
            "191/448, train_loss: 0.1287\n",
            "192/448, train_loss: 0.1411\n",
            "193/448, train_loss: 0.0949\n",
            "194/448, train_loss: 0.1452\n",
            "195/448, train_loss: 0.1362\n",
            "196/448, train_loss: 0.1965\n",
            "197/448, train_loss: 0.1615\n",
            "198/448, train_loss: 0.1288\n",
            "199/448, train_loss: 0.1547\n",
            "200/448, train_loss: 0.1689\n",
            "201/448, train_loss: 0.2482\n",
            "202/448, train_loss: 0.1726\n",
            "203/448, train_loss: 0.0961\n",
            "204/448, train_loss: 0.1426\n",
            "205/448, train_loss: 0.1386\n",
            "206/448, train_loss: 0.1659\n",
            "207/448, train_loss: 0.1729\n",
            "208/448, train_loss: 0.1077\n",
            "209/448, train_loss: 0.2347\n",
            "210/448, train_loss: 0.1001\n",
            "211/448, train_loss: 0.1579\n",
            "212/448, train_loss: 0.2098\n",
            "213/448, train_loss: 0.1558\n",
            "214/448, train_loss: 0.4213\n",
            "215/448, train_loss: 0.2289\n",
            "216/448, train_loss: 0.1953\n",
            "217/448, train_loss: 0.1516\n",
            "218/448, train_loss: 0.1208\n",
            "219/448, train_loss: 0.2131\n",
            "220/448, train_loss: 0.1896\n",
            "221/448, train_loss: 0.1232\n",
            "222/448, train_loss: 0.2314\n",
            "223/448, train_loss: 0.1988\n",
            "224/448, train_loss: 0.2047\n",
            "225/448, train_loss: 0.2225\n",
            "226/448, train_loss: 0.1615\n",
            "227/448, train_loss: 0.2427\n",
            "228/448, train_loss: 0.0839\n",
            "229/448, train_loss: 0.1133\n",
            "230/448, train_loss: 0.1709\n",
            "231/448, train_loss: 0.1629\n",
            "232/448, train_loss: 0.2086\n",
            "233/448, train_loss: 0.1256\n",
            "234/448, train_loss: 0.2383\n",
            "235/448, train_loss: 0.1635\n",
            "236/448, train_loss: 0.2068\n",
            "237/448, train_loss: 0.1626\n",
            "238/448, train_loss: 0.1747\n",
            "239/448, train_loss: 0.1244\n",
            "240/448, train_loss: 0.1707\n",
            "241/448, train_loss: 0.2800\n",
            "242/448, train_loss: 0.1824\n",
            "243/448, train_loss: 0.1165\n",
            "244/448, train_loss: 0.2022\n",
            "245/448, train_loss: 0.1448\n",
            "246/448, train_loss: 0.2262\n",
            "247/448, train_loss: 0.2401\n",
            "248/448, train_loss: 0.1349\n",
            "249/448, train_loss: 0.1305\n",
            "250/448, train_loss: 0.2321\n",
            "251/448, train_loss: 0.1585\n",
            "252/448, train_loss: 0.1052\n",
            "253/448, train_loss: 0.1349\n",
            "254/448, train_loss: 0.1624\n",
            "255/448, train_loss: 0.1704\n",
            "256/448, train_loss: 0.1037\n",
            "257/448, train_loss: 0.0899\n",
            "258/448, train_loss: 0.1959\n",
            "259/448, train_loss: 0.1941\n",
            "260/448, train_loss: 0.2151\n",
            "261/448, train_loss: 0.1717\n",
            "262/448, train_loss: 0.1379\n",
            "263/448, train_loss: 0.0902\n",
            "264/448, train_loss: 0.1596\n",
            "265/448, train_loss: 0.1229\n",
            "266/448, train_loss: 0.1537\n",
            "267/448, train_loss: 0.1658\n",
            "268/448, train_loss: 0.1906\n",
            "269/448, train_loss: 0.1328\n",
            "270/448, train_loss: 0.1714\n",
            "271/448, train_loss: 0.1329\n",
            "272/448, train_loss: 0.1756\n",
            "273/448, train_loss: 0.1629\n",
            "274/448, train_loss: 0.1301\n",
            "275/448, train_loss: 0.1017\n",
            "276/448, train_loss: 0.1214\n",
            "277/448, train_loss: 0.1790\n",
            "278/448, train_loss: 0.1571\n",
            "279/448, train_loss: 0.0941\n",
            "280/448, train_loss: 0.2377\n",
            "281/448, train_loss: 0.1058\n",
            "282/448, train_loss: 0.1150\n",
            "283/448, train_loss: 0.1246\n",
            "284/448, train_loss: 0.1888\n",
            "285/448, train_loss: 0.0972\n",
            "286/448, train_loss: 0.1001\n",
            "287/448, train_loss: 0.4982\n",
            "288/448, train_loss: 0.1607\n",
            "289/448, train_loss: 0.1285\n",
            "290/448, train_loss: 0.1870\n",
            "291/448, train_loss: 0.2045\n",
            "292/448, train_loss: 0.1021\n",
            "293/448, train_loss: 0.1727\n",
            "294/448, train_loss: 0.1660\n",
            "295/448, train_loss: 0.2073\n",
            "296/448, train_loss: 0.2040\n",
            "297/448, train_loss: 0.2160\n",
            "298/448, train_loss: 0.1294\n",
            "299/448, train_loss: 0.1026\n",
            "300/448, train_loss: 0.1210\n",
            "301/448, train_loss: 0.1020\n",
            "302/448, train_loss: 0.1637\n",
            "303/448, train_loss: 0.1765\n",
            "304/448, train_loss: 0.1696\n",
            "305/448, train_loss: 0.2655\n",
            "306/448, train_loss: 0.1389\n",
            "307/448, train_loss: 0.1664\n",
            "308/448, train_loss: 0.1946\n",
            "309/448, train_loss: 0.0952\n",
            "310/448, train_loss: 0.1858\n",
            "311/448, train_loss: 0.1553\n",
            "312/448, train_loss: 0.3715\n",
            "313/448, train_loss: 0.1728\n",
            "314/448, train_loss: 0.1415\n",
            "315/448, train_loss: 0.0899\n",
            "316/448, train_loss: 0.1513\n",
            "317/448, train_loss: 0.1613\n",
            "318/448, train_loss: 0.2296\n",
            "319/448, train_loss: 0.0565\n",
            "320/448, train_loss: 0.2355\n",
            "321/448, train_loss: 0.1904\n",
            "322/448, train_loss: 0.1562\n",
            "323/448, train_loss: 0.1308\n",
            "324/448, train_loss: 0.1480\n",
            "325/448, train_loss: 0.2291\n",
            "326/448, train_loss: 0.1639\n",
            "327/448, train_loss: 0.0910\n",
            "328/448, train_loss: 0.2416\n",
            "329/448, train_loss: 0.1507\n",
            "330/448, train_loss: 0.1654\n",
            "331/448, train_loss: 0.1267\n",
            "332/448, train_loss: 0.1946\n",
            "333/448, train_loss: 0.3366\n",
            "334/448, train_loss: 0.1493\n",
            "335/448, train_loss: 0.2022\n",
            "336/448, train_loss: 0.2879\n",
            "337/448, train_loss: 0.2926\n",
            "338/448, train_loss: 0.1774\n",
            "339/448, train_loss: 0.1888\n",
            "340/448, train_loss: 0.1397\n",
            "341/448, train_loss: 0.1325\n",
            "342/448, train_loss: 0.2488\n",
            "343/448, train_loss: 0.2183\n",
            "344/448, train_loss: 0.1370\n",
            "345/448, train_loss: 0.1674\n",
            "346/448, train_loss: 0.1223\n",
            "347/448, train_loss: 0.1981\n",
            "348/448, train_loss: 0.1216\n",
            "349/448, train_loss: 0.1509\n",
            "350/448, train_loss: 0.2591\n",
            "351/448, train_loss: 0.1271\n",
            "352/448, train_loss: 0.1267\n",
            "353/448, train_loss: 0.0860\n",
            "354/448, train_loss: 0.1018\n",
            "355/448, train_loss: 0.1647\n",
            "356/448, train_loss: 0.1774\n",
            "357/448, train_loss: 0.2262\n",
            "358/448, train_loss: 0.1651\n",
            "359/448, train_loss: 0.0949\n",
            "360/448, train_loss: 0.1351\n",
            "361/448, train_loss: 0.1933\n",
            "362/448, train_loss: 0.1397\n",
            "363/448, train_loss: 0.1207\n",
            "364/448, train_loss: 0.1730\n",
            "365/448, train_loss: 0.1322\n",
            "366/448, train_loss: 0.1060\n",
            "367/448, train_loss: 0.1340\n",
            "368/448, train_loss: 0.1602\n",
            "369/448, train_loss: 0.2141\n",
            "370/448, train_loss: 0.1924\n",
            "371/448, train_loss: 0.1119\n",
            "372/448, train_loss: 0.1361\n",
            "373/448, train_loss: 0.1743\n",
            "374/448, train_loss: 0.1730\n",
            "375/448, train_loss: 0.1272\n",
            "376/448, train_loss: 0.2483\n",
            "377/448, train_loss: 0.2224\n",
            "378/448, train_loss: 0.1557\n",
            "379/448, train_loss: 0.2061\n",
            "380/448, train_loss: 0.1755\n",
            "381/448, train_loss: 0.1972\n",
            "382/448, train_loss: 0.1110\n",
            "383/448, train_loss: 0.2372\n",
            "384/448, train_loss: 0.1011\n",
            "385/448, train_loss: 0.1499\n",
            "386/448, train_loss: 0.1651\n",
            "387/448, train_loss: 0.1322\n",
            "388/448, train_loss: 0.1264\n",
            "389/448, train_loss: 0.1238\n",
            "390/448, train_loss: 0.2500\n",
            "391/448, train_loss: 0.2299\n",
            "392/448, train_loss: 0.1864\n",
            "393/448, train_loss: 0.1993\n",
            "394/448, train_loss: 0.1638\n",
            "395/448, train_loss: 0.1401\n",
            "396/448, train_loss: 0.3886\n",
            "397/448, train_loss: 0.1471\n",
            "398/448, train_loss: 0.1010\n",
            "399/448, train_loss: 0.2438\n",
            "400/448, train_loss: 0.1309\n",
            "401/448, train_loss: 0.1823\n",
            "402/448, train_loss: 0.1460\n",
            "403/448, train_loss: 0.2075\n",
            "404/448, train_loss: 0.2157\n",
            "405/448, train_loss: 0.0895\n",
            "406/448, train_loss: 0.0963\n",
            "407/448, train_loss: 0.2365\n",
            "408/448, train_loss: 0.1326\n",
            "409/448, train_loss: 0.1356\n",
            "410/448, train_loss: 0.2509\n",
            "411/448, train_loss: 0.1319\n",
            "412/448, train_loss: 0.1378\n",
            "413/448, train_loss: 0.1925\n",
            "414/448, train_loss: 0.1718\n",
            "415/448, train_loss: 0.1643\n",
            "416/448, train_loss: 0.2168\n",
            "417/448, train_loss: 0.1285\n",
            "418/448, train_loss: 0.1492\n",
            "419/448, train_loss: 0.1012\n",
            "420/448, train_loss: 0.2074\n",
            "421/448, train_loss: 0.2706\n",
            "422/448, train_loss: 0.2093\n",
            "423/448, train_loss: 0.1380\n",
            "424/448, train_loss: 0.1439\n",
            "425/448, train_loss: 0.1754\n",
            "426/448, train_loss: 0.1951\n",
            "427/448, train_loss: 0.1717\n",
            "428/448, train_loss: 0.1322\n",
            "429/448, train_loss: 0.0899\n",
            "430/448, train_loss: 0.2145\n",
            "431/448, train_loss: 0.2158\n",
            "432/448, train_loss: 0.1730\n",
            "433/448, train_loss: 0.1429\n",
            "434/448, train_loss: 0.0931\n",
            "435/448, train_loss: 0.2025\n",
            "436/448, train_loss: 0.1046\n",
            "437/448, train_loss: 0.0761\n",
            "438/448, train_loss: 0.1905\n",
            "439/448, train_loss: 0.3268\n",
            "440/448, train_loss: 0.1478\n",
            "441/448, train_loss: 0.2253\n",
            "442/448, train_loss: 0.1366\n",
            "443/448, train_loss: 0.1843\n",
            "444/448, train_loss: 0.1781\n",
            "445/448, train_loss: 0.1798\n",
            "446/448, train_loss: 0.0936\n",
            "447/448, train_loss: 0.1726\n",
            "448/448, train_loss: 0.2489\n",
            "449/448, train_loss: 0.0680\n",
            "epoch 49 average loss: 0.1695\n",
            "current epoch: 49 current mean dice: 0.8299 best mean dice: 0.8339 at epoch: 44\n",
            "----------\n",
            "epoch 50/50\n",
            "1/448, train_loss: 0.0920\n",
            "2/448, train_loss: 0.1687\n",
            "3/448, train_loss: 0.1142\n",
            "4/448, train_loss: 0.1007\n",
            "5/448, train_loss: 0.1516\n",
            "6/448, train_loss: 0.2269\n",
            "7/448, train_loss: 0.1043\n",
            "8/448, train_loss: 0.2190\n",
            "9/448, train_loss: 0.1314\n",
            "10/448, train_loss: 0.1656\n",
            "11/448, train_loss: 0.1749\n",
            "12/448, train_loss: 0.1659\n",
            "13/448, train_loss: 0.1311\n",
            "14/448, train_loss: 0.1569\n",
            "15/448, train_loss: 0.1695\n",
            "16/448, train_loss: 0.3293\n",
            "17/448, train_loss: 0.2127\n",
            "18/448, train_loss: 0.1262\n",
            "19/448, train_loss: 0.1242\n",
            "20/448, train_loss: 0.1417\n",
            "21/448, train_loss: 0.1500\n",
            "22/448, train_loss: 0.1715\n",
            "23/448, train_loss: 0.2188\n",
            "24/448, train_loss: 0.1646\n",
            "25/448, train_loss: 0.2140\n",
            "26/448, train_loss: 0.5103\n",
            "27/448, train_loss: 0.1319\n",
            "28/448, train_loss: 0.2057\n",
            "29/448, train_loss: 0.1728\n",
            "30/448, train_loss: 0.1466\n",
            "31/448, train_loss: 0.1176\n",
            "32/448, train_loss: 0.1513\n",
            "33/448, train_loss: 0.1824\n",
            "34/448, train_loss: 0.2299\n",
            "35/448, train_loss: 0.1459\n",
            "36/448, train_loss: 0.1317\n",
            "37/448, train_loss: 0.2055\n",
            "38/448, train_loss: 0.2481\n",
            "39/448, train_loss: 0.1401\n",
            "40/448, train_loss: 0.2558\n",
            "41/448, train_loss: 0.1369\n",
            "42/448, train_loss: 0.1657\n",
            "43/448, train_loss: 0.1392\n",
            "44/448, train_loss: 0.1748\n",
            "45/448, train_loss: 0.1238\n",
            "46/448, train_loss: 0.1867\n",
            "47/448, train_loss: 0.1454\n",
            "48/448, train_loss: 0.2114\n",
            "49/448, train_loss: 0.1331\n",
            "50/448, train_loss: 0.1559\n",
            "51/448, train_loss: 0.1610\n",
            "52/448, train_loss: 0.2198\n",
            "53/448, train_loss: 0.1682\n",
            "54/448, train_loss: 0.1270\n",
            "55/448, train_loss: 0.1404\n",
            "56/448, train_loss: 0.1071\n",
            "57/448, train_loss: 0.1561\n",
            "58/448, train_loss: 0.2941\n",
            "59/448, train_loss: 0.1302\n",
            "60/448, train_loss: 0.1352\n",
            "61/448, train_loss: 0.3188\n",
            "62/448, train_loss: 0.2450\n",
            "63/448, train_loss: 0.2035\n",
            "64/448, train_loss: 0.1148\n",
            "65/448, train_loss: 0.2014\n",
            "66/448, train_loss: 0.1998\n",
            "67/448, train_loss: 0.5546\n",
            "68/448, train_loss: 0.1060\n",
            "69/448, train_loss: 0.2640\n",
            "70/448, train_loss: 0.1457\n",
            "71/448, train_loss: 0.1387\n",
            "72/448, train_loss: 0.1733\n",
            "73/448, train_loss: 0.1329\n",
            "74/448, train_loss: 0.1614\n",
            "75/448, train_loss: 0.0969\n",
            "76/448, train_loss: 0.1324\n",
            "77/448, train_loss: 0.2005\n",
            "78/448, train_loss: 0.1839\n",
            "79/448, train_loss: 0.1730\n",
            "80/448, train_loss: 0.1644\n",
            "81/448, train_loss: 0.0923\n",
            "82/448, train_loss: 0.1675\n",
            "83/448, train_loss: 0.1403\n",
            "84/448, train_loss: 0.2906\n",
            "85/448, train_loss: 0.1701\n",
            "86/448, train_loss: 0.2338\n",
            "87/448, train_loss: 0.2702\n",
            "88/448, train_loss: 0.1586\n",
            "89/448, train_loss: 0.1269\n",
            "90/448, train_loss: 0.1755\n",
            "91/448, train_loss: 0.1284\n",
            "92/448, train_loss: 0.1376\n",
            "93/448, train_loss: 0.2799\n",
            "94/448, train_loss: 0.1331\n",
            "95/448, train_loss: 0.1611\n",
            "96/448, train_loss: 0.1280\n",
            "97/448, train_loss: 0.1607\n",
            "98/448, train_loss: 0.3884\n",
            "99/448, train_loss: 0.1688\n",
            "100/448, train_loss: 0.0898\n",
            "101/448, train_loss: 0.1650\n",
            "102/448, train_loss: 0.1340\n",
            "103/448, train_loss: 0.2806\n",
            "104/448, train_loss: 0.2010\n",
            "105/448, train_loss: 0.2518\n",
            "106/448, train_loss: 0.2298\n",
            "107/448, train_loss: 0.2116\n",
            "108/448, train_loss: 0.1322\n",
            "109/448, train_loss: 0.2472\n",
            "110/448, train_loss: 0.1097\n",
            "111/448, train_loss: 0.1222\n",
            "112/448, train_loss: 0.1930\n",
            "113/448, train_loss: 0.1206\n",
            "114/448, train_loss: 0.2031\n",
            "115/448, train_loss: 0.1580\n",
            "116/448, train_loss: 0.2045\n",
            "117/448, train_loss: 0.1594\n",
            "118/448, train_loss: 0.1514\n",
            "119/448, train_loss: 0.1031\n",
            "120/448, train_loss: 0.2707\n",
            "121/448, train_loss: 0.3087\n",
            "122/448, train_loss: 0.1713\n",
            "123/448, train_loss: 0.1731\n",
            "124/448, train_loss: 0.1394\n",
            "125/448, train_loss: 0.2216\n",
            "126/448, train_loss: 0.1509\n",
            "127/448, train_loss: 0.1784\n",
            "128/448, train_loss: 0.2034\n",
            "129/448, train_loss: 0.1858\n",
            "130/448, train_loss: 0.1507\n",
            "131/448, train_loss: 0.1219\n",
            "132/448, train_loss: 0.1758\n",
            "133/448, train_loss: 0.0979\n",
            "134/448, train_loss: 0.1590\n",
            "135/448, train_loss: 0.0945\n",
            "136/448, train_loss: 0.1157\n",
            "137/448, train_loss: 0.1876\n",
            "138/448, train_loss: 0.1000\n",
            "139/448, train_loss: 0.2713\n",
            "140/448, train_loss: 0.2085\n",
            "141/448, train_loss: 0.2115\n",
            "142/448, train_loss: 0.1423\n",
            "143/448, train_loss: 0.0609\n",
            "144/448, train_loss: 0.1706\n",
            "145/448, train_loss: 0.3379\n",
            "146/448, train_loss: 0.1958\n",
            "147/448, train_loss: 0.1687\n",
            "148/448, train_loss: 0.2867\n",
            "149/448, train_loss: 0.1267\n",
            "150/448, train_loss: 0.2268\n",
            "151/448, train_loss: 0.1902\n",
            "152/448, train_loss: 0.1779\n",
            "153/448, train_loss: 0.1740\n",
            "154/448, train_loss: 0.1572\n",
            "155/448, train_loss: 0.1438\n",
            "156/448, train_loss: 0.1808\n",
            "157/448, train_loss: 0.2806\n",
            "158/448, train_loss: 0.1705\n",
            "159/448, train_loss: 0.1879\n",
            "160/448, train_loss: 0.2651\n",
            "161/448, train_loss: 0.1709\n",
            "162/448, train_loss: 0.1036\n",
            "163/448, train_loss: 0.1166\n",
            "164/448, train_loss: 0.1991\n",
            "165/448, train_loss: 0.2494\n",
            "166/448, train_loss: 0.1246\n",
            "167/448, train_loss: 0.2114\n",
            "168/448, train_loss: 0.1573\n",
            "169/448, train_loss: 0.1007\n",
            "170/448, train_loss: 0.1934\n",
            "171/448, train_loss: 0.2032\n",
            "172/448, train_loss: 0.0988\n",
            "173/448, train_loss: 0.0916\n",
            "174/448, train_loss: 0.1395\n",
            "175/448, train_loss: 0.1326\n",
            "176/448, train_loss: 0.1938\n",
            "177/448, train_loss: 0.0996\n",
            "178/448, train_loss: 0.1731\n",
            "179/448, train_loss: 0.1289\n",
            "180/448, train_loss: 0.1331\n",
            "181/448, train_loss: 0.1589\n",
            "182/448, train_loss: 0.1558\n",
            "183/448, train_loss: 0.1332\n",
            "184/448, train_loss: 0.1280\n",
            "185/448, train_loss: 0.0936\n",
            "186/448, train_loss: 0.2080\n",
            "187/448, train_loss: 0.1226\n",
            "188/448, train_loss: 0.1741\n",
            "189/448, train_loss: 0.2231\n",
            "190/448, train_loss: 0.1550\n",
            "191/448, train_loss: 0.1688\n",
            "192/448, train_loss: 0.1267\n",
            "193/448, train_loss: 0.3208\n",
            "194/448, train_loss: 0.3131\n",
            "195/448, train_loss: 0.1479\n",
            "196/448, train_loss: 0.2375\n",
            "197/448, train_loss: 0.1388\n",
            "198/448, train_loss: 0.1353\n",
            "199/448, train_loss: 0.0976\n",
            "200/448, train_loss: 0.4225\n",
            "201/448, train_loss: 0.1905\n",
            "202/448, train_loss: 0.1787\n",
            "203/448, train_loss: 0.1735\n",
            "204/448, train_loss: 0.2125\n",
            "205/448, train_loss: 0.2618\n",
            "206/448, train_loss: 0.1276\n",
            "207/448, train_loss: 0.0908\n",
            "208/448, train_loss: 0.0968\n",
            "209/448, train_loss: 0.1812\n",
            "210/448, train_loss: 0.1666\n",
            "211/448, train_loss: 0.2377\n",
            "212/448, train_loss: 0.2250\n",
            "213/448, train_loss: 0.1383\n",
            "214/448, train_loss: 0.1853\n",
            "215/448, train_loss: 0.1154\n",
            "216/448, train_loss: 0.1755\n",
            "217/448, train_loss: 0.2581\n",
            "218/448, train_loss: 0.1004\n",
            "219/448, train_loss: 0.0983\n",
            "220/448, train_loss: 0.1598\n",
            "221/448, train_loss: 0.1863\n",
            "222/448, train_loss: 0.1305\n",
            "223/448, train_loss: 0.1377\n",
            "224/448, train_loss: 0.0900\n",
            "225/448, train_loss: 0.1725\n",
            "226/448, train_loss: 0.1297\n",
            "227/448, train_loss: 0.1042\n",
            "228/448, train_loss: 0.1054\n",
            "229/448, train_loss: 0.1460\n",
            "230/448, train_loss: 0.1040\n",
            "231/448, train_loss: 0.1709\n",
            "232/448, train_loss: 0.1684\n",
            "233/448, train_loss: 0.1899\n",
            "234/448, train_loss: 0.2000\n",
            "235/448, train_loss: 0.1900\n",
            "236/448, train_loss: 0.1194\n",
            "237/448, train_loss: 0.1836\n",
            "238/448, train_loss: 0.1690\n",
            "239/448, train_loss: 0.2769\n",
            "240/448, train_loss: 0.2040\n",
            "241/448, train_loss: 0.1031\n",
            "242/448, train_loss: 0.1234\n",
            "243/448, train_loss: 0.1836\n",
            "244/448, train_loss: 0.0946\n",
            "245/448, train_loss: 0.1043\n",
            "246/448, train_loss: 0.1735\n",
            "247/448, train_loss: 0.1492\n",
            "248/448, train_loss: 0.2528\n",
            "249/448, train_loss: 0.1482\n",
            "250/448, train_loss: 0.2637\n",
            "251/448, train_loss: 0.1350\n",
            "252/448, train_loss: 0.1014\n",
            "253/448, train_loss: 0.1162\n",
            "254/448, train_loss: 0.1426\n",
            "255/448, train_loss: 0.1285\n",
            "256/448, train_loss: 0.1965\n",
            "257/448, train_loss: 0.1451\n",
            "258/448, train_loss: 0.1237\n",
            "259/448, train_loss: 0.2255\n",
            "260/448, train_loss: 0.1451\n",
            "261/448, train_loss: 0.1491\n",
            "262/448, train_loss: 0.2034\n",
            "263/448, train_loss: 0.1800\n",
            "264/448, train_loss: 0.1879\n",
            "265/448, train_loss: 0.2671\n",
            "266/448, train_loss: 0.4317\n",
            "267/448, train_loss: 0.1764\n",
            "268/448, train_loss: 0.1548\n",
            "269/448, train_loss: 0.2167\n",
            "270/448, train_loss: 0.1370\n",
            "271/448, train_loss: 0.2030\n",
            "272/448, train_loss: 0.1286\n",
            "273/448, train_loss: 0.1223\n",
            "274/448, train_loss: 0.1664\n",
            "275/448, train_loss: 0.0894\n",
            "276/448, train_loss: 0.1425\n",
            "277/448, train_loss: 0.1545\n",
            "278/448, train_loss: 0.1839\n",
            "279/448, train_loss: 0.2492\n",
            "280/448, train_loss: 0.1443\n",
            "281/448, train_loss: 0.1655\n",
            "282/448, train_loss: 0.1493\n",
            "283/448, train_loss: 0.2019\n",
            "284/448, train_loss: 0.1919\n",
            "285/448, train_loss: 0.1663\n",
            "286/448, train_loss: 0.5735\n",
            "287/448, train_loss: 0.1997\n",
            "288/448, train_loss: 0.1967\n",
            "289/448, train_loss: 0.2149\n",
            "290/448, train_loss: 0.1134\n",
            "291/448, train_loss: 0.1492\n",
            "292/448, train_loss: 0.1176\n",
            "293/448, train_loss: 0.1689\n",
            "294/448, train_loss: 0.1099\n",
            "295/448, train_loss: 0.1622\n",
            "296/448, train_loss: 0.1630\n",
            "297/448, train_loss: 0.1333\n",
            "298/448, train_loss: 0.1048\n",
            "299/448, train_loss: 0.1220\n",
            "300/448, train_loss: 0.1764\n",
            "301/448, train_loss: 0.1514\n",
            "302/448, train_loss: 0.1796\n",
            "303/448, train_loss: 0.1774\n",
            "304/448, train_loss: 0.1788\n",
            "305/448, train_loss: 0.1733\n",
            "306/448, train_loss: 0.0993\n",
            "307/448, train_loss: 0.1816\n",
            "308/448, train_loss: 0.1799\n",
            "309/448, train_loss: 0.1630\n",
            "310/448, train_loss: 0.1630\n",
            "311/448, train_loss: 0.0843\n",
            "312/448, train_loss: 0.1574\n",
            "313/448, train_loss: 0.2266\n",
            "314/448, train_loss: 0.0888\n",
            "315/448, train_loss: 0.1278\n",
            "316/448, train_loss: 0.1720\n",
            "317/448, train_loss: 0.1502\n",
            "318/448, train_loss: 0.1239\n",
            "319/448, train_loss: 0.1374\n",
            "320/448, train_loss: 0.1782\n",
            "321/448, train_loss: 0.1807\n",
            "322/448, train_loss: 0.1425\n",
            "323/448, train_loss: 0.1493\n",
            "324/448, train_loss: 0.1208\n",
            "325/448, train_loss: 0.2078\n",
            "326/448, train_loss: 0.1341\n",
            "327/448, train_loss: 0.1306\n",
            "328/448, train_loss: 0.1611\n",
            "329/448, train_loss: 0.1385\n",
            "330/448, train_loss: 0.1202\n",
            "331/448, train_loss: 0.1849\n",
            "332/448, train_loss: 0.1447\n",
            "333/448, train_loss: 0.1269\n",
            "334/448, train_loss: 0.1340\n",
            "335/448, train_loss: 0.1871\n",
            "336/448, train_loss: 0.1614\n",
            "337/448, train_loss: 0.1336\n",
            "338/448, train_loss: 0.1579\n",
            "339/448, train_loss: 0.1388\n",
            "340/448, train_loss: 0.2076\n",
            "341/448, train_loss: 0.0990\n",
            "342/448, train_loss: 0.1840\n",
            "343/448, train_loss: 0.0921\n",
            "344/448, train_loss: 0.1982\n",
            "345/448, train_loss: 0.1777\n",
            "346/448, train_loss: 0.1697\n",
            "347/448, train_loss: 0.1503\n",
            "348/448, train_loss: 0.1510\n",
            "349/448, train_loss: 0.1238\n",
            "350/448, train_loss: 0.1577\n",
            "351/448, train_loss: 0.0873\n",
            "352/448, train_loss: 0.2092\n",
            "353/448, train_loss: 0.1064\n",
            "354/448, train_loss: 0.1398\n",
            "355/448, train_loss: 0.1476\n",
            "356/448, train_loss: 0.1519\n",
            "357/448, train_loss: 0.3410\n",
            "358/448, train_loss: 0.1708\n",
            "359/448, train_loss: 0.2066\n",
            "360/448, train_loss: 0.1353\n",
            "361/448, train_loss: 0.1223\n",
            "362/448, train_loss: 0.1959\n",
            "363/448, train_loss: 0.0916\n",
            "364/448, train_loss: 0.1829\n",
            "365/448, train_loss: 0.0996\n",
            "366/448, train_loss: 0.1592\n",
            "367/448, train_loss: 0.1660\n",
            "368/448, train_loss: 0.3411\n",
            "369/448, train_loss: 0.1690\n",
            "370/448, train_loss: 0.1331\n",
            "371/448, train_loss: 0.2422\n",
            "372/448, train_loss: 0.2148\n",
            "373/448, train_loss: 0.1833\n",
            "374/448, train_loss: 0.1365\n",
            "375/448, train_loss: 0.1917\n",
            "376/448, train_loss: 0.1744\n",
            "377/448, train_loss: 0.2469\n",
            "378/448, train_loss: 0.1406\n",
            "379/448, train_loss: 0.1195\n",
            "380/448, train_loss: 0.2071\n",
            "381/448, train_loss: 0.0847\n",
            "382/448, train_loss: 0.1688\n",
            "383/448, train_loss: 0.1584\n",
            "384/448, train_loss: 0.1271\n",
            "385/448, train_loss: 0.0514\n",
            "386/448, train_loss: 0.1644\n",
            "387/448, train_loss: 0.2461\n",
            "388/448, train_loss: 0.1715\n",
            "389/448, train_loss: 0.1251\n",
            "390/448, train_loss: 0.1264\n",
            "391/448, train_loss: 0.3078\n",
            "392/448, train_loss: 0.1496\n",
            "393/448, train_loss: 0.1390\n",
            "394/448, train_loss: 0.2235\n",
            "395/448, train_loss: 0.1390\n",
            "396/448, train_loss: 0.1693\n",
            "397/448, train_loss: 0.1524\n",
            "398/448, train_loss: 0.1353\n",
            "399/448, train_loss: 0.2036\n",
            "400/448, train_loss: 0.1726\n",
            "401/448, train_loss: 0.1744\n",
            "402/448, train_loss: 0.1233\n",
            "403/448, train_loss: 0.2396\n",
            "404/448, train_loss: 0.1468\n",
            "405/448, train_loss: 0.1266\n",
            "406/448, train_loss: 0.1962\n",
            "407/448, train_loss: 0.2198\n",
            "408/448, train_loss: 0.1800\n",
            "409/448, train_loss: 0.1489\n",
            "410/448, train_loss: 0.2368\n",
            "411/448, train_loss: 0.1889\n",
            "412/448, train_loss: 0.2387\n",
            "413/448, train_loss: 0.1016\n",
            "414/448, train_loss: 0.1155\n",
            "415/448, train_loss: 0.1899\n",
            "416/448, train_loss: 0.1150\n",
            "417/448, train_loss: 0.3833\n",
            "418/448, train_loss: 0.1023\n",
            "419/448, train_loss: 0.3460\n",
            "420/448, train_loss: 0.1823\n",
            "421/448, train_loss: 0.1539\n",
            "422/448, train_loss: 0.2260\n",
            "423/448, train_loss: 0.1481\n",
            "424/448, train_loss: 0.3147\n",
            "425/448, train_loss: 0.1282\n",
            "426/448, train_loss: 0.1469\n",
            "427/448, train_loss: 0.0927\n",
            "428/448, train_loss: 0.1415\n",
            "429/448, train_loss: 0.1728\n",
            "430/448, train_loss: 0.2249\n",
            "431/448, train_loss: 0.1470\n",
            "432/448, train_loss: 0.2086\n",
            "433/448, train_loss: 0.1588\n",
            "434/448, train_loss: 0.1414\n",
            "435/448, train_loss: 0.3055\n",
            "436/448, train_loss: 0.1719\n",
            "437/448, train_loss: 0.1972\n",
            "438/448, train_loss: 0.1978\n",
            "439/448, train_loss: 0.1079\n",
            "440/448, train_loss: 0.2867\n",
            "441/448, train_loss: 0.2056\n",
            "442/448, train_loss: 0.1288\n",
            "443/448, train_loss: 0.0979\n",
            "444/448, train_loss: 0.2274\n",
            "445/448, train_loss: 0.0845\n",
            "446/448, train_loss: 0.1033\n",
            "447/448, train_loss: 0.1370\n",
            "448/448, train_loss: 0.1526\n",
            "449/448, train_loss: 0.0843\n",
            "epoch 50 average loss: 0.1716\n",
            "current epoch: 50 current mean dice: 0.8311 best mean dice: 0.8339 at epoch: 44\n",
            "train completed, best_metric: 0.8339 at epoch: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [ i + 1 for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZEQO73n5vY3p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "c6ac4f50-61e6-4d63-a485-771e6291650f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "current epoch: 50 current mean dice: 0.8311 best mean dice: 0.8339 at epoch: 44\n",
        "train completed, best_metric: 0.8339 at epoch: 44\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "kPPtFbYBufCh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed517ec4b3ac440baaa57f27e7803502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bde6aaf540934878965e745c1dfbccd3",
              "IPY_MODEL_5f5b557bef694ea6aa8f048755269629",
              "IPY_MODEL_bc1a1f24ef4548b3a1f6dc814ebfce65"
            ],
            "layout": "IPY_MODEL_ddfa71c98d484e899177dbd39f0a0816"
          }
        },
        "bde6aaf540934878965e745c1dfbccd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a08482bbbdd4f56ad84da9a233ec52c",
            "placeholder": "​",
            "style": "IPY_MODEL_e912aacd85ef46bfae064b3ca3517307",
            "value": "100%"
          }
        },
        "5f5b557bef694ea6aa8f048755269629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7da7c9a619544f6921d934fb7770f0d",
            "max": 102502400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84c0dac9a61543fca1bb62622f665cc1",
            "value": 102502400
          }
        },
        "bc1a1f24ef4548b3a1f6dc814ebfce65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_389899da1dc14246b7f5eb7723ddd515",
            "placeholder": "​",
            "style": "IPY_MODEL_c9424144d80547c7a3239d9ce17f2443",
            "value": " 97.8M/97.8M [00:03&lt;00:00, 99.1MB/s]"
          }
        },
        "ddfa71c98d484e899177dbd39f0a0816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a08482bbbdd4f56ad84da9a233ec52c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e912aacd85ef46bfae064b3ca3517307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7da7c9a619544f6921d934fb7770f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84c0dac9a61543fca1bb62622f665cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "389899da1dc14246b7f5eb7723ddd515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9424144d80547c7a3239d9ce17f2443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}